{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "SEED = 202408\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.02\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "CHANNELS = 3\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "# Folders\n",
    "DATA_DIR = './rsna-mil-training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    # bsb_img = np.array([brain_img, subdural_img, soft_img])\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "\n",
    "    return bsb_img\n",
    "\n",
    "def _read(path, SHAPE):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(SHAPE)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_slice(slice, target_size=(224, 224)):\n",
    "    slice = resize(slice, target_size, anti_aliasing=True)\n",
    "    \n",
    "    slice = apply_windowing(slice)\n",
    "    \n",
    "    return slice\n",
    "\n",
    "def apply_windowing(slice):\n",
    "    brain_window = (40, 80, -100, 200)\n",
    "    slice = np.clip(slice, brain_window[2], brain_window[3])\n",
    "    slice = (slice - brain_window[2]) / (brain_window[3] - brain_window[2])\n",
    "    return slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dicom_folder(folder_path):\n",
    "    slices = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            slices.append(ds.pixel_array)\n",
    "    \n",
    "    # Add black images if the number of slices is less than 60\n",
    "    num_slices = len(slices)\n",
    "    if num_slices < 60:\n",
    "        black_slice = np.zeros_like(slices[0])\n",
    "        for _ in range(60 - num_slices):\n",
    "            slices.append(black_slice)\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(data_dir, row):\n",
    "    \"\"\"\n",
    "    Process data for a single patient based on the row from the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): The directory containing DICOM folders.\n",
    "        row (pd.Series): A row from the patient_scan_labels DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Preprocessed slices and label.\n",
    "    \"\"\"\n",
    "    patient_id = row['patient_id'].replace('ID_', '')  # Remove 'ID_' prefix\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')  # Remove 'ID_' prefix\n",
    "    \n",
    "    # Construct folder path based on patient_id and study_instance_uid\n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(data_dir, folder_name)\n",
    "    \n",
    "    # Read and preprocess DICOM slices\n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Determine label based on any of the hemorrhage indicators\n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        return preprocessed_slices, label\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None  # Handle the case where the folder is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found\n",
    "\n",
    "class TestDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for testing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found\n",
    "        \n",
    "\n",
    "# Function to create DataLoader for training\n",
    "def get_train_loader(data_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE, shuffle=True):\n",
    "    train_dataset = TrainDatasetGenerator(data_dir, patient_scan_labels)\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Function to create DataLoader for testing\n",
    "def get_test_loader(data_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(data_dir, patient_scan_labels)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNNFeatureExtractor(nn.Module):\n",
    "#     \"\"\"\n",
    "#     CNN backbone to extract features from each slice\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "#         # Calculate the size after convolutions and pooling\n",
    "#         self.output_size = 64 * (224 // 2 // 2) * (224 // 2 // 2)  # Output size after conv2 and pooling\n",
    "\n",
    "#         self.fc1 = nn.Linear(self.output_size, 128)  # Adjust based on input size\n",
    "#         self.fc2 = nn.Linear(128, 64)  # Output size for attention\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Pass slice through CNN layers\n",
    "#         x = self.pool(F.relu(self.conv1(x)))  # Shape: (batch_size, 32, 112, 112)\n",
    "#         x = self.pool(F.relu(self.conv2(x)))  # Shape: (batch_size, 64, 56, 56)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         features = self.fc2(x)  # Output features for attention\n",
    "#         return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN backbone to extract features from each slice\n",
    "    \"\"\"\n",
    "    def __init__(self, slice_size=(224, 224), num_slices=60):\n",
    "        super().__init__()\n",
    "        self.slice_size = slice_size\n",
    "        self.num_slices = num_slices\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Calculate the size after convolutions and pooling\n",
    "        self.output_size = 64 * (slice_size[0] // 2 // 2) * (slice_size[1] // 2 // 2)  # Output size after conv2 and pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(self.output_size, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 64)  # Output size for attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input has the expected number of slices\n",
    "        if x.size(1) < self.num_slices:\n",
    "            padding = torch.zeros(x.size(0), self.num_slices - x.size(1), *self.slice_size)\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "        elif x.size(1) > self.num_slices:\n",
    "            x = x[:, :self.num_slices]\n",
    "\n",
    "        # Pass slices through CNN layers\n",
    "        slice_features = []\n",
    "        for i in range(self.num_slices):\n",
    "            slice_input = x[:, i].unsqueeze(1)  # Shape: (batch_size, 1, height, width)\n",
    "            slice_feature = self.cnn(slice_input)\n",
    "            slice_features.append(slice_feature)\n",
    "\n",
    "        slice_features = torch.stack(slice_features, dim=1)  # Shape: (batch_size, num_slices, feature_dim)\n",
    "        \n",
    "        x = slice_features.view(slice_features.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        features = self.fc2(x)  # Output features for attention\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism to aggregate slice features into bag features\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(input_dim, output_dim)\n",
    "        self.v = nn.Parameter(torch.rand(output_dim))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        # Compute attention weights for each slice\n",
    "        att_weights = torch.softmax(torch.tanh(self.W(features)) @ self.v, dim=1)\n",
    "        return att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2EAttGP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = CNNFeatureExtractor()\n",
    "        self.attention = AttentionLayer(input_dim=64, output_dim=32)  # Adjust input_dim based on ResNet output\n",
    "        self.classifier = nn.Linear(32, 1)  # Classifier for bag prediction\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape (batch_size, num_slices, height, width)\n",
    "        slice_features = []\n",
    "\n",
    "        for i in range(x.size(1)):  # Loop over each slice in the batch\n",
    "            slice_input = x[:, i, :, :].unsqueeze(1)  # Shape: (batch_size, 1, height, width)\n",
    "            slice_feature = self.cnn(slice_input)  # Process each slice through ResNet\n",
    "            slice_features.append(slice_feature)\n",
    "\n",
    "        slice_features = torch.stack(slice_features, dim=1)  # Shape: (batch_size, num_slices, feature_dim)\n",
    "        \n",
    "        # Compute attention weights for each slice\n",
    "        att_weights = self.attention(slice_features)\n",
    "        \n",
    "        # Aggregate slice features into bag feature using attention\n",
    "        bag_feature = (slice_features * att_weights.unsqueeze(-1)).sum(dim=1)  # Shape: (batch_size, feature_dim)\n",
    "\n",
    "        # Pass bag feature through the classifier\n",
    "        logits = self.classifier(bag_feature).squeeze(-1)  # Shape: (batch_size,)\n",
    "        probs = self.sigmoid(logits)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './rsna-mil-training'\n",
    "patient_scan_labels = pd.read_csv('training_1000_scan_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader(data_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "test_loader = get_test_loader(data_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "Epoch [1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                      \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.71 GiB of which 28.50 MiB is free. Including non-PyTorch memory, this process has 11.23 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 44.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m batch_labels \u001b[38;5;241m=\u001b[39m batch_labels\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move labels to GPU\u001b[39;00m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_slices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mE2EAttGP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):  \u001b[38;5;66;03m# Loop over each slice in the batch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     slice_input \u001b[38;5;241m=\u001b[39m x[:, i, :, :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, 1, height, width)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     slice_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Process each slice through ResNet\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     slice_features\u001b[38;5;241m.\u001b[39mappend(slice_feature)\n\u001b[1;32m     18\u001b[0m slice_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(slice_features, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, num_slices, feature_dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36mCNNFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Pass slice through CNN layers\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Shape: (batch_size, 32, 112, 112)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))  \u001b[38;5;66;03m# Shape: (batch_size, 64, 56, 56)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 11.71 GiB of which 28.50 MiB is free. Including non-PyTorch memory, this process has 11.23 GiB memory in use. Of the allocated memory 10.97 GiB is allocated by PyTorch, and 44.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "model = E2EAttGP().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print('############################################')\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    model.train()\n",
    "    total_loss = 0  # To track total loss for the epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_loader_tqdm = tqdm(train_loader, leave=False)\n",
    "    for batch_slices, batch_labels in train_loader_tqdm:\n",
    "        batch_slices = batch_slices.to(device)  # Move batch to GPU\n",
    "        batch_labels = batch_labels.to(device)  # Move labels to GPU\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_slices)\n",
    "        loss = criterion(outputs, batch_labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        predicted = (outputs >= 0.5).float()  # Convert outputs to binary predictions\n",
    "        correct += (predicted.squeeze() == batch_labels.float()).sum().item()  # Count correct predictions\n",
    "        total += batch_labels.size(0)  # Update total count\n",
    "\n",
    "        # Calculate accuracy for the batch\n",
    "        batch_accuracy = (predicted.squeeze() == batch_labels.float()).float().mean().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_loader_tqdm.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item(), accuracy=batch_accuracy)\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_slices, batch_labels in test_loader:\n",
    "        outputs = model(batch_slices)\n",
    "        \n",
    "        # Convert outputs to binary predictions\n",
    "        predicted = (outputs >= 0.5).float()  # Assuming outputs are probabilities\n",
    "\n",
    "        # Update total and correct counts\n",
    "        total += batch_labels.size(0)  # Number of samples in the batch\n",
    "        correct += (predicted.squeeze() == batch_labels.float()).sum().item()  # Count correct predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
