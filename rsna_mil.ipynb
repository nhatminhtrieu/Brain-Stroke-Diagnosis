{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Training will run on CPU.\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "SEED = 202408\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.02\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "CHANNELS = 3\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 64\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "# Folders\n",
    "DATA_DIR = './rsna-mil-training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    # bsb_img = np.array([brain_img, subdural_img, soft_img])\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "\n",
    "    return bsb_img\n",
    "\n",
    "def _read(path, SHAPE):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(SHAPE)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_slice(slice, target_size=(224, 224)):\n",
    "    slice = resize(slice, target_size, anti_aliasing=True)\n",
    "    \n",
    "    slice = apply_windowing(slice)\n",
    "    \n",
    "    return slice\n",
    "\n",
    "def apply_windowing(slice):\n",
    "    brain_window = (40, 80, -100, 200)\n",
    "    slice = np.clip(slice, brain_window[2], brain_window[3])\n",
    "    slice = (slice - brain_window[2]) / (brain_window[3] - brain_window[2])\n",
    "    return slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dicom_folder(folder_path):\n",
    "    slices = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            slices.append(ds.pixel_array)\n",
    "    \n",
    "    # Add black images if the number of slices is less than 60\n",
    "    num_slices = len(slices)\n",
    "    if num_slices < 60:\n",
    "        black_slice = np.zeros_like(slices[0])\n",
    "        for _ in range(60 - num_slices):\n",
    "            slices.append(black_slice)\n",
    "    \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_data(data_dir, row):\n",
    "    \"\"\"\n",
    "    Process data for a single patient based on the row from the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): The directory containing DICOM folders.\n",
    "        row (pd.Series): A row from the patient_scan_labels DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Preprocessed slices and label.\n",
    "    \"\"\"\n",
    "    patient_id = row['patient_id'].replace('ID_', '')  # Remove 'ID_' prefix\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')  # Remove 'ID_' prefix\n",
    "    \n",
    "    # Construct folder path based on patient_id and study_instance_uid\n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(data_dir, folder_name)\n",
    "    \n",
    "    # Read and preprocess DICOM slices\n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Determine label based on any of the hemorrhage indicators\n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        return preprocessed_slices, label\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None  # Handle the case where the folder is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found\n",
    "\n",
    "class TestDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for testing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found\n",
    "        \n",
    "\n",
    "# Function to create DataLoader for training\n",
    "def get_train_loader(data_dir, patient_scan_labels, batch_size=16, shuffle=True):\n",
    "    train_dataset = TrainDatasetGenerator(data_dir, patient_scan_labels)\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Function to create DataLoader for testing\n",
    "def get_test_loader(data_dir, patient_scan_labels, batch_size=16):\n",
    "    test_dataset = TestDatasetGenerator(data_dir, patient_scan_labels)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN backbone to extract features from each slice\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Calculate the size after convolutions and pooling\n",
    "        self.output_size = 64 * (224 // 2 // 2) * (224 // 2 // 2)  # Output size after conv2 and pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(self.output_size, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 64)  # Output size for attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass slice through CNN layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Shape: (batch_size, 32, 112, 112)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Shape: (batch_size, 64, 56, 56)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        features = self.fc2(x)  # Output features for attention\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism to aggregate slice features into bag features\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(input_dim, output_dim)\n",
    "        self.v = nn.Parameter(torch.rand(output_dim))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        # Compute attention weights for each slice\n",
    "        att_weights = torch.softmax(torch.tanh(self.W(features)) @ self.v, dim=1)\n",
    "        return att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2EAttGP(nn.Module):\n",
    "    \"\"\"\n",
    "    End-to-end model combining ResNet18, Attention and a simple classifier\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = CNNFeatureExtractor()\n",
    "        self.attention = AttentionLayer(input_dim=64, output_dim=32)  # Adjust input_dim based on ResNet output\n",
    "        self.classifier = nn.Linear(32, 1)  # Classifier for bag prediction\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape (batch_size, num_slices, height, width)\n",
    "        slice_features = []\n",
    "\n",
    "        for i in range(x.size(1)):  # Loop over each slice in the batch\n",
    "            slice_input = x[:, i, :, :].unsqueeze(1)  # Shape: (batch_size, 1, height, width)\n",
    "            slice_feature = self.cnn(slice_input)  # Process each slice through ResNet\n",
    "            slice_features.append(slice_feature)\n",
    "\n",
    "        slice_features = torch.stack(slice_features, dim=1)  # Shape: (batch_size, num_slices, feature_dim)\n",
    "        \n",
    "        # Compute attention weights for each slice\n",
    "        att_weights = self.attention(slice_features)\n",
    "        \n",
    "        # Aggregate slice features into bag feature using attention\n",
    "        bag_feature = (slice_features * att_weights.unsqueeze(-1)).sum(dim=1)  # Shape: (batch_size, feature_dim)\n",
    "\n",
    "        # Pass bag feature through the classifier\n",
    "        logits = self.classifier(bag_feature).squeeze(-1)  # Shape: (batch_size,)\n",
    "        probs = self.sigmoid(logits)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './rsna-mil-training'\n",
    "patient_scan_labels = pd.read_csv(os.path.join(data_dir, 'training_1000_scan_subset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_train_loader(data_dir, patient_scan_labels, batch_size=16)\n",
    "test_loader = get_test_loader(data_dir, patient_scan_labels, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "model = E2EAttGP().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print('############################################')\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    model.train()\n",
    "    total_loss = 0  # To track total loss for the epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_loader_tqdm = tqdm(train_loader, leave=False)\n",
    "    for batch_slices, batch_labels in train_loader_tqdm:\n",
    "        batch_slices = batch_slices.to(device)  # Move batch to GPU\n",
    "        batch_labels = batch_labels.to(device)  # Move labels to GPU\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_slices)\n",
    "        loss = criterion(outputs, batch_labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "        predicted = (outputs >= 0.5).float()  # Convert outputs to binary predictions\n",
    "        correct += (predicted.squeeze() == batch_labels.float()).sum().item()  # Count correct predictions\n",
    "        total += batch_labels.size(0)  # Update total count\n",
    "\n",
    "        # Calculate accuracy for the batch\n",
    "        batch_accuracy = (predicted.squeeze() == batch_labels.float()).float().mean().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_loader_tqdm.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item(), accuracy=batch_accuracy)\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_slices, batch_labels in test_loader:\n",
    "        outputs = model(batch_slices)\n",
    "        \n",
    "        # Convert outputs to binary predictions\n",
    "        predicted = (outputs >= 0.5).float()  # Assuming outputs are probabilities\n",
    "\n",
    "        # Update total and correct counts\n",
    "        total += batch_labels.size(0)  # Number of samples in the batch\n",
    "        correct += (predicted.squeeze() == batch_labels.float()).sum().item()  # Count correct predictions\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
