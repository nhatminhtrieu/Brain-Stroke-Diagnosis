{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T19:59:32.815372Z",
     "start_time": "2024-12-14T19:59:32.805132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import zipfile\n",
    "# import os\n",
    "#\n",
    "# def extract_folder_from_zip(zip_path, folder_name, output_path):\n",
    "#     \"\"\"\n",
    "#     Extracts all files from a specific folder in a ZIP file to a destination directory,\n",
    "#     preserving the original folder structure.\n",
    "#\n",
    "#     Parameters:\n",
    "#     zip_path (str): Path to the ZIP file.\n",
    "#     folder_name (str): Name of the folder within the ZIP file to extract.\n",
    "#     output_path (str): Destination directory where files will be extracted.\n",
    "#     \"\"\"\n",
    "#     with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#         # Iterate over all the items in the ZIP file\n",
    "#         for file_info in zip_ref.infolist():\n",
    "#             # Check if the current item is in the specified folder\n",
    "#             if file_info.filename.startswith(folder_name + '/'):\n",
    "#                 # Define the full path for extraction\n",
    "#                 # Use os.path.relpath to keep the structure relative to folder_name\n",
    "#                 extracted_path = os.path.join(output_path, os.path.relpath(file_info.filename, start=folder_name))\n",
    "#                 # Create any necessary directories\n",
    "#                 os.makedirs(os.path.dirname(extracted_path), exist_ok=True)\n",
    "#                 # Extract the file\n",
    "#                 with zip_ref.open(file_info) as source_file, open(extracted_path, 'wb') as target_file:\n",
    "#                     target_file.write(source_file.read())\n",
    "#                 # print(f\"Extracted: {file_info.filename} to {extracted_path}\")\n",
    "#\n",
    "# # Example usage\n",
    "# zip_file_path = '/media02/tdhoang01/21127112-21127734/data/rsna-intracranial-hemorrhage-detection.zip'  # Path to your ZIP file\n",
    "# folder_to_extract = 'rsna-intracranial-hemorrhage-detection/stage_2_train'  # Folder inside the ZIP to extract\n",
    "# destination_directory = '/media02/tdhoang01/21127112-21127734/data/rsna-ich-mil/'  # Where to extract files\n",
    "#\n",
    "# extract_folder_from_zip(zip_file_path, folder_to_extract, destination_directory)"
   ],
   "id": "fd0c14b1c942707f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T19:59:32.831375Z",
     "start_time": "2024-12-14T19:59:32.824374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import shutil\n",
    "#\n",
    "# def organize_dicom_files(csv_file, source_dir):\n",
    "#     # Read the CSV file into a DataFrame\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#\n",
    "#     # Remove 'ID_' prefix from patient_id and study_instance_uid\n",
    "#     df['patient_id'] = df['patient_id'].str.replace('ID_', '', regex=False)\n",
    "#     df['study_instance_uid'] = df['study_instance_uid'].str.replace('ID_', '', regex=False)\n",
    "#\n",
    "#     # Group by patient_id and study_instance_uid\n",
    "#     grouped = df.groupby(['patient_id', 'study_instance_uid'])\n",
    "#\n",
    "#     for (patient_id, study_instance_uid), group in grouped:\n",
    "#         # Create a subfolder name based on patient_id and study_instance_uid\n",
    "#         subfolder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "#         subfolder_path = os.path.join(source_dir, subfolder_name)\n",
    "#\n",
    "#         # Create the subfolder if it does not exist\n",
    "#         os.makedirs(subfolder_path, exist_ok=True)\n",
    "#\n",
    "#         # Move each file in the group to the respective subfolder\n",
    "#         for _, row in group.iterrows():\n",
    "#             filename = row['filename']\n",
    "#             source_file_path = os.path.join(source_dir, filename)\n",
    "#             destination_file_path = os.path.join(subfolder_path, filename)\n",
    "#\n",
    "#             # Move the file\n",
    "#             if os.path.exists(source_file_path):\n",
    "#                 shutil.move(source_file_path, destination_file_path)\n",
    "#                 # print(f\"Moved: {source_file_path} to {destination_file_path}\")\n",
    "#             else:\n",
    "#                 print(f\"File not found: {source_file_path}\")\n",
    "#\n",
    "# # Example usage\n",
    "# csv_file_path = '/media02/tdhoang01/21127112-21127734/data/training_dataset.csv'  # Path to your CSV file\n",
    "# source_directory = '/media02/tdhoang01/21127112-21127734/data/rsna-ich-mil/'        # Source directory containing DICOM files\n",
    "#\n",
    "# organize_dicom_files(csv_file_path, source_directory)"
   ],
   "id": "eccc0a04fe5664b4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T20:08:51.380944Z",
     "start_time": "2024-12-14T20:08:51.373943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_and_organize_from_zip(zip_path, csv_file, output_path, n, folder_name):\n",
    "    \"\"\"\n",
    "    Extracts specific files from a ZIP file based on the filenames listed in a CSV,\n",
    "    and organizes them into subfolders named based on patient_id and study_instance_uid,\n",
    "    but only for the first n rows in the CSV. Only files within a specific folder in the ZIP are considered.\n",
    "\n",
    "    Parameters:\n",
    "    zip_path (str): Path to the ZIP file.\n",
    "    csv_file (str): Path to the CSV file containing filenames and metadata.\n",
    "    output_path (str): Destination directory where files will be extracted and organized.\n",
    "    n (int): Number of rows to process from the CSV file.\n",
    "    folder_name (str): The specific folder within the ZIP file to read files from.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a DataFrame, limiting to the first n rows\n",
    "    df = pd.read_csv(csv_file, nrows=n)\n",
    "\n",
    "    # Ensure the CSV column names match the expected column names\n",
    "    df.columns = df.columns.str.strip()  # Remove leading/trailing spaces from column names\n",
    "    if 'patient_id' not in df.columns or 'study_instance_uid' not in df.columns or 'filename' not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain 'patient_id', 'study_instance_uid', and 'filename' columns\")\n",
    "\n",
    "    # Remove 'ID_' prefix from patient_id and study_instance_uid\n",
    "    df['patient_id'] = df['patient_id'].str.replace('ID_', '', regex=False)\n",
    "    df['study_instance_uid'] = df['study_instance_uid'].str.replace('ID_', '', regex=False)\n",
    "\n",
    "    # Parse the filenames column from string representation of list to actual list\n",
    "    df['filename'] = df['filename'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "    # Flatten the DataFrame to have one filename per row\n",
    "    df = df.explode('filename')\n",
    "\n",
    "    # Create a set of filenames for quick lookup\n",
    "    required_files = set(df['filename'].tolist())\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Iterate over all the items in the ZIP file\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Check if the current item is in the specified folder and in the required files list\n",
    "            if file_info.filename.startswith(folder_name + '/') and os.path.basename(file_info.filename) in required_files:\n",
    "                # Find the corresponding row in the DataFrame\n",
    "                row = df[df['filename'] == os.path.basename(file_info.filename)].iloc[0]\n",
    "                patient_id = row['patient_id']\n",
    "                study_instance_uid = row['study_instance_uid']\n",
    "\n",
    "                # Create a subfolder name based on patient_id and study_instance_uid\n",
    "                subfolder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "                subfolder_path = os.path.join(output_path, subfolder_name)\n",
    "\n",
    "                # Create the subfolder if it does not exist\n",
    "                os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "                # Define the full path for extraction\n",
    "                extracted_path = os.path.join(subfolder_path, os.path.basename(file_info.filename))\n",
    "\n",
    "                # Extract the file\n",
    "                with zip_ref.open(file_info) as source_file, open(extracted_path, 'wb') as target_file:\n",
    "                    target_file.write(source_file.read())\n",
    "                # print(f\"Extracted: {file_info.filename} to {extracted_path}\")"
   ],
   "id": "aa836760f23a076e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T20:09:04.877420Z",
     "start_time": "2024-12-14T20:08:52.512935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "zip_file_path = \"D:/Datasets/rsna-intracranial-hemorrhage-detection.zip\"  # Path to your ZIP file\n",
    "csv_file_path = \"C:/Users/MINH/OneDrive - nhatminhtrieu/Thesis 2025 Storage/training_dataset_1.csv\"  # Path to your CSV file\n",
    "destination_directory = 'D:/Datasets/rsna-ich-mil/'  # Where to extract and organize files\n",
    "n_rows = 50  # Limit to first n rows in the CSV\n",
    "folder_to_read = 'rsna-intracranial-hemorrhage-detection/stage_2_train'  # Specific folder within the ZIP file\n",
    "\n",
    "extract_and_organize_from_zip(zip_file_path, csv_file_path, destination_directory, n_rows, folder_to_read)"
   ],
   "id": "761f9b6f326a13bf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T15:10:00.947968Z",
     "start_time": "2024-12-27T15:09:58.540971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./rsna/data_analyze/training_dataset_1.csv')\n",
    "\n",
    "multi_label_columns = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "\n",
    "for column in multi_label_columns:\n",
    "    df[column] = df[column].apply(\n",
    "        lambda x: eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "# Create new 6 columns for each multi-label column\n",
    "for column in multi_label_columns:\n",
    "        df[column + f'_'] = df[column].apply(lambda x: 1 if sum(x) > 0 else 0)\n",
    "df.head()\n",
    "\n",
    "# df.to_csv('temp.csv', index=False)"
   ],
   "id": "a9e42c3742783706",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_45785016b.dcm', 'ID_37f32aed2.dcm', 'ID_1...   \n",
       "1  ['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...   \n",
       "2  ['ID_c6f9f68c9.dcm', 'ID_520df89aa.dcm', 'ID_b...   \n",
       "3  ['ID_31b14de96.dcm', 'ID_203ef1efe.dcm', 'ID_9...   \n",
       "4  ['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0002cd41   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00054f3f   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0006d192   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00086119   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_000e5623   \n",
       "\n",
       "  study_instance_uid  ...                                       window_width  \\\n",
       "0      ID_66929e09d4  ...  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "1      ID_8a449ae31b  ...  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "2      ID_25690b4725  ...  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "3      ID_fdde2979b0  ...  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "4      ID_9a4be35b9a  ...  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "\n",
       "                                   rescale_intercept  \\\n",
       "0  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "1  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "2  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "3  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "4  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "\n",
       "                                       rescale_slope patient_label any_  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0    0   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0    0   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0    0   \n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0    0   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0    0   \n",
       "\n",
       "  epidural_ intraparenchymal_ intraventricular_  subarachnoid_  subdural_  \n",
       "0         0                 0                 0              0          0  \n",
       "1         0                 0                 0              0          0  \n",
       "2         0                 0                 0              0          0  \n",
       "3         0                 0                 0              0          0  \n",
       "4         0                 0                 0              0          0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>...</th>\n",
       "      <th>window_width</th>\n",
       "      <th>rescale_intercept</th>\n",
       "      <th>rescale_slope</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>any_</th>\n",
       "      <th>epidural_</th>\n",
       "      <th>intraparenchymal_</th>\n",
       "      <th>intraventricular_</th>\n",
       "      <th>subarachnoid_</th>\n",
       "      <th>subdural_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_45785016b.dcm', 'ID_37f32aed2.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0002cd41</td>\n",
       "      <td>ID_66929e09d4</td>\n",
       "      <td>...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00054f3f</td>\n",
       "      <td>ID_8a449ae31b</td>\n",
       "      <td>...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_c6f9f68c9.dcm', 'ID_520df89aa.dcm', 'ID_b...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0006d192</td>\n",
       "      <td>ID_25690b4725</td>\n",
       "      <td>...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_31b14de96.dcm', 'ID_203ef1efe.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00086119</td>\n",
       "      <td>ID_fdde2979b0</td>\n",
       "      <td>...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_000e5623</td>\n",
       "      <td>ID_9a4be35b9a</td>\n",
       "      <td>...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T15:10:02.921088Z",
     "start_time": "2024-12-27T15:10:02.915521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the number of row that has more than 1 label in these columns\n",
    "multi_label_columns = ['any_', 'epidural_', 'intraparenchymal_', 'intraventricular_', 'subarachnoid_', 'subdural_']\n",
    "df['multi_label'] = df[multi_label_columns].sum(axis=1)\n",
    "df['multi_label'].value_counts()\n"
   ],
   "id": "8ad4ef9c7f754ae4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_label\n",
       "0    12862\n",
       "2     3650\n",
       "3     2862\n",
       "4     1770\n",
       "5      573\n",
       "6       27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T06:22:21.111986Z",
     "start_time": "2025-01-12T06:10:45.456427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pydicom\n",
    "from pydicom.errors import InvalidDicomError\n",
    "\n",
    "def check_dicom_files(folder_path, log_file='dicom_errors.log'):\n",
    "    \"\"\"\n",
    "    Check all DICOM files in the specified folder for errors or warnings.\n",
    "    Log any issues to the specified log file.\n",
    "\n",
    "    :param folder_path: Path to the folder containing DICOM files.\n",
    "    :param log_file: Path to the log file where errors/warnings will be recorded.\n",
    "    \"\"\"\n",
    "    with open(log_file, 'w') as log:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        # Read the DICOM file\n",
    "                        ds = pydicom.dcmread(file_path)\n",
    "\n",
    "                        # Check for pixel data issues\n",
    "                        if 'PixelData' in ds:\n",
    "                            # expected_length = ds.Rows * ds.Columns * ds.SamplesPerPixel\n",
    "                            expected_length = 524288\n",
    "                            actual_length = len(ds.PixelData)\n",
    "                            if actual_length != expected_length:\n",
    "                                log.write(f\"Error reading {file_path}: The length of the pixel data in the dataset ({actual_length} bytes) doesn't match the expected length ({expected_length} bytes).\\n\")\n",
    "\n",
    "                        # Log the dimensions of the pixel array\n",
    "                        if 'PixelData' in ds:\n",
    "                            log.write(f\"File '{file_path}' has pixel array dimensions: ({ds.Rows}, {ds.Columns})\\n\")\n",
    "\n",
    "                    except InvalidDicomError:\n",
    "                        log.write(f\"Error: {file_path} is not a valid DICOM file.\\n\")\n",
    "                    except Exception as e:\n",
    "                        log.write(f\"Error reading {file_path}: {str(e)}\\n\")\n",
    "\n",
    "# Example usage\n",
    "check_dicom_files('rsna-ich-mil')"
   ],
   "id": "28a2ee7a89f46eb1",
   "outputs": [],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
