{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 9399351,
     "sourceType": "datasetVersion",
     "datasetId": 5705276
    }
   ],
   "dockerImageVersionId": 30762,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Library",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:06.657632Z",
     "iopub.execute_input": "2024-09-17T13:12:06.658489Z",
     "iopub.status.idle": "2024-09-17T13:12:12.565284Z",
     "shell.execute_reply.started": "2024-09-17T13:12:06.658444Z",
     "shell.execute_reply": "2024-09-17T13:12:12.564455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Init GPU",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize GPU Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\nelse:\n    print(\"No GPU available. Training will run on CPU.\")\n\nprint(device)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.567164Z",
     "iopub.execute_input": "2024-09-17T13:12:12.567970Z",
     "iopub.status.idle": "2024-09-17T13:12:12.623942Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.567924Z",
     "shell.execute_reply": "2024-09-17T13:12:12.623018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.625028Z",
     "iopub.execute_input": "2024-09-17T13:12:12.625361Z",
     "iopub.status.idle": "2024-09-17T13:12:12.677673Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.625328Z",
     "shell.execute_reply": "2024-09-17T13:12:12.676808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Config Info",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "\n",
    "MAX_SLICES = 60\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "NUM_EPOCHS = 50\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.679620Z",
     "iopub.execute_input": "2024-09-17T13:12:12.679915Z",
     "iopub.status.idle": "2024-09-17T13:12:12.726300Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.679883Z",
     "shell.execute_reply": "2024-09-17T13:12:12.725447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Folders\n",
    "DATA_DIR = '../rsna-mil-training/'\n",
    "DICOM_DIR = DATA_DIR + 'rsna-mil-training'\n",
    "CSV_PATH = './data_analyze/training_1000_scan_subset.csv'\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.727356Z",
     "iopub.execute_input": "2024-09-17T13:12:12.727619Z",
     "iopub.status.idle": "2024-09-17T13:12:12.784661Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.727588Z",
     "shell.execute_reply": "2024-09-17T13:12:12.783914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    \n",
    "    bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n",
    "    return bsb_img.astype(np.float16)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n",
    "    # Check if type of slice is dicom \n",
    "    if (type(slice) == np.ndarray):\n",
    "        slice = resize(slice, target_size, anti_aliasing=True)\n",
    "        brain_window = apply_windowing(slice, window=(40, 80))\n",
    "        subdural_window = apply_windowing(slice, window=(80, 200))\n",
    "        bone_window = apply_windowing(slice, window=(600, 2800))\n",
    "        \n",
    "        multichannel_slice = np.stack([brain_window, subdural_window, bone_window], axis=-1)\n",
    "        return multichannel_slice.astype(np.float16)\n",
    "    else:\n",
    "        slice = bsb_window(slice)\n",
    "        return slice.astype(np.float16)\n",
    "\n",
    "def apply_windowing(slice, window):\n",
    "    window_width, window_level = window\n",
    "    lower_bound = window_level - window_width // 2\n",
    "    upper_bound = window_level + window_width // 2\n",
    "    \n",
    "    windowed_slice = np.clip(slice, lower_bound, upper_bound)\n",
    "    windowed_slice = (windowed_slice - lower_bound) / (upper_bound - lower_bound)\n",
    "    return windowed_slice"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.785792Z",
     "iopub.execute_input": "2024-09-17T13:12:12.786096Z",
     "iopub.status.idle": "2024-09-17T13:12:12.834386Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.786063Z",
     "shell.execute_reply": "2024-09-17T13:12:12.833478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def read_dicom_folder(folder_path):\n",
    "    slices = []\n",
    "    for filename in sorted(os.listdir(folder_path))[:MAX_SLICES]:  # Limit to MAX_SLICES\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            slices.append(ds)\n",
    "    \n",
    "    # Pad with black images if necessary\n",
    "    while len(slices) < MAX_SLICES:\n",
    "        slices.append(np.zeros_like(slices[0].pixel_array))\n",
    "    \n",
    "    return slices[:MAX_SLICES]  # Ensure we return exactly MAX_SLICES"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.835371Z",
     "iopub.execute_input": "2024-09-17T13:12:12.835679Z",
     "iopub.status.idle": "2024-09-17T13:12:12.880681Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.835649Z",
     "shell.execute_reply": "2024-09-17T13:12:12.879837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and testing sets while maintaining the same ratio of labels.\n",
    "\n",
    "    Args:\n",
    "        patient_scan_labels (pd.DataFrame): The DataFrame containing patient scan labels.\n",
    "        test_size (float): The proportion of the dataset to include in the test split.\n",
    "        val_size (float): The proportion of the training set to include in the validation split.\n",
    "        random_state (int): The seed used by the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: train_labels, val_labels, test_labels\n",
    "    \"\"\"\n",
    "    # If any of the hemorrhage indicators is 1, the label is 1, otherwise 0\n",
    "    patient_scan_labels['label'] = patient_scan_labels[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any(axis=1).astype(int)\n",
    "\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['label']\n",
    "\n",
    "    # First, split off the test set\n",
    "    train_val_labels, test_labels = train_test_split(\n",
    "        patient_scan_labels, \n",
    "        test_size=test_size, \n",
    "        stratify=labels, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Calculate the validation size relative to the train_val set\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "    # Split the train_val set into train and validation sets\n",
    "    train_labels, val_labels = train_test_split(\n",
    "        train_val_labels, \n",
    "        test_size=val_size_adjusted, \n",
    "        stratify=train_val_labels['label'], \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_labels, val_labels, test_labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def process_patient_data(dicom_dir, row):\n",
    "    patient_id = row['patient_id'].replace('ID_', '')\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n",
    "    \n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(dicom_dir, folder_name)\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        \n",
    "        # Vì đã có dòng stack rồi nên có thể dòng này không cần thiết\n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Thêm chiều depth\n",
    "        preprocessed_slices = np.stack(preprocessed_slices, axis=0)  # (depth, height, width, channels)\n",
    "        preprocessed_slices = np.transpose(preprocessed_slices, (3, 0, 1, 2))  # (channels, depth, height, width)\n",
    "        \n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        return preprocessed_slices, label\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.881844Z",
     "iopub.execute_input": "2024-09-17T13:12:12.882455Z",
     "iopub.status.idle": "2024-09-17T13:12:12.928171Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.882413Z",
     "shell.execute_reply": "2024-09-17T13:12:12.927277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class TrainDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found\n",
    "\n",
    "class TestDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for testing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.929648Z",
     "iopub.execute_input": "2024-09-17T13:12:12.929932Z",
     "iopub.status.idle": "2024-09-17T13:12:12.980970Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.929900Z",
     "shell.execute_reply": "2024-09-17T13:12:12.980108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE, shuffle=True):\n    train_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels)\n    return DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ndef get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels)\n    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-17T13:12:12.983835Z",
     "iopub.execute_input": "2024-09-17T13:12:12.984133Z",
     "iopub.status.idle": "2024-09-17T13:12:13.026742Z",
     "shell.execute_reply.started": "2024-09-17T13:12:12.984084Z",
     "shell.execute_reply": "2024-09-17T13:12:13.025761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## CNN Feature Extractor",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False).to(device)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _, _ = x.size()\n",
    "    \n",
    "        avg_pool = F.adaptive_avg_pool3d(x, 1).view(batch_size, channels)\n",
    "        max_pool = F.adaptive_max_pool3d(x, 1).view(batch_size, channels)\n",
    "\n",
    "        avg_out = self.fc2(F.relu(self.fc1(avg_pool)))\n",
    "        max_out = self.fc2(F.relu(self.fc1(max_pool)))\n",
    "\n",
    "        out = torch.sigmoid(avg_out + max_out).view(batch_size, channels, 1, 1, 1)\n",
    "        return out * x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Apply channel attention methods\n",
    "        out = ChannelAttention(out.size(1))(out)\n",
    "        \n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResNet3D_MIL(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=2):\n",
    "        super(ResNet3D_MIL, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, channels, depth, height, width)\n",
    "        batch_size, c, d, h, w = x.size()\n",
    "        x = x.view(batch_size, c, d, h, w)\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # MIL aggregation (max pooling over instances)\n",
    "        out = torch.max(out, dim=1)[0]\n",
    "\n",
    "        return out\n",
    "\n",
    "def ResNet3D18_MIL():\n",
    "    return ResNet3D_MIL(ResidualBlock3D, [2, 2, 2, 2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend((outputs > 0).cpu().numpy())\n",
    "            train_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        train_precision = precision_score(train_labels, train_predictions)\n",
    "        train_recall = recall_score(train_labels, train_predictions)\n",
    "        train_f1 = f1_score(train_labels, train_predictions)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "                outputs = model(batch_data).squeeze()\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend((outputs > 0).cpu().numpy())\n",
    "                val_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        val_precision = precision_score(val_labels, val_predictions)\n",
    "        val_recall = recall_score(val_labels, val_predictions)\n",
    "        val_f1 = f1_score(val_labels, val_predictions)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = ResNet3D18_MIL()\n",
    "    \n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    \n",
    "    train_loader = get_train_loader(DATA_DIR, train_labels, batch_size=4)\n",
    "    val_loader = get_train_loader(DATA_DIR, val_labels, batch_size=2)\n",
    "    test_loader = get_test_loader(DATA_DIR, test_labels, batch_size=2)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_model(model, test_loader, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            test_predictions.extend((outputs > 0).cpu().numpy())\n",
    "            test_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    test_precision = precision_score(test_labels, test_predictions)\n",
    "    test_recall = recall_score(test_labels, test_predictions)\n",
    "    test_f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return test_accuracy, test_precision, test_recall, test_f1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "torch.save(trained_model.state_dict(), 'trained_model.pth')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the model and test it on test loader and print the results of classification to a csv \n",
    "\n",
    "# Load the model\n",
    "model = ResNet3D18_MIL()\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_model(model, test_loader)\n",
    "\n",
    "# Save the results to a csv file\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "        outputs = model(batch_data).squeeze()\n",
    "        predictions = (outputs > 0).cpu().numpy()\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            results.append({\n",
    "                'prediction': predictions[i],\n",
    "                'label': batch_labels[i].cpu().numpy()\n",
    "            })\n",
    "            \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results.csv', index=False)\n",
    "\n",
    "results_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_roc_curve(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            all_scores.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.numpy())\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(trained_model, test_loader, device)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
