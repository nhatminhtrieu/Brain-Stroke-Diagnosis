{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.009554Z",
     "start_time": "2024-09-20T03:51:19.180812Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.241205Z",
     "iopub.status.busy": "2024-09-19T12:13:21.240776Z",
     "iopub.status.idle": "2024-09-19T12:13:21.290568Z",
     "shell.execute_reply": "2024-09-19T12:13:21.289633Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.241162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.048757Z",
     "start_time": "2024-09-20T03:51:20.013522Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.292832Z",
     "iopub.status.busy": "2024-09-19T12:13:21.292462Z",
     "iopub.status.idle": "2024-09-19T12:13:21.340372Z",
     "shell.execute_reply": "2024-09-19T12:13:21.339462Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.292800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.098773Z",
     "start_time": "2024-09-20T03:51:20.086548Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.341972Z",
     "iopub.status.busy": "2024-09-19T12:13:21.341562Z",
     "iopub.status.idle": "2024-09-19T12:13:21.388935Z",
     "shell.execute_reply": "2024-09-19T12:13:21.387824Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.341938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.151668Z",
     "start_time": "2024-09-20T03:51:20.133047Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.391598Z",
     "iopub.status.busy": "2024-09-19T12:13:21.391235Z",
     "iopub.status.idle": "2024-09-19T12:13:21.436958Z",
     "shell.execute_reply": "2024-09-19T12:13:21.436075Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.391561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "TEST_BATCH_SIZE = 2\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "\n",
    "MAX_SLICES = 60\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.208198Z",
     "start_time": "2024-09-20T03:51:20.183914Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.438417Z",
     "iopub.status.busy": "2024-09-19T12:13:21.438040Z",
     "iopub.status.idle": "2024-09-19T12:13:21.497520Z",
     "shell.execute_reply": "2024-09-19T12:13:21.496423Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.438374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggle and local switch\n",
    "import os\n",
    "\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "\n",
    "DATA_DIR = '/kaggle/input/rsna-mil-training/' if KAGGLE else '../rsna-mil-training/'\n",
    "DICOM_DIR = DATA_DIR + 'rsna-mil-training'\n",
    "CSV_PATH = DATA_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_1000_scan_subset.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "# Load patient scan labels\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.250466Z",
     "start_time": "2024-09-20T03:51:20.233757Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.499274Z",
     "iopub.status.busy": "2024-09-19T12:13:21.498938Z",
     "iopub.status.idle": "2024-09-19T12:13:21.555792Z",
     "shell.execute_reply": "2024-09-19T12:13:21.554733Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.499237Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    \n",
    "    bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n",
    "    return bsb_img.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.309905Z",
     "start_time": "2024-09-20T03:51:20.284258Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.557641Z",
     "iopub.status.busy": "2024-09-19T12:13:21.557321Z",
     "iopub.status.idle": "2024-09-19T12:13:21.609398Z",
     "shell.execute_reply": "2024-09-19T12:13:21.608348Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.557608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n",
    "    # Check if type of slice is dicom or an empty numpy array\n",
    "    if (type(slice) == np.ndarray):\n",
    "        slice = resize(slice, target_size, anti_aliasing=True)\n",
    "        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n",
    "        return multichannel_slice.astype(np.float16)\n",
    "    else:\n",
    "        slice = bsb_window(slice)\n",
    "        return slice.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.360171Z",
     "start_time": "2024-09-20T03:51:20.336336Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.613480Z",
     "iopub.status.busy": "2024-09-19T12:13:21.613048Z",
     "iopub.status.idle": "2024-09-19T12:13:21.662347Z",
     "shell.execute_reply": "2024-09-19T12:13:21.661352Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.613445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_dicom_folder(folder_path):\n",
    "    slices = []\n",
    "    for filename in sorted(os.listdir(folder_path))[:MAX_SLICES]:  # Limit to MAX_SLICES\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            slices.append(ds)\n",
    "            \n",
    "    # Sort slices by images position (z-coordinate) in ascending order\n",
    "    slices = sorted(slices, key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "    \n",
    "    # Pad with black images if necessary\n",
    "    while len(slices) < MAX_SLICES:\n",
    "        slices.append(np.zeros_like(slices[0].pixel_array))\n",
    "    \n",
    "    return slices[:MAX_SLICES]  # Ensure we return exactly MAX_SLICES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.406483Z",
     "start_time": "2024-09-20T03:51:20.390322Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.663989Z",
     "iopub.status.busy": "2024-09-19T12:13:21.663689Z",
     "iopub.status.idle": "2024-09-19T12:13:21.712399Z",
     "shell.execute_reply": "2024-09-19T12:13:21.711424Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.663957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # If any of the hemorrhage indicators is 1, the label is 1, otherwise 0\n",
    "    patient_scan_labels['label'] = patient_scan_labels[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any(axis=1).astype(int)\n",
    "\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['label']\n",
    "\n",
    "    # First, split off the test set\n",
    "    train_val_labels, test_labels = train_test_split(\n",
    "        patient_scan_labels, \n",
    "        test_size=test_size, \n",
    "        stratify=labels, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Calculate the validation size relative to the train_val set\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "    # Split the train_val set into train and validation sets\n",
    "    train_labels, val_labels = train_test_split(\n",
    "        train_val_labels, \n",
    "        test_size=val_size_adjusted, \n",
    "        stratify=train_val_labels['label'], \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.462378Z",
     "start_time": "2024-09-20T03:51:20.442605Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.714109Z",
     "iopub.status.busy": "2024-09-19T12:13:21.713730Z",
     "iopub.status.idle": "2024-09-19T12:13:21.762040Z",
     "shell.execute_reply": "2024-09-19T12:13:21.761036Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.714075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_patient_data(dicom_dir, row, num_instances=12, depth=5):\n",
    "    patient_id = row['patient_id'].replace('ID_', '')\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n",
    "    \n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(dicom_dir, folder_name)\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        \n",
    "        # Ensure we have enough slices to create the specified instances\n",
    "        if len(slices) < depth * num_instances:\n",
    "            print(f\"Not enough slices for patient {patient_id}: found {len(slices)}, needed {depth * num_instances}\")\n",
    "            return None, None\n",
    "        \n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Stack preprocessed slices into an array\n",
    "        preprocessed_slices = np.stack(preprocessed_slices, axis=0)  # (num_slices, height, width, channels)\n",
    "\n",
    "        # Calculate how many complete instances we can form\n",
    "        total_slices = preprocessed_slices.shape[0]\n",
    "        \n",
    "        # Reshape to (num_instances, depth, height, width, channels)\n",
    "        reshaped_slices = preprocessed_slices[:num_instances * depth].reshape(num_instances, depth, *preprocessed_slices.shape[1:])  # (num_instances, depth, height, width, channels)\n",
    "        \n",
    "        # Labeling remains consistent  \n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        return reshaped_slices, label\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.515816Z",
     "start_time": "2024-09-20T03:51:20.494626Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.763497Z",
     "iopub.status.busy": "2024-09-19T12:13:21.763148Z",
     "iopub.status.idle": "2024-09-19T12:13:21.815641Z",
     "shell.execute_reply": "2024-09-19T12:13:21.814612Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.763464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found\n",
    "\n",
    "class TestDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for testing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            return None, None  # Handle the case where the folder is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.564698Z",
     "start_time": "2024-09-20T03:51:20.550220Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.817340Z",
     "iopub.status.busy": "2024-09-19T12:13:21.816995Z",
     "iopub.status.idle": "2024-09-19T12:13:21.864059Z",
     "shell.execute_reply": "2024-09-19T12:13:21.862994Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.817306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE, shuffle=True):\n",
    "    train_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels)\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.626555Z",
     "start_time": "2024-09-20T03:51:20.602386Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.865756Z",
     "iopub.status.busy": "2024-09-19T12:13:21.865381Z",
     "iopub.status.idle": "2024-09-19T12:13:21.914622Z",
     "shell.execute_reply": "2024-09-19T12:13:21.913667Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.865714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False).to(device)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _, _ = x.size()\n",
    "    \n",
    "        avg_pool = F.adaptive_avg_pool3d(x, 1).view(batch_size, channels)\n",
    "        max_pool = F.adaptive_max_pool3d(x, 1).view(batch_size, channels)\n",
    "\n",
    "        avg_out = self.fc2(F.relu(self.fc1(avg_pool)))\n",
    "        max_out = self.fc2(F.relu(self.fc1(max_pool)))\n",
    "\n",
    "        out = torch.sigmoid(avg_out + max_out).view(batch_size, channels, 1, 1, 1)\n",
    "        return out * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.677026Z",
     "start_time": "2024-09-20T03:51:20.655588Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:21.916473Z",
     "iopub.status.busy": "2024-09-19T12:13:21.916001Z",
     "iopub.status.idle": "2024-09-19T12:13:21.965512Z",
     "shell.execute_reply": "2024-09-19T12:13:21.964624Z",
     "shell.execute_reply.started": "2024-09-19T12:13:21.916427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_rate = 0.25):\n",
    "        super(ResidualBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.dropout = nn.Dropout3d(dropout_rate)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Apply channel attention methods\n",
    "#         out = ChannelAttention(out.size(1))(out)\n",
    "        \n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.727050Z",
     "start_time": "2024-09-20T03:51:20.709848Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:22.016002Z",
     "iopub.status.busy": "2024-09-19T12:13:22.015520Z",
     "iopub.status.idle": "2024-09-19T12:13:22.072772Z",
     "shell.execute_reply": "2024-09-19T12:13:22.071727Z",
     "shell.execute_reply.started": "2024-09-19T12:13:22.015958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNet3D_MIL(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1):\n",
    "        super(ResNet3D_MIL, self).__init__()\n",
    "        self.in_channels = 16\n",
    "\n",
    "        self.conv1 = nn.Conv3d(3, 16, kernel_size=7, stride=(1, 2, 2), padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(16)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1, dropout_rate=0.2)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2, dropout_rate=0.2)\n",
    "        self.layer3 = self._make_layer(block, 32, num_blocks[2], stride=2, dropout_rate=0.3)\n",
    "        self.layer4 = self._make_layer(block, 32, num_blocks[3], stride=2, dropout_rate=0.3)\n",
    "        self.layer5 = self._make_layer(block, 32, num_blocks[4], stride=2, dropout_rate=0.3)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(32, num_classes)\n",
    "#         self.fc = nn.Linear(32 * num_blocks[-1], num_classes)  # Adjust based on number of blocks\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride, dropout_rate = 0.25):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride, dropout_rate))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, channels, depth, height, width)\n",
    "        # print(x.size()) # 4, 12, 5, 224, 224, 3\n",
    "        batch_size, num_instances, d, h, w, c = x.size()\n",
    "        x = x.view(batch_size * num_instances, c, d, h, w)\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        # print(f'out shape after avgpool: {out.size()}')\n",
    "        \n",
    "        # Reshape and aggregate features across instances\n",
    "        out = out.view(batch_size, num_instances, -1)  # Reshape to (batch_size, num_instances, feature_dim)\n",
    "        # print(f'X shape after aggregate features: {out.size()}')\n",
    "        \n",
    "        # Aggregate features (e.g., max pooling across instances)\n",
    "        out = torch.max(out, dim=1)[0]  # Take max across instances\n",
    "        # print(f'out shape after take max across instances: {out.size()}')\n",
    "        \n",
    "        # Final classification layer\n",
    "        # out = torch.flatten(out)  \n",
    "        \n",
    "        # print(f\"x classification layer shape: {out.size()}\")\n",
    "        \n",
    "        return self.fc(out)\n",
    "\n",
    "def ResNet3D18_MIL():\n",
    "    return ResNet3D_MIL(ResidualBlock3D, [1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.771988Z",
     "start_time": "2024-09-20T03:51:20.759496Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:22.074437Z",
     "iopub.status.busy": "2024-09-19T12:13:22.074051Z",
     "iopub.status.idle": "2024-09-19T12:13:22.131486Z",
     "shell.execute_reply": "2024-09-19T12:13:22.130450Z",
     "shell.execute_reply.started": "2024-09-19T12:13:22.074391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, learning_rate=0.001, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "\n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend((outputs > 0).cpu().numpy())\n",
    "            train_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        train_precision = precision_score(train_labels, train_predictions)\n",
    "        train_recall = recall_score(train_labels, train_predictions)\n",
    "        train_f1 = f1_score(train_labels, train_predictions)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data = batch_data.to(device)\n",
    "                batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "                outputs = model(batch_data).squeeze()\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend((outputs > 0).cpu().numpy())\n",
    "                val_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        val_precision = precision_score(val_labels, val_predictions)\n",
    "        val_recall = recall_score(val_labels, val_predictions)\n",
    "        val_f1 = f1_score(val_labels, val_predictions)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.816471Z",
     "start_time": "2024-09-20T03:51:20.805343Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:22.135392Z",
     "iopub.status.busy": "2024-09-19T12:13:22.135054Z",
     "iopub.status.idle": "2024-09-19T12:13:22.186273Z",
     "shell.execute_reply": "2024-09-19T12:13:22.185166Z",
     "shell.execute_reply.started": "2024-09-19T12:13:22.135339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            test_predictions.extend((outputs > 0).cpu().numpy())\n",
    "            test_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    test_precision = precision_score(test_labels, test_predictions)\n",
    "    test_recall = recall_score(test_labels, test_predictions)\n",
    "    test_f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "    return test_accuracy, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.863238Z",
     "start_time": "2024-09-20T03:51:20.851688Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:22.188003Z",
     "iopub.status.busy": "2024-09-19T12:13:22.187661Z",
     "iopub.status.idle": "2024-09-19T12:13:22.236191Z",
     "shell.execute_reply": "2024-09-19T12:13:22.235297Z",
     "shell.execute_reply.started": "2024-09-19T12:13:22.187970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            all_scores.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.numpy())\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T03:51:20.909850Z",
     "start_time": "2024-09-20T03:51:20.898850Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            predictions = (outputs > 0).cpu().numpy()\n",
    "\n",
    "            y_true.extend(batch_labels.cpu().numpy())\n",
    "            y_pred.extend(predictions)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T04:09:55.046197Z",
     "start_time": "2024-09-20T03:51:20.943149Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:13:22.238238Z",
     "iopub.status.busy": "2024-09-19T12:13:22.237380Z",
     "iopub.status.idle": "2024-09-19T12:38:25.829001Z",
     "shell.execute_reply": "2024-09-19T12:38:25.828017Z",
     "shell.execute_reply.started": "2024-09-19T12:13:22.238193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model = ResNet3D18_MIL()\n",
    "    \n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    \n",
    "    train_loader = get_train_loader(DATA_DIR, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(DATA_DIR, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(DATA_DIR, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs=30, learning_rate=0.001)\n",
    "    \n",
    "    # Plot the Confusion Matrix\n",
    "    plot_confusion_matrix(trained_model, test_loader, device)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(trained_model.state_dict(), 'trained_model.pth')\n",
    "    \n",
    "    plot_roc_curve(trained_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T04:10:08.650865Z",
     "start_time": "2024-09-20T04:09:55.055422Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:38:25.831104Z",
     "iopub.status.busy": "2024-09-19T12:38:25.830660Z",
     "iopub.status.idle": "2024-09-19T12:40:06.831321Z",
     "shell.execute_reply": "2024-09-19T12:40:06.830257Z",
     "shell.execute_reply.started": "2024-09-19T12:38:25.831053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model and test it on test loader and print the results of classification to a csv \n",
    "model = ResNet3D18_MIL()\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_model(model, test_loader)\n",
    "\n",
    "# Save the results to a csv file\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    for batch_data, batch_labels in test_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "        outputs = model(batch_data).squeeze()\n",
    "        predictions = (outputs > 0).cpu().numpy()\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            results.append({\n",
    "                'prediction': predictions[i],\n",
    "                'label': batch_labels[i].cpu().numpy()\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results.csv', index=False)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T04:10:11.816954Z",
     "start_time": "2024-09-20T04:10:08.700960Z"
    },
    "execution": {
     "iopub.execute_input": "2024-09-19T12:40:06.833222Z",
     "iopub.status.busy": "2024-09-19T12:40:06.832866Z",
     "iopub.status.idle": "2024-09-19T12:40:16.873611Z",
     "shell.execute_reply": "2024-09-19T12:40:16.872541Z",
     "shell.execute_reply.started": "2024-09-19T12:40:06.833183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = get_test_loader(DATA_DIR, test_labels, batch_size=2)\n",
    "\n",
    "def plot_first_item(dataloader):\n",
    "    first_item = next(iter(dataloader))\n",
    "    \n",
    "    print(f\"Images shape: {first_item[0].size()}\")\n",
    "    images, labels = first_item[0]\n",
    "    \n",
    "    # Reshape images \n",
    "    images = images.squeeze(0)  \n",
    "    images = images.permute(1, 0, 2, 3)  \n",
    "    \n",
    "    num_images = images.size(0)  \n",
    "    images = images.numpy() \n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(8, 8, i + 1) \n",
    "        plt.imshow(images[i][0], cmap='gray') \n",
    "        plt.title(f'Image {i + 1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "# Gọi hàm với test_loader\n",
    "plot_first_item(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5705276,
     "sourceId": 9399351,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
