{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5705276,
     "sourceId": 9399351,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Library",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from typing import Type\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import gpytorch\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:30.317416Z",
     "iopub.execute_input": "2024-09-21T17:49:30.318238Z",
     "iopub.status.idle": "2024-09-21T17:49:34.633268Z",
     "shell.execute_reply.started": "2024-09-21T17:49:30.318197Z",
     "shell.execute_reply": "2024-09-21T17:49:34.632459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Init GPU",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize GPU Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\nelse:\n    print(\"No GPU available. Training will run on CPU.\")\n\nprint(device)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:34.634821Z",
     "iopub.execute_input": "2024-09-21T17:49:34.635253Z",
     "iopub.status.idle": "2024-09-21T17:49:34.723958Z",
     "shell.execute_reply.started": "2024-09-21T17:49:34.635218Z",
     "shell.execute_reply": "2024-09-21T17:49:34.723042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:34.724997Z",
     "iopub.execute_input": "2024-09-21T17:49:34.725294Z",
     "iopub.status.idle": "2024-09-21T17:49:34.769458Z",
     "shell.execute_reply.started": "2024-09-21T17:49:34.725261Z",
     "shell.execute_reply": "2024-09-21T17:49:34.768647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Config Info",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "\n",
    "MAX_SLICES = 60\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# TARGET_LABELS = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "TARGET_LABELS = ['intraparenchymal']\n",
    "\n",
    "MODEL_PATH = 'trained_model.pth'\n",
    "DEVICE = 'cuda'"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:34.772209Z",
     "iopub.execute_input": "2024-09-21T17:49:34.772795Z",
     "iopub.status.idle": "2024-09-21T17:49:34.808724Z",
     "shell.execute_reply.started": "2024-09-21T17:49:34.772761Z",
     "shell.execute_reply": "2024-09-21T17:49:34.807898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "\n",
    "DATA_DIR = '/kaggle/input/' if KAGGLE else '../rsna-mil-training/'\n",
    "DICOM_DIR = DATA_DIR + 'rsna-mil-training/'\n",
    "CSV_PATH = DATA_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_1000_scan_subset.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "# Load patient scan labels\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:34.851522Z",
     "iopub.execute_input": "2024-09-21T17:49:34.851890Z",
     "iopub.status.idle": "2024-09-21T17:49:34.898494Z",
     "shell.execute_reply.started": "2024-09-21T17:49:34.851850Z",
     "shell.execute_reply": "2024-09-21T17:49:34.897585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef window_image(dcm, window_center, window_width):    \n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    \n    # Resize\n    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n   \n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 40, 80)\n    subdural_img = window_image(dcm, 80, 200)\n    soft_img = window_image(dcm, 40, 380)\n    \n    brain_img = (brain_img - 0) / 80\n    subdural_img = (subdural_img - (-20)) / 200\n    soft_img = (soft_img - (-150)) / 380\n    \n    bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n    return bsb_img.astype(np.float16)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:34.899639Z",
     "iopub.execute_input": "2024-09-21T17:49:34.899918Z",
     "iopub.status.idle": "2024-09-21T17:49:34.942493Z",
     "shell.execute_reply.started": "2024-09-21T17:49:34.899887Z",
     "shell.execute_reply": "2024-09-21T17:49:34.941247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n    # Check if type of slice is dicom or an empty numpy array\n    if (type(slice) == np.ndarray):\n        slice = resize(slice, target_size, anti_aliasing=True)\n        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n        return multichannel_slice.astype(np.float16)\n    else:\n        slice = bsb_window(slice)\n        return slice.astype(np.float16)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:34.944152Z",
     "iopub.execute_input": "2024-09-21T17:49:34.944568Z",
     "iopub.status.idle": "2024-09-21T17:49:34.988714Z",
     "shell.execute_reply.started": "2024-09-21T17:49:34.944513Z",
     "shell.execute_reply": "2024-09-21T17:49:34.987480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def read_dicom_folder(folder_path):\n    slices = []\n    for filename in sorted(os.listdir(folder_path))[:MAX_SLICES]:  # Limit to MAX_SLICES\n        if filename.endswith(\".dcm\"):\n            file_path = os.path.join(folder_path, filename)\n            ds = pydicom.dcmread(file_path)\n            slices.append(ds)\n            \n    # Sort slices by images position (z-coordinate) in ascending order\n    slices = sorted(slices, key=lambda x: float(x.ImagePositionPatient[2]))\n    \n    # Pad with black images if necessary\n    while len(slices) < MAX_SLICES:\n        slices.append(np.zeros_like(slices[0].pixel_array))\n    \n    return slices[:MAX_SLICES]  # Ensure we return exactly MAX_SLICES",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:34.990731Z",
     "iopub.execute_input": "2024-09-21T17:49:34.991508Z",
     "iopub.status.idle": "2024-09-21T17:49:35.039115Z",
     "shell.execute_reply.started": "2024-09-21T17:49:34.991462Z",
     "shell.execute_reply": "2024-09-21T17:49:35.038095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset and DataLoader",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Splitting the Dataset"
  },
  {
   "cell_type": "code",
   "source": "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n    # If any of the hemorrhage indicators is 1, the label is 1, otherwise 0\n    patient_scan_labels['label'] = patient_scan_labels[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any(axis=1).astype(int)\n\n    # Extract the labels from the DataFrame\n    labels = patient_scan_labels['label']\n\n    # First, split off the test set\n    train_val_labels, test_labels = train_test_split(\n        patient_scan_labels, \n        test_size=test_size, \n        stratify=labels, \n        random_state=random_state\n    )\n\n    # Calculate the validation size relative to the train_val set\n    val_size_adjusted = val_size / (1 - test_size)\n\n    # Split the train_val set into train and validation sets\n    train_labels, val_labels = train_test_split(\n        train_val_labels, \n        test_size=val_size_adjusted, \n        stratify=train_val_labels['label'], \n        random_state=random_state\n    )\n\n    return train_labels, val_labels, test_labels",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:35.044082Z",
     "iopub.execute_input": "2024-09-21T17:49:35.044395Z",
     "iopub.status.idle": "2024-09-21T17:49:35.086247Z",
     "shell.execute_reply.started": "2024-09-21T17:49:35.044363Z",
     "shell.execute_reply": "2024-09-21T17:49:35.085062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Processing the Data"
  },
  {
   "cell_type": "code",
   "source": [
    "def process_patient_data(dicom_dir, row, num_instances=12, depth=5):\n",
    "    patient_id = row['patient_id'].replace('ID_', '')\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n",
    "    \n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(dicom_dir, folder_name)\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        \n",
    "        # Ensure we have enough slices to create the specified instances\n",
    "        if len(slices) < depth * num_instances:\n",
    "            print(f\"Not enough slices for patient {patient_id}: found {len(slices)}, needed {depth * num_instances}\")\n",
    "            return None, None\n",
    "        \n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Stack preprocessed slices into an array\n",
    "        preprocessed_slices = np.stack(preprocessed_slices, axis=0)  # (num_slices, height, width, channels)\n",
    "        \n",
    "        # Reshape to (num_instances, depth, height, width, channels)\n",
    "        reshaped_slices = preprocessed_slices[:num_instances * depth].reshape(num_instances, depth, *preprocessed_slices.shape[1:])  # (num_instances, depth, height, width, channels)\n",
    "        \n",
    "        # Labeling remains consistent  \n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        return reshaped_slices, label\n",
    "    \n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:35.087470Z",
     "iopub.execute_input": "2024-09-21T17:49:35.087871Z",
     "iopub.status.idle": "2024-09-21T17:49:35.132772Z",
     "shell.execute_reply.started": "2024-09-21T17:49:35.087821Z",
     "shell.execute_reply": "2024-09-21T17:49:35.131784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class TrainDatasetGenerator(Dataset):\n    \"\"\"\n    A custom dataset class for training data.\n    \"\"\"\n    def __init__(self, data_dir, patient_scan_labels):\n        self.data_dir = data_dir\n        self.patient_scan_labels = patient_scan_labels\n\n    def __len__(self):\n        return len(self.patient_scan_labels)\n\n    def __getitem__(self, idx):\n        row = self.patient_scan_labels.iloc[idx]\n        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n        \n        if preprocessed_slices is not None:\n            # Convert the list of numpy arrays to a single numpy array\n            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n        else:\n            return None, None  # Handle the case where the folder is not found\n\nclass TestDatasetGenerator(Dataset):\n    \"\"\"\n    A custom dataset class for testing data.\n    \"\"\"\n    def __init__(self, data_dir, patient_scan_labels):\n        self.data_dir = data_dir\n        self.patient_scan_labels = patient_scan_labels\n\n    def __len__(self):\n        return len(self.patient_scan_labels)\n\n    def __getitem__(self, idx):\n        row = self.patient_scan_labels.iloc[idx]\n        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n        \n        if preprocessed_slices is not None:\n            # Convert the list of numpy arrays to a single numpy array\n            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n        else:\n            return None, None  # Handle the case where the folder is not found",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:35.133936Z",
     "iopub.execute_input": "2024-09-21T17:49:35.134285Z",
     "iopub.status.idle": "2024-09-21T17:49:35.184295Z",
     "shell.execute_reply.started": "2024-09-21T17:49:35.134251Z",
     "shell.execute_reply": "2024-09-21T17:49:35.183360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE, shuffle=True):\n    train_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels)\n    return DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ndef get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels)\n    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:35.186353Z",
     "iopub.execute_input": "2024-09-21T17:49:35.186847Z",
     "iopub.status.idle": "2024-09-21T17:49:35.227411Z",
     "shell.execute_reply.started": "2024-09-21T17:49:35.186803Z",
     "shell.execute_reply": "2024-09-21T17:49:35.226425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## CNN Feature Extractor",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention Layer"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.fc = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = torch.softmax(self.fc(x), dim=1)  # Shape: (batch_size, num_instances, 1)\n",
    "        weighted_sum = torch.sum(attention_weights * x, dim=1)  # Shape: (batch_size, feature_dim)\n",
    "        return weighted_sum, attention_weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GatedAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(GatedAttentionLayer, self).__init__()\n",
    "        self.fc_attention = nn.Linear(in_channels, 1)\n",
    "        self.fc_gate = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = torch.softmax(self.fc_attention(x), dim=1)  # Shape: (batch_size, num_instances, 1)\n",
    "        gate_weights = torch.sigmoid(self.fc_gate(x))  # Shape: (batch_size, num_instances, 1)\n",
    "\n",
    "        gated_attention = attention_weights * gate_weights\n",
    "        weighted_sum = torch.sum(gated_attention * x, dim=1)  # Shape: (batch_size, feature_dim)\n",
    "        return weighted_sum, gated_attention"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sparse Gaussian Process Layer"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SparseGPlayer(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        inducing_points = inducing_points.to(device)\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(0)\n",
    "        )\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(SparseGPlayer, self).__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = gpytorch.means.ConstantMean().to(device)\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        \n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ResNet3D Model"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Basic Block"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class BasicBlock3D(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        expansion: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        dropout_rate: float = 0.25\n",
    "    ) -> None:\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=3, \n",
    "            stride=stride, \n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout3d(dropout_rate)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            out_channels, \n",
    "            out_channels*self.expansion, \n",
    "            kernel_size=3, \n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels*self.expansion)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ResNet3D Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ResNet3D_MIL(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_channels: int,\n",
    "        num_layers: int,\n",
    "        block: Type[BasicBlock3D],\n",
    "        num_classes: int = 1,\n",
    "        dropout_rate: float = 0.25,\n",
    "        num_introducing_points = 100\n",
    "    ) -> None:\n",
    "        super(ResNet3D_MIL, self).__init__()\n",
    "        \n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "            self.expansion = 1\n",
    "        elif num_layers == 34:\n",
    "            layers = [3, 4, 6, 3]\n",
    "            self.expansion = 1\n",
    "        elif num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "            self.expansion = 4\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported number of layers\")\n",
    "        \n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels=img_channels,\n",
    "            out_channels=self.in_channels,\n",
    "            kernel_size=7, \n",
    "            stride=(1, 2, 2),\n",
    "            padding=3,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1, dropout_rate=dropout_rate)\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2, dropout_rate=dropout_rate)\n",
    "        self.layer3 = self._make_layer(block, 32, layers[2], stride=2, dropout_rate=dropout_rate)\n",
    "        self.layer4 = self._make_layer(block, 32, layers[3], stride=2, dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(32*self.expansion, num_classes)\n",
    "        \n",
    "        self.attention_layer = GatedAttentionLayer(32*self.expansion)\n",
    "        \n",
    "        # Inducing points for Sparse Gaussian Process (example size; adjust as needed)\n",
    "        inducing_points_size = num_introducing_points\n",
    "        inducing_points_tensor = torch.randn(inducing_points_size , 32).to(device)\n",
    "        # Initialize Sparse Gaussian Process model with inducing points \n",
    "        self.sparse_gp_model = SparseGPlayer(inducing_points_tensor).to(device)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, \n",
    "        block: Type[BasicBlock3D],\n",
    "        out_channels: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dropout_rate: float = 0.25\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * self.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(\n",
    "                    self.in_channels, \n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False \n",
    "                ),\n",
    "                nn.BatchNorm3d(out_channels * self.expansion),\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels, out_channels, stride, self.expansion, downsample, dropout_rate\n",
    "            )\n",
    "        )\n",
    "        self.in_channels = out_channels * self.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(\n",
    "                self.in_channels,\n",
    "                out_channels,\n",
    "                expansion=self.expansion,\n",
    "                dropout_rate=dropout_rate\n",
    "            ))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: (batch_size, num_instances, channels, height, width)\n",
    "        batch_size, num_instances, d, h, w, c = x.size()\n",
    "        # print(f'x shape: {x.size()}')\n",
    "        \n",
    "        # Reshape to (batch_size * num_instances, channels, height, width)\n",
    "        out = x.view(batch_size * num_instances, c, d, h, w)\n",
    "        # print(f'x shape after view: {out.size()}')\n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        # print(f'out shape after avgpool: {out.size()}')\n",
    "        \n",
    "        # Reshape back to (batch_size, num_instances, features)\n",
    "        out = out.view(batch_size, num_instances, -1)\n",
    "        # print(f'out shape after view: {out.size()}')\n",
    "        \n",
    "        # Apply attention layer\n",
    "        # out, attention_weights = self.attention_layer(out)\n",
    "        \n",
    "        out = torch.max(out, dim=1)[0]  # Take max across instances\n",
    "        \n",
    "        # # Prepare input for Sparse GP model (requires reshaping) \n",
    "        # gp_input=torch.randn(batch_size, num_instances , 32).to(device)  \n",
    "        # \n",
    "        # # Print shape of GP input\n",
    "        # print(f'GP input shape: {gp_input.size()}')\n",
    "        # \n",
    "        # gp_output=self.sparse_gp_model(gp_input)\n",
    "        # \n",
    "        # # Use GP output mean (or any other desired statistic) as additional features \n",
    "        # gp_mean_features=gp_output.mean\n",
    "        # \n",
    "        # # Print shape of out and GP mean features\n",
    "        # print(f'out shape: {out.size()}')\n",
    "        # print(f'GP mean features shape: {gp_mean_features.size()}')\n",
    "        # # Print shape gp unsqueezed\n",
    "        # print(f'GP mean features unsqueezed shape: {gp_mean_features.unsqueeze(0).size()}')\n",
    "        # \n",
    "        # # Concatenate GP features with attention output \n",
    "        # combined_out=torch.cat((out , gp_mean_features), dim=-1)\n",
    "        # \n",
    "        # # Apply final fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet3D18_MIL(img_channels: int = 3, num_classes: int = 1):\n",
    "    return ResNet3D_MIL(img_channels, 18, BasicBlock3D, num_classes)\n",
    "\n",
    "def ResNet3D34_MIL(img_channels: int = 3, num_classes: int = 1):\n",
    "    return ResNet3D_MIL(img_channels, 34, BasicBlock3D, num_classes)\n",
    "\n",
    "def ResNet3D50_MIL(img_channels: int = 3, num_classes: int = 1):\n",
    "    return ResNet3D_MIL(img_channels, 50, BasicBlock3D, num_classes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training"
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for batch_data, batch_labels in tqdm(data_loader, desc=\"Training\"):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data).squeeze()\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions.extend((outputs > 0).cpu().numpy())\n",
    "        labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in tqdm(data_loader, desc=\"Validating\"):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predictions.extend((outputs > 0).cpu().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, learning_rate, device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "                                              steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, val_loader, criterion, device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:35.376948Z",
     "iopub.execute_input": "2024-09-21T17:49:35.377526Z",
     "iopub.status.idle": "2024-09-21T17:49:35.433092Z",
     "shell.execute_reply.started": "2024-09-21T17:49:35.377484Z",
     "shell.execute_reply": "2024-09-21T17:49:35.431823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation Functions"
  },
  {
   "cell_type": "code",
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            predictions.extend((outputs > 0).cpu().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:35.434552Z",
     "iopub.execute_input": "2024-09-21T17:49:35.434931Z",
     "iopub.status.idle": "2024-09-21T17:49:35.479170Z",
     "shell.execute_reply.started": "2024-09-21T17:49:35.434875Z",
     "shell.execute_reply": "2024-09-21T17:49:35.477974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualization Functions"
  },
  {
   "cell_type": "code",
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T17:49:35.480257Z",
     "iopub.execute_input": "2024-09-21T17:49:35.480810Z",
     "iopub.status.idle": "2024-09-21T17:49:35.521559Z",
     "shell.execute_reply.started": "2024-09-21T17:49:35.480778Z",
     "shell.execute_reply": "2024-09-21T17:49:35.520725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Processing Functions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class()\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader)\n",
    "    \n",
    "    results = []\n",
    "    for i, (patient_id, study_instance_uid, true_label) in enumerate(test_labels.itertuples(index=False)):\n",
    "        results.append({\n",
    "            'patient_id': patient_id,\n",
    "            'study_instance_uid': study_instance_uid,\n",
    "            'prediction': predictions[i],\n",
    "            'label': true_label\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Main Execution\n",
    "def main():\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = ResNet3D18_MIL()  # Assuming this is your model class\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, LEARNING_RATE, DEVICE)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # Load best model\n",
    "    trained_model = load_model(ResNet3D18_MIL, MODEL_PATH)\n",
    "\n",
    "    # Evaluate model\n",
    "    predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    metrics = calculate_metrics(predictions, labels)\n",
    "    print_metrics(metrics)\n",
    "\n",
    "    # Visualizations\n",
    "    plot_roc_curve(trained_model, test_loader, DEVICE)\n",
    "    plot_confusion_matrix(trained_model, test_loader, DEVICE)\n",
    "\n",
    "    # Save results\n",
    "    results_df = get_test_results(trained_model, test_loader, test_labels)\n",
    "    results_df.to_csv('results.csv', index=False)\n",
    "    print(results_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualizing Attention Weights and Images"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Show Attention Weights and Images for a Single Patient"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to test a single patient\n",
    "def test_single_patient(model, patient_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in patient_data:\n",
    "            batch_data = batch_data.to(device)  # Move data to device\n",
    "            outputs, gated_attention = model(batch_data)  # Get outputs and attention weights\n",
    "            outputs = outputs.squeeze()  # Remove extra dimension\n",
    "            \n",
    "            predictions = (outputs > 0).cpu().numpy()\n",
    "            \n",
    "            print(\"Predictions:\", predictions)\n",
    "            print(\"Labels: \", batch_labels.cpu().numpy())\n",
    "            # Print attention weights\n",
    "            attention_weights_np = gated_attention.cpu().numpy()  # Move to CPU for easier handling\n",
    "            print(\"Attention Weights for Slides:\")\n",
    "            \n",
    "            for i in range(attention_weights_np.shape[1]):  # Iterate over instances/slides\n",
    "                print(f'Slide {i+1}: Attention Weight: {attention_weights_np[0][i][0]}')  # Print weight for each slide\n",
    "                \n",
    "            # Plot the images\n",
    "            plot_images(batch_data[0].cpu(), batch_labels[0].cpu())\n",
    "            \n",
    "            break  # Only test one patient"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_images(images, labels):\n",
    "    print(images.shape)\n",
    "    batch_size, num_images_per_batch, height, width, channels = images.shape\n",
    "    images = images.view(batch_size * num_images_per_batch, height, width, channels)\n",
    "    \n",
    "    print(f'Images shape: {images.shape}')\n",
    "    num_images = images.size(0) \n",
    "    images = images.numpy()\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(f'Image {i + 1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    \n",
    "# # Call the function with the test_loader\n",
    "# test_loader = get_test_loader(dicom_dir, test_labels, batch_size=2)\n",
    "# test_single_patient(trained_model, test_loader)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-21T18:02:50.799081Z",
     "iopub.execute_input": "2024-09-21T18:02:50.799908Z",
     "iopub.status.idle": "2024-09-21T18:03:00.508242Z",
     "shell.execute_reply.started": "2024-09-21T18:02:50.799851Z",
     "shell.execute_reply": "2024-09-21T18:03:00.507293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
