{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e112c5",
   "metadata": {
    "papermill": {
     "duration": 0.015487,
     "end_time": "2024-10-04T19:23:30.290610",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.275123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "id": "89f70fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:23:30.320852Z",
     "iopub.status.busy": "2024-10-04T19:23:30.320493Z",
     "iopub.status.idle": "2024-10-04T19:23:44.145288Z",
     "shell.execute_reply": "2024-10-04T19:23:44.144289Z"
    },
    "papermill": {
     "duration": 13.842163,
     "end_time": "2024-10-04T19:23:44.147775",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.305612",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:03.383064Z",
     "start_time": "2024-10-17T15:18:02.764424Z"
    }
   },
   "source": [
    "!pip install gpytorch"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.24.3)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "73bbd2bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:23:44.179787Z",
     "iopub.status.busy": "2024-10-04T19:23:44.179449Z",
     "iopub.status.idle": "2024-10-04T19:23:55.953829Z",
     "shell.execute_reply": "2024-10-04T19:23:55.952913Z"
    },
    "papermill": {
     "duration": 11.793387,
     "end_time": "2024-10-04T19:23:55.956283",
     "exception": false,
     "start_time": "2024-10-04T19:23:44.162896",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:03.404858Z",
     "start_time": "2024-10-17T15:18:03.386470Z"
    }
   },
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from typing import Type\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torchvision.models as models\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "from torchvision import transforms\n",
    "from scipy.ndimage import rotate\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "0daf5d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:23:55.991702Z",
     "iopub.status.busy": "2024-10-04T19:23:55.990634Z",
     "iopub.status.idle": "2024-10-04T19:23:55.995892Z",
     "shell.execute_reply": "2024-10-04T19:23:55.995016Z"
    },
    "papermill": {
     "duration": 0.02447,
     "end_time": "2024-10-04T19:23:55.997833",
     "exception": false,
     "start_time": "2024-10-04T19:23:55.973363",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:03.448130Z",
     "start_time": "2024-10-17T15:18:03.430727Z"
    }
   },
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "1ad1ee3a",
   "metadata": {
    "papermill": {
     "duration": 0.024749,
     "end_time": "2024-10-04T19:23:56.038412",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.013663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Init GPU"
   ]
  },
  {
   "cell_type": "code",
   "id": "25c728bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:23:56.071941Z",
     "iopub.status.busy": "2024-10-04T19:23:56.071173Z",
     "iopub.status.idle": "2024-10-04T19:23:56.157322Z",
     "shell.execute_reply": "2024-10-04T19:23:56.156313Z"
    },
    "papermill": {
     "duration": 0.105269,
     "end_time": "2024-10-04T19:23:56.159555",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.054286",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:03.504058Z",
     "start_time": "2024-10-17T15:18:03.486884Z"
    }
   },
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "7c58e1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:23:56.194557Z",
     "iopub.status.busy": "2024-10-04T19:23:56.194156Z",
     "iopub.status.idle": "2024-10-04T19:23:56.256164Z",
     "shell.execute_reply": "2024-10-04T19:23:56.255175Z"
    },
    "papermill": {
     "duration": 0.081958,
     "end_time": "2024-10-04T19:23:56.258405",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.176447",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:03.567267Z",
     "start_time": "2024-10-17T15:18:03.544867Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "3d70740f",
   "metadata": {
    "papermill": {
     "duration": 0.016234,
     "end_time": "2024-10-04T19:23:56.291671",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.275437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Config Info"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9de4a9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:23:56.325514Z",
     "iopub.status.busy": "2024-10-04T19:23:56.325087Z",
     "iopub.status.idle": "2024-10-04T19:23:56.382086Z",
     "shell.execute_reply": "2024-10-04T19:23:56.381116Z"
    },
    "papermill": {
     "duration": 0.076674,
     "end_time": "2024-10-04T19:23:56.384583",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.307909",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:03.622140Z",
     "start_time": "2024-10-17T15:18:03.607763Z"
    }
   },
   "source": [
    "# Constants\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 3\n",
    "\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "TEST_BATCH_SIZE = 2\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "\n",
    "MAX_SLICES = 60\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "INDUCING_POINTS = 128\n",
    "\n",
    "# TARGET_LABELS = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "TARGET_LABELS = ['intraparenchymal']\n",
    "\n",
    "MODEL_PATH = 'results/trained_model.pth'\n",
    "DEVICE = 'cuda'"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "0243af16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:23:56.419172Z",
     "iopub.status.busy": "2024-10-04T19:23:56.418830Z",
     "iopub.status.idle": "2024-10-04T19:24:00.484735Z",
     "shell.execute_reply": "2024-10-04T19:24:00.483612Z"
    },
    "papermill": {
     "duration": 4.085647,
     "end_time": "2024-10-04T19:24:00.487263",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.401616",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:04.662575Z",
     "start_time": "2024-10-17T15:18:03.658733Z"
    }
   },
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "ROOT_DIR = '/kaggle/input/' if KAGGLE else None\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-mil-training/'\n",
    "DICOM_DIR = DATA_DIR + 'rsna-mil-training/'\n",
    "CSV_PATH = DATA_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_1000_scan_subset.csv'\n",
    "SLICE_LABEL_PATH = ROOT_DIR + \"sorted_training_dataset_with_labels.csv\" if KAGGLE else './data_analyze/sorted_training_dataset_with_labels.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "# Load patient scan labels\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "patient_slice_labels = pd.read_csv(SLICE_LABEL_PATH)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "52751957",
   "metadata": {
    "papermill": {
     "duration": 0.016012,
     "end_time": "2024-10-04T19:24:00.520265",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.504253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ae8bf58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:00.555576Z",
     "iopub.status.busy": "2024-10-04T19:24:00.555209Z",
     "iopub.status.idle": "2024-10-04T19:24:00.614877Z",
     "shell.execute_reply": "2024-10-04T19:24:00.613997Z"
    },
    "papermill": {
     "duration": 0.079596,
     "end_time": "2024-10-04T19:24:00.617176",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.537580",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:04.735677Z",
     "start_time": "2024-10-17T15:18:04.712404Z"
    }
   },
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    \n",
    "    bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n",
    "    return bsb_img.astype(np.float16)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "305ee3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:00.652784Z",
     "iopub.status.busy": "2024-10-04T19:24:00.651800Z",
     "iopub.status.idle": "2024-10-04T19:24:00.708600Z",
     "shell.execute_reply": "2024-10-04T19:24:00.707793Z"
    },
    "papermill": {
     "duration": 0.076531,
     "end_time": "2024-10-04T19:24:00.710787",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.634256",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:04.782508Z",
     "start_time": "2024-10-17T15:18:04.761221Z"
    }
   },
   "source": [
    "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n",
    "    # Check if type of slice is dicom or an empty numpy array\n",
    "    if (type(slice) == np.ndarray):\n",
    "        slice = resize(slice, target_size, anti_aliasing=True)\n",
    "        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n",
    "        return multichannel_slice.astype(np.float16)\n",
    "    else:\n",
    "        slice = bsb_window(slice)\n",
    "        return slice.astype(np.float16)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "40f59631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:00.745478Z",
     "iopub.status.busy": "2024-10-04T19:24:00.744794Z",
     "iopub.status.idle": "2024-10-04T19:24:00.801023Z",
     "shell.execute_reply": "2024-10-04T19:24:00.800038Z"
    },
    "papermill": {
     "duration": 0.076142,
     "end_time": "2024-10-04T19:24:00.803356",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.727214",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:04.826689Z",
     "start_time": "2024-10-17T15:18:04.808293Z"
    }
   },
   "source": [
    "def read_dicom_folder(folder_path):\n",
    "    slices = []\n",
    "    for filename in sorted(os.listdir(folder_path))[:MAX_SLICES]:  # Limit to MAX_SLICES\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            slices.append(ds)\n",
    "            \n",
    "    # Sort slices by images position (z-coordinate) in ascending order\n",
    "    slices = sorted(slices, key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "    \n",
    "    # Pad with black images if necessary\n",
    "    while len(slices) < MAX_SLICES:\n",
    "        slices.append(np.zeros_like(slices[0].pixel_array))\n",
    "    \n",
    "    return slices[:MAX_SLICES]  # Ensure we return exactly MAX_SLICES"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "fab612ea",
   "metadata": {
    "papermill": {
     "duration": 0.016487,
     "end_time": "2024-10-04T19:24:00.836476",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.819989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8d017",
   "metadata": {
    "papermill": {
     "duration": 0.015844,
     "end_time": "2024-10-04T19:24:00.868235",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.852391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "127a0a5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:00.901894Z",
     "iopub.status.busy": "2024-10-04T19:24:00.901275Z",
     "iopub.status.idle": "2024-10-04T19:24:00.957617Z",
     "shell.execute_reply": "2024-10-04T19:24:00.956836Z"
    },
    "papermill": {
     "duration": 0.075616,
     "end_time": "2024-10-04T19:24:00.959889",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.884273",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:04.872382Z",
     "start_time": "2024-10-17T15:18:04.855789Z"
    }
   },
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # If any of the hemorrhage indicators is 1, the label is 1, otherwise 0\n",
    "    patient_scan_labels['label'] = patient_scan_labels[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any(axis=1).astype(int)\n",
    "\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['label']\n",
    "\n",
    "    # First, split off the test set\n",
    "    train_val_labels, test_labels = train_test_split(\n",
    "        patient_scan_labels, \n",
    "        test_size=test_size, \n",
    "        stratify=labels, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Calculate the validation size relative to the train_val set\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "    # Split the train_val set into train and validation sets\n",
    "    train_labels, val_labels = train_test_split(\n",
    "        train_val_labels, \n",
    "        test_size=val_size_adjusted, \n",
    "        stratify=train_val_labels['label'], \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_labels, val_labels, test_labels"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "1e702879",
   "metadata": {
    "papermill": {
     "duration": 0.017012,
     "end_time": "2024-10-04T19:24:00.993325",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.976313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "c90f3ad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:01.026944Z",
     "iopub.status.busy": "2024-10-04T19:24:01.026563Z",
     "iopub.status.idle": "2024-10-04T19:24:01.083961Z",
     "shell.execute_reply": "2024-10-04T19:24:01.083029Z"
    },
    "papermill": {
     "duration": 0.076813,
     "end_time": "2024-10-04T19:24:01.086193",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.009380",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:04.920468Z",
     "start_time": "2024-10-17T15:18:04.902868Z"
    }
   },
   "source": [
    "def process_patient_data(dicom_dir, row, num_instances=12, depth=5):\n",
    "    patient_id = row['patient_id'].replace('ID_', '')\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n",
    "    \n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(dicom_dir, folder_name)\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        \n",
    "        # Ensure we have enough slices to create the specified instances\n",
    "        if len(slices) < depth * num_instances:\n",
    "            print(f\"Not enough slices for patient {patient_id}: found {len(slices)}, needed {depth * num_instances}\")\n",
    "            return None, None\n",
    "        \n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Stack preprocessed slices into an array\n",
    "        preprocessed_slices = np.stack(preprocessed_slices, axis=0)  # (num_slices, height, width, channels)\n",
    "        \n",
    "        # Reshape to (num_instances, depth, height, width, channels)\n",
    "        # reshaped_slices = preprocessed_slices[:num_instances * depth].reshape(num_instances, depth, *preprocessed_slices.shape[1:])  # (num_instances, depth, height, width, channels)\n",
    "        \n",
    "        # Labeling remains consistent  \n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        # return reshaped_slices, label\n",
    "        return preprocessed_slices, label\n",
    "    \n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "2ed41314",
   "metadata": {
    "papermill": {
     "duration": 0.016194,
     "end_time": "2024-10-04T19:24:01.118549",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.102355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "id": "722310c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:01.152058Z",
     "iopub.status.busy": "2024-10-04T19:24:01.151687Z",
     "iopub.status.idle": "2024-10-04T19:24:01.217970Z",
     "shell.execute_reply": "2024-10-04T19:24:01.217160Z"
    },
    "papermill": {
     "duration": 0.085905,
     "end_time": "2024-10-04T19:24:01.220399",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.134494",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:04.971413Z",
     "start_time": "2024-10-17T15:18:04.953205Z"
    }
   },
   "source": [
    "# class RandomRotation2D:\n",
    "#     def __init__(self, degrees):\n",
    "#         self.degrees = degrees\n",
    "# \n",
    "#     def __call__(self, sample):\n",
    "#         angle = np.random.uniform(-self.degrees, self.degrees)\n",
    "#         return rotate(sample, angle, axes=(1, 2), reshape=False, order=1, mode='constant', cval=0)\n",
    "# \n",
    "# class RandomFlip2D:\n",
    "#     def __init__(self, horizontal=True):\n",
    "#         self.horizontal = horizontal\n",
    "# \n",
    "#     def __call__(self, sample):\n",
    "#         if np.random.random() > 0.5:\n",
    "#             if self.horizontal:\n",
    "#                 return np.flip(sample, axis=2).copy()  # Flip horizontally\n",
    "#             else:\n",
    "#                 return np.flip(sample, axis=1).copy()  # Flip vertically\n",
    "#         return sample\n",
    "#     \n",
    "# class RandomNoise2D:\n",
    "#     def __init__(self, mean=0, std=0.01):\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "# \n",
    "#     def __call__(self, sample):\n",
    "#         noise = np.random.normal(self.mean, self.std, sample.shape)\n",
    "#         return np.clip(sample + noise, 0, 1)\n",
    "# \n",
    "# class RandomBrightnessContrast2D:\n",
    "#     def __init__(self, brightness=0.1, contrast=0.1):\n",
    "#         self.brightness = brightness\n",
    "#         self.contrast = contrast\n",
    "# \n",
    "#     def __call__(self, sample):\n",
    "#         alpha = 1.0 + np.random.uniform(-self.contrast, self.contrast)\n",
    "#         beta = np.random.uniform(-self.brightness, self.brightness)\n",
    "#         return np.clip(alpha * sample + beta, 0, 1)\n",
    "# \n",
    "# class RandomBlur2D:\n",
    "#     def __init__(self, sigma=(0.1, 1.0)):\n",
    "#         self.sigma = sigma\n",
    "# \n",
    "#     def __call__(self, sample):\n",
    "#         sigma = np.random.uniform(self.sigma[0], self.sigma[1])\n",
    "#         return gaussian_filter(sample, sigma=sigma)\n",
    "# \n",
    "# def get_augmentation_transform():\n",
    "#     return transforms.Compose([\n",
    "#         RandomRotation2D(degrees=10),\n",
    "#         RandomFlip2D(),\n",
    "#         RandomNoise2D(mean=0, std=0.01),\n",
    "#         RandomBrightnessContrast2D(brightness=0.05, contrast=0.05),\n",
    "#         RandomBlur2D(sigma=(0.1, 0.5)),\n",
    "#         transforms.Lambda(lambda x: torch.tensor(x).float())\n",
    "#     ])\n",
    "# \n",
    "# class AugmentedDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, original_dataset, transform=None, aug_prob=0.5):\n",
    "#         self.original_dataset = original_dataset\n",
    "#         self.transform = transform\n",
    "#         self.aug_prob = aug_prob\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return len(self.original_dataset)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         sample, label, patient_study_instaces = self.original_dataset[idx]\n",
    "#         if self.transform and np.random.random() < self.aug_prob:\n",
    "#             sample = self.transform(sample)\n",
    "#         return sample, label, patient_study_instaces"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "b4436645",
   "metadata": {
    "papermill": {
     "duration": 0.015494,
     "end_time": "2024-10-04T19:24:01.252123",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.236629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "id": "f48cbeb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:01.285710Z",
     "iopub.status.busy": "2024-10-04T19:24:01.285304Z",
     "iopub.status.idle": "2024-10-04T19:24:01.345104Z",
     "shell.execute_reply": "2024-10-04T19:24:01.344295Z"
    },
    "papermill": {
     "duration": 0.079407,
     "end_time": "2024-10-04T19:24:01.347420",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.268013",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.019199Z",
     "start_time": "2024-10-17T15:18:05.001174Z"
    }
   },
   "source": [
    "class TrainDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        patient_study_instance = row['patient_id'] + '_' + row['study_instance_uid']\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long), patient_study_instance\n",
    "        else:\n",
    "            return None, None, None  # Handle the case where the folder is not found\n",
    "\n",
    "class TestDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for testing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        patient_study_instance = row['patient_id'] + '_' + row['study_instance_uid']\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long), patient_study_instance\n",
    "        else:\n",
    "            return None, None, None  # Handle the case where the folder is not found"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "31362283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:01.381847Z",
     "iopub.status.busy": "2024-10-04T19:24:01.381433Z",
     "iopub.status.idle": "2024-10-04T19:24:01.436483Z",
     "shell.execute_reply": "2024-10-04T19:24:01.435666Z"
    },
    "papermill": {
     "duration": 0.074769,
     "end_time": "2024-10-04T19:24:01.438666",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.363897",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.061851Z",
     "start_time": "2024-10-17T15:18:05.046460Z"
    }
   },
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels)\n",
    "    # augmented_dataset = AugmentedDataset(original_dataset, transform=get_augmentation_transform())\n",
    "    # return DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "aae33ddb",
   "metadata": {
    "papermill": {
     "duration": 0.015548,
     "end_time": "2024-10-04T19:24:01.470464",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.454916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CNN Feature Extractor"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GP Model",
   "id": "a7fbd4fe4f3abe4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.113743Z",
     "start_time": "2024-10-17T15:18:05.092224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "id": "a64b3826039dc9c6",
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "613f452a",
   "metadata": {
    "papermill": {
     "duration": 0.015987,
     "end_time": "2024-10-04T19:24:01.719520",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.703533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "id": "f86abcb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:01.753594Z",
     "iopub.status.busy": "2024-10-04T19:24:01.752729Z",
     "iopub.status.idle": "2024-10-04T19:24:01.810353Z",
     "shell.execute_reply": "2024-10-04T19:24:01.809354Z"
    },
    "papermill": {
     "duration": 0.077274,
     "end_time": "2024-10-04T19:24:01.812862",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.735588",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.165855Z",
     "start_time": "2024-10-17T15:18:05.142495Z"
    }
   },
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            # nn.ReLU(),\n",
    "            # nn.PReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "e6c058a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:01.848386Z",
     "iopub.status.busy": "2024-10-04T19:24:01.848012Z",
     "iopub.status.idle": "2024-10-04T19:24:01.906714Z",
     "shell.execute_reply": "2024-10-04T19:24:01.905885Z"
    },
    "papermill": {
     "duration": 0.079025,
     "end_time": "2024-10-04T19:24:01.908938",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.829913",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.216685Z",
     "start_time": "2024-10-17T15:18:05.195325Z"
    }
   },
   "source": [
    "class GatedAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GatedAttention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.Tanh(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, input_dim)\n",
    "        attention_weights = self.attention(x)\n",
    "        gate_weights = torch.sigmoid(self.gate(x))\n",
    "        \n",
    "        weights = attention_weights * gate_weights\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        \n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "f0926878",
   "metadata": {
    "papermill": {
     "duration": 0.015805,
     "end_time": "2024-10-04T19:24:02.065521",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.049716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ResNet2D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b940bb",
   "metadata": {
    "papermill": {
     "duration": 0.016427,
     "end_time": "2024-10-04T19:24:02.227392",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.210965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Mixed Pooling"
   ]
  },
  {
   "cell_type": "code",
   "id": "4ef8d304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:02.261598Z",
     "iopub.status.busy": "2024-10-04T19:24:02.261205Z",
     "iopub.status.idle": "2024-10-04T19:24:02.318508Z",
     "shell.execute_reply": "2024-10-04T19:24:02.317523Z"
    },
    "papermill": {
     "duration": 0.077269,
     "end_time": "2024-10-04T19:24:02.320872",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.243603",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.262230Z",
     "start_time": "2024-10-17T15:18:05.243411Z"
    }
   },
   "source": [
    "class MixedPooling(nn.Module):\n",
    "    def __init__(self, in_channels: int, alpha: float=0.5):\n",
    "        super(MixedPooling, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Convolution layer\n",
    "        self.conv = nn.Conv2d(in_channels, 16, kernel_size=1)  # Adjust output channels as needed\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform average pooling and apply weight\n",
    "        avg_out = self.avgpool(x)\n",
    "        avg_out_weighted = avg_out * self.alpha\n",
    "        \n",
    "        # Perform max pooling and apply weight\n",
    "        max_out = self.maxpool(x)\n",
    "        max_out_weighted = max_out * (1 - self.alpha)\n",
    "\n",
    "        # Add the weighted outputs together\n",
    "        # combined = avg_out_weighted + max_out_weighted\n",
    "        combined = torch.cat((avg_out_weighted, max_out_weighted), dim=1)  # Concatenate along channel dimension\n",
    "        combined = self.conv(combined)  # Reduce channels\n",
    "        return combined  # Output with combined features"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "bac27bdb",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.353086",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.337113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### ResNet2D Model"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.310551Z",
     "start_time": "2024-10-17T15:18:05.290491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.attention = AttentionLayer(input_dim=512, hidden_dim=512)\n",
    "        self.classifier = nn.Linear(512 + 1, 1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # Define inducing points for the GP layer\n",
    "        inducing_points = torch.randn(128, 512)\n",
    "        # inducing_points = torch.full((256, 512), 1e-20)  \n",
    "        self.gp_layer = GPModel(inducing_points)\n",
    "\n",
    "    def forward(self, bags):\n",
    "        # batch_size, num_instances = bags.size(0), bags.size(1)\n",
    "        # bags_flattened = bags.view(-1, *bags.shape[2:])\n",
    "        batch_size, num_instances, h, w, c = bags.size()\n",
    "        bags_flattened = bags.view(batch_size * num_instances, c, h, w)\n",
    "\n",
    "        features = self.resnet(bags_flattened)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "        attended_features, attended_weights = self.attention(features)\n",
    "\n",
    "        attended_features_reshaped = attended_features.view(-1, 512)\n",
    "        gp_output = self.gp_layer(attended_features_reshaped)\n",
    "        gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "\n",
    "        combine_features = torch.cat((attended_features, gp_mean), dim=1)\n",
    "        combine_features = self.dropout(combine_features)\n",
    "\n",
    "        outputs = torch.sigmoid(self.classifier(combine_features))\n",
    "        return outputs, attended_weights"
   ],
   "id": "6d73010c7c93d496",
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "f5d31545",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.496444",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.480471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss Function",
   "id": "ad87920f63fb870"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.407731Z",
     "start_time": "2024-10-17T15:18:05.384334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    # Cross-Entropy Loss for CNN outputs\n",
    "    bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n",
    "    kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "    total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    \n",
    "    return total_loss"
   ],
   "id": "dec85654847b2a02",
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "7b1cfd9f",
   "metadata": {
    "papermill": {
     "duration": 0.016325,
     "end_time": "2024-10-04T19:24:02.529050",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.512725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b539e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:02.565057Z",
     "iopub.status.busy": "2024-10-04T19:24:02.563921Z",
     "iopub.status.idle": "2024-10-04T19:24:02.634003Z",
     "shell.execute_reply": "2024-10-04T19:24:02.632980Z"
    },
    "papermill": {
     "duration": 0.089765,
     "end_time": "2024-10-04T19:24:02.636485",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.546720",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.451176Z",
     "start_time": "2024-10-17T15:18:05.432650Z"
    }
   },
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for batch_data, batch_labels, _ in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, attention_weights = model(batch_data)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = combined_loss(outputs, model.gp_layer, batch_labels, alpha)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Store predictions and labels\n",
    "        predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "        labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, _ in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, attention_weights = model(batch_data)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = combined_loss(outputs, model.gp_layer, batch_labels, alpha)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            # Store predictions and labels\n",
    "            predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, learning_rate, device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "                                              steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, val_loader, criterion, device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "id": "63ff964b",
   "metadata": {
    "papermill": {
     "duration": 0.016054,
     "end_time": "2024-10-04T19:24:02.668925",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.652871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "763baba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:02.703261Z",
     "iopub.status.busy": "2024-10-04T19:24:02.702625Z",
     "iopub.status.idle": "2024-10-04T19:24:02.762115Z",
     "shell.execute_reply": "2024-10-04T19:24:02.761292Z"
    },
    "papermill": {
     "duration": 0.079611,
     "end_time": "2024-10-04T19:24:02.764721",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.685110",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.502327Z",
     "start_time": "2024-10-17T15:18:05.481048Z"
    }
   },
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, _ in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs, attention_weights = model(batch_data)\n",
    "            outputs = outputs.squeeze()\n",
    "            predictions.extend((outputs > 0.5).cpu().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "271dbe08",
   "metadata": {
    "papermill": {
     "duration": 0.01814,
     "end_time": "2024-10-04T19:24:02.800208",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.782068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "e134a5b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:02.839344Z",
     "iopub.status.busy": "2024-10-04T19:24:02.838624Z",
     "iopub.status.idle": "2024-10-04T19:24:02.908472Z",
     "shell.execute_reply": "2024-10-04T19:24:02.907355Z"
    },
    "papermill": {
     "duration": 0.092339,
     "end_time": "2024-10-04T19:24:02.911187",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.818848",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.551847Z",
     "start_time": "2024-10-17T15:18:05.530737Z"
    }
   },
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    # predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, _ in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs, attention_weights = model(batch_data)\n",
    "            outputs = outputs.squeeze()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "id": "281587d3",
   "metadata": {
    "papermill": {
     "duration": 0.018543,
     "end_time": "2024-10-04T19:24:02.948830",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.930287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "a01b3a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:02.985315Z",
     "iopub.status.busy": "2024-10-04T19:24:02.984895Z",
     "iopub.status.idle": "2024-10-04T19:24:03.044052Z",
     "shell.execute_reply": "2024-10-04T19:24:03.043041Z"
    },
    "papermill": {
     "duration": 0.079014,
     "end_time": "2024-10-04T19:24:03.046531",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.967517",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.601238Z",
     "start_time": "2024-10-17T15:18:05.580782Z"
    }
   },
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class()\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader)\n",
    "    \n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "id": "2064097a",
   "metadata": {
    "papermill": {
     "duration": 0.01837,
     "end_time": "2024-10-04T19:24:03.082997",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.064627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualizing Attention Weights and Images"
   ]
  },
  {
   "cell_type": "code",
   "id": "9cf5239b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:03.122536Z",
     "iopub.status.busy": "2024-10-04T19:24:03.122068Z",
     "iopub.status.idle": "2024-10-04T19:24:03.194273Z",
     "shell.execute_reply": "2024-10-04T19:24:03.193161Z"
    },
    "papermill": {
     "duration": 0.095451,
     "end_time": "2024-10-04T19:24:03.197071",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.101620",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:18:05.650108Z",
     "start_time": "2024-10-17T15:18:05.629582Z"
    }
   },
   "source": [
    "def separate_patient_study_instance(patient_study_instance):\n",
    "    # ID_000012eaf_ID_5c8b5d701, split them into ID_000012eaf and ID_5c8b5d701\n",
    "    split_criteria = '_ID_'\n",
    "    patient_id, study_instance_uid = patient_study_instance.rsplit(split_criteria, 1)\n",
    "    \n",
    "    return patient_id, 'ID_' + study_instance_uid\n",
    "\n",
    "def find_matching_row(patient_id, study_instance_uid, patient_slice_labels):\n",
    "    # Filter for matching rows based on patient_id and study_instance_uid\n",
    "    matching_row = patient_slice_labels[\n",
    "        (patient_slice_labels['patient_id'] == patient_id) & \n",
    "        (patient_slice_labels['study_instance_uid'] == study_instance_uid)\n",
    "    ]\n",
    "    return matching_row\n",
    "\n",
    "\n",
    "def prepare_labels_for_images(images, patient_slice_labels, patient_id, study_instance_uid):\n",
    "    # Get matching rows\n",
    "    matching_rows = find_matching_row(patient_id, study_instance_uid, patient_slice_labels)\n",
    "    \n",
    "    # Prepare labels for images\n",
    "    labels = []\n",
    "    \n",
    "    # Check if there are any matching rows\n",
    "    if not matching_rows.empty:\n",
    "        # Loop through the number of images and assign labels\n",
    "        for i in range(len(images)):\n",
    "            label_found = 0\n",
    "            \n",
    "            # If there are fewer matching rows than images, we need to handle that\n",
    "            if i < len(matching_rows):  # Ensure we do not exceed the number of matching rows\n",
    "                # Check the relevant columns for the current matching row\n",
    "                if matching_rows.iloc[i][['any', 'epidural', 'intraparenchymal',\n",
    "                                           'intraventricular', 'subarachnoid', \n",
    "                                           'subdural']].any():\n",
    "                    label_found = 1  # Set label to 1 if any of the conditions are met\n",
    "            \n",
    "            labels.append(label_found)  # Append found label to the list\n",
    "            \n",
    "        # If there are more images than matching rows, append 0 for unmatched images\n",
    "        labels.extend([0] * (len(images) - len(matching_rows)))  # Fill remaining labels with 0\n",
    "    else:\n",
    "        # If no matching rows, assign label 0 for all images\n",
    "        labels = [0] * len(images)  # Create a list of zeros with the same length as images\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def plot_images_with_attention(images, labels, attention_weights, patient_study_instance):\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    num_images = images.size(0)\n",
    "    images = images.numpy()\n",
    "    \n",
    "    patient_id, study_instance_uid = separate_patient_study_instance(patient_study_instance)\n",
    "    slice_labels = prepare_labels_for_images(images, patient_slice_labels, patient_id, study_instance_uid)\n",
    "    \n",
    "    # Calculate number of rows needed (60 images with 4 per row)\n",
    "    num_rows = num_images // 4 + (num_images % 4 > 0)  # Add an extra row if there's a remainder\n",
    "    \n",
    "    plt.figure(figsize=(15, num_rows * 3))  # Adjust height based on number of rows\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_rows, 4, i + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title(f'Image {i + 1} - Attention: {attention_weights[i]:.4f} - Label: {slice_labels[i]}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    \n",
    "def test_patient_bag(model, patient_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, patient_study_instance in patient_data:\n",
    "            batch_data = batch_data.to(device)  # Move data to device\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "            outputs, gated_attention = model(batch_data)  # Get outputs and attention weights\n",
    "            outputs = outputs.squeeze()  # Remove extra dimension\n",
    "            \n",
    "            # Aggregate predictions across instances (bags)\n",
    "            bag_predictions = (outputs > 0.5).cpu().numpy()\n",
    "            print(\"Bag Prediction:\", bag_predictions)\n",
    "            print(\"Labels: \", batch_labels.cpu().numpy())\n",
    "            \n",
    "            # Print attention weights for each slide\n",
    "            attention_weights_np = gated_attention.cpu().numpy()  # Move to CPU for easier handling\n",
    "            print(\"Attention Weights for Slides:\")\n",
    "            \n",
    "            # for i in range(attention_weights_np.shape[1]):  # Iterate over instances/slides\n",
    "                # print(f'Slide {i+1}: Attention Weight: {attention_weights_np[0][i]}')  # Print weight for each slide\n",
    "                \n",
    "            # Plot the images with attention weights\n",
    "            plot_images_with_attention(batch_data[0].cpu(), batch_labels[0].cpu(), attention_weights_np[0], patient_study_instance[0])\n",
    "            \n",
    "            break  # Only test one patient"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "a6de818e",
   "metadata": {
    "papermill": {
     "duration": 0.016248,
     "end_time": "2024-10-04T19:24:03.231225",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.214977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6af9ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T19:24:03.265872Z",
     "iopub.status.busy": "2024-10-04T19:24:03.265507Z",
     "iopub.status.idle": "2024-10-05T02:20:26.799042Z",
     "shell.execute_reply": "2024-10-05T02:20:26.798168Z"
    },
    "papermill": {
     "duration": 24983.553969,
     "end_time": "2024-10-05T02:20:26.801619",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.247650",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-10-17T15:23:57.734754Z",
     "start_time": "2024-10-17T15:18:05.681617Z"
    }
   },
   "source": [
    "## Main Execution\n",
    "def main(mode='train'):\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model, criterion, and optimizer\n",
    "    # model = ResNet2D18_MIL()\n",
    "    model = MILResNet18()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss // Not used\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        # Train model\n",
    "        trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, LEARNING_RATE, DEVICE)\n",
    "        \n",
    "        # Save model\n",
    "        torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # Load best model\n",
    "    # trained_model = load_model(ResNet2D18_MIL, MODEL_PATH)\n",
    "    trained_model = load_model(MILResNet18, MODEL_PATH)\n",
    "    \n",
    "    # Evaluate model\n",
    "    predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    metrics = calculate_metrics(predictions, labels)\n",
    "    print_metrics(metrics)\n",
    "    \n",
    "    # Visualizations\n",
    "    plot_roc_curve(trained_model, test_loader, DEVICE)\n",
    "    plot_confusion_matrix(trained_model, test_loader, DEVICE)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        # Select only the required columns\n",
    "        required_columns = ['patient_id', 'study_instance_uid', 'label']\n",
    "        temp_test_labels = test_labels[required_columns]\n",
    "        \n",
    "        # Save results\n",
    "        results_df = get_test_results(trained_model, test_loader, temp_test_labels)\n",
    "        results_df.to_csv('results/results.csv', index=False)\n",
    "        print(results_df.head())\n",
    "    \n",
    "    # Call the function with the test_loader\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=2)\n",
    "    test_patient_bag(trained_model, test_loader)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train:\n",
      "Loss: 60.7129, Accuracy: 0.4986, Precision: 0.4992, Recall: 0.9257, F1: 0.6486\n",
      "Epoch 1/30 - Validation:\n",
      "Loss: 26.0045, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "Epoch 2/30 - Train:\n",
      "Loss: 60.7171, Accuracy: 0.4943, Precision: 0.4967, Recall: 0.8629, F1: 0.6305\n",
      "Epoch 2/30 - Validation:\n",
      "Loss: 26.0020, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "Epoch 3/30 - Train:\n",
      "Loss: 60.7385, Accuracy: 0.4800, Precision: 0.4857, Recall: 0.6800, F1: 0.5667\n",
      "Epoch 3/30 - Validation:\n",
      "Loss: 25.9649, Accuracy: 0.5067, Precision: 0.5035, Recall: 0.9467, F1: 0.6574\n",
      "Epoch 4/30 - Train:\n",
      "Loss: 60.5838, Accuracy: 0.5157, Precision: 0.5098, Recall: 0.8200, F1: 0.6287\n",
      "Epoch 4/30 - Validation:\n",
      "Loss: 25.9853, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "Epoch 5/30 - Train:\n",
      "Loss: 60.6597, Accuracy: 0.4929, Precision: 0.4931, Recall: 0.5086, F1: 0.5007\n",
      "Epoch 5/30 - Validation:\n",
      "Loss: 25.9466, Accuracy: 0.5000, Precision: 0.5000, Recall: 0.0133, F1: 0.0260\n",
      "Epoch 6/30 - Train:\n",
      "Loss: 60.9981, Accuracy: 0.5100, Precision: 0.5060, Recall: 0.8371, F1: 0.6308\n",
      "Epoch 6/30 - Validation:\n",
      "Loss: 26.0010, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "Epoch 7/30 - Train:\n",
      "Loss: 60.7578, Accuracy: 0.4657, Precision: 0.4613, Recall: 0.4086, F1: 0.4333\n",
      "Epoch 7/30 - Validation:\n",
      "Loss: 26.0142, Accuracy: 0.5000, Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "Epoch 8/30 - Train:\n",
      "Loss: 60.6911, Accuracy: 0.4914, Precision: 0.4953, Recall: 0.9000, F1: 0.6389\n",
      "Epoch 8/30 - Validation:\n",
      "Loss: 25.9931, Accuracy: 0.5000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch 9/30 - Train:\n",
      "Loss: 60.7664, Accuracy: 0.4786, Precision: 0.4851, Recall: 0.7000, F1: 0.5731\n",
      "Epoch 9/30 - Validation:\n",
      "Loss: 25.9934, Accuracy: 0.5000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 49\u001B[0m\n\u001B[1;32m     46\u001B[0m     test_patient_bag(trained_model, test_loader)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 49\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[56], line 16\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(mode)\u001B[0m\n\u001B[1;32m     12\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLEARNING_RATE)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNUM_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLEARNING_RATE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Save model\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(trained_model\u001B[38;5;241m.\u001B[39mstate_dict(), MODEL_PATH)\n",
      "Cell \u001B[0;32mIn[51], line 81\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, learning_rate, device)\u001B[0m\n\u001B[1;32m     77\u001B[0m best_model_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;66;03m# Training phase\u001B[39;00m\n\u001B[0;32m---> 81\u001B[0m     train_loss, train_predictions, train_labels \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m calculate_metrics(train_predictions, train_labels)\n\u001B[1;32m     83\u001B[0m     print_epoch_stats(epoch, num_epochs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_loss, train_metrics)\n",
      "Cell \u001B[0;32mIn[51], line 14\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, data_loader, criterion, optimizer, scheduler, device)\u001B[0m\n\u001B[1;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m outputs, attention_weights \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m outputs \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     16\u001B[0m loss \u001B[38;5;241m=\u001B[39m combined_loss(outputs, model\u001B[38;5;241m.\u001B[39mgp_layer, batch_labels, alpha)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[49], line 85\u001B[0m, in \u001B[0;36mMILResNet18.forward\u001B[0;34m(self, bags)\u001B[0m\n\u001B[1;32m     82\u001B[0m attended_features, attended_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(features)\n\u001B[1;32m     84\u001B[0m attended_features_reshaped \u001B[38;5;241m=\u001B[39m attended_features\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m32\u001B[39m)\n\u001B[0;32m---> 85\u001B[0m gp_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgp_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattended_features_reshaped\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m gp_mean \u001B[38;5;241m=\u001B[39m gp_output\u001B[38;5;241m.\u001B[39mmean\u001B[38;5;241m.\u001B[39mview(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     88\u001B[0m combine_features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((attended_features, gp_mean), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/gpytorch/models/approximate_gp.py:114\u001B[0m, in \u001B[0;36mApproximateGP.__call__\u001B[0;34m(self, inputs, prior, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    113\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvariational_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprior\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprior\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/gpytorch/variational/variational_strategy.py:239\u001B[0m, in \u001B[0;36mVariationalStrategy.__call__\u001B[0;34m(self, x, prior, **kwargs)\u001B[0m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor, prior: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m MultivariateNormal:\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdated_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m prior:\n\u001B[1;32m    240\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    241\u001B[0m             \u001B[38;5;66;03m# Get unwhitened p(u)\u001B[39;00m\n\u001B[1;32m    242\u001B[0m             prior_function_dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minducing_points, prior\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9762367,
     "datasetId": 5705276,
     "sourceId": 9546002,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25022.389189,
   "end_time": "2024-10-05T02:20:29.848670",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-04T19:23:27.459481",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
