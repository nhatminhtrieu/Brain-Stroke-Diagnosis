{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5705276,
     "sourceId": 9399351,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Library",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install gpytorch triton",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:43.884185Z",
     "iopub.execute_input": "2024-09-26T10:43:43.884874Z",
     "iopub.status.idle": "2024-09-26T10:43:56.997192Z",
     "shell.execute_reply.started": "2024-09-26T10:43:43.884823Z",
     "shell.execute_reply": "2024-09-26T10:43:56.995964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom tqdm import tqdm\nfrom skimage.transform import resize\nimport cv2\nfrom typing import Type\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport gpytorch\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.variational import CholeskyVariationalDistribution\nfrom gpytorch.variational import VariationalStrategy\n\nfrom triton.ops import attention\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:56.999365Z",
     "iopub.execute_input": "2024-09-26T10:43:56.999693Z",
     "iopub.status.idle": "2024-09-26T10:43:57.064421Z",
     "shell.execute_reply.started": "2024-09-26T10:43:56.999657Z",
     "shell.execute_reply": "2024-09-26T10:43:57.063341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.065725Z",
     "iopub.execute_input": "2024-09-26T10:43:57.066113Z",
     "iopub.status.idle": "2024-09-26T10:43:57.120324Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.066077Z",
     "shell.execute_reply": "2024-09-26T10:43:57.119267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Init GPU",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize GPU Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\nelse:\n    print(\"No GPU available. Training will run on CPU.\")\n\nprint(device)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.122807Z",
     "iopub.execute_input": "2024-09-26T10:43:57.123175Z",
     "iopub.status.idle": "2024-09-26T10:43:57.183999Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.123141Z",
     "shell.execute_reply": "2024-09-26T10:43:57.182807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.185592Z",
     "iopub.execute_input": "2024-09-26T10:43:57.186560Z",
     "iopub.status.idle": "2024-09-26T10:43:57.244537Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.186507Z",
     "shell.execute_reply": "2024-09-26T10:43:57.243500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Config Info",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Constants\nHEIGHT = 224\nWIDTH = 224\nCHANNELS = 3\n\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nTEST_BATCH_SIZE = 4\nTEST_SIZE = 0.15\nVALID_SIZE = 0.15\n\nMAX_SLICES = 60\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nNUM_EPOCHS = 30\nLEARNING_RATE = 1e-3\nINDUCING_POINTS = 128\n\n# TARGET_LABELS = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\nTARGET_LABELS = ['intraparenchymal']\n\nMODEL_PATH = 'trained_model.pth'\nDEVICE = 'cuda'",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.245693Z",
     "iopub.execute_input": "2024-09-26T10:43:57.246021Z",
     "iopub.status.idle": "2024-09-26T10:43:57.300952Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.245987Z",
     "shell.execute_reply": "2024-09-26T10:43:57.299960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Kaggle and local switch\nKAGGLE = os.path.exists('/kaggle')\nprint(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n\nDATA_DIR = '/kaggle/input/' if KAGGLE else '../rsna-mil-training/'\nDICOM_DIR = DATA_DIR + 'rsna-mil-training/'\nCSV_PATH = DATA_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_1000_scan_subset.csv'\n\ndicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n# Load patient scan labels\npatient_scan_labels = pd.read_csv(CSV_PATH)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.302404Z",
     "iopub.execute_input": "2024-09-26T10:43:57.303167Z",
     "iopub.status.idle": "2024-09-26T10:43:57.367393Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.303119Z",
     "shell.execute_reply": "2024-09-26T10:43:57.366414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef window_image(dcm, window_center, window_width):    \n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    \n    # Resize\n    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n   \n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 40, 80)\n    subdural_img = window_image(dcm, 80, 200)\n    soft_img = window_image(dcm, 40, 380)\n    \n    brain_img = (brain_img - 0) / 80\n    subdural_img = (subdural_img - (-20)) / 200\n    soft_img = (soft_img - (-150)) / 380\n    \n    bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n    return bsb_img.astype(np.float16)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.368993Z",
     "iopub.execute_input": "2024-09-26T10:43:57.369295Z",
     "iopub.status.idle": "2024-09-26T10:43:57.426744Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.369264Z",
     "shell.execute_reply": "2024-09-26T10:43:57.425670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n    # Check if type of slice is dicom or an empty numpy array\n    if (type(slice) == np.ndarray):\n        slice = resize(slice, target_size, anti_aliasing=True)\n        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n        return multichannel_slice.astype(np.float16)\n    else:\n        slice = bsb_window(slice)\n        return slice.astype(np.float16)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.428018Z",
     "iopub.execute_input": "2024-09-26T10:43:57.428434Z",
     "iopub.status.idle": "2024-09-26T10:43:57.484356Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.428388Z",
     "shell.execute_reply": "2024-09-26T10:43:57.483350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def read_dicom_folder(folder_path):\n    slices = []\n    for filename in sorted(os.listdir(folder_path))[:MAX_SLICES]:  # Limit to MAX_SLICES\n        if filename.endswith(\".dcm\"):\n            file_path = os.path.join(folder_path, filename)\n            ds = pydicom.dcmread(file_path)\n            slices.append(ds)\n            \n    # Sort slices by images position (z-coordinate) in ascending order\n    slices = sorted(slices, key=lambda x: float(x.ImagePositionPatient[2]))\n    \n    # Pad with black images if necessary\n    while len(slices) < MAX_SLICES:\n        slices.append(np.zeros_like(slices[0].pixel_array))\n    \n    return slices[:MAX_SLICES]  # Ensure we return exactly MAX_SLICES",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.488175Z",
     "iopub.execute_input": "2024-09-26T10:43:57.488606Z",
     "iopub.status.idle": "2024-09-26T10:43:57.545450Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.488573Z",
     "shell.execute_reply": "2024-09-26T10:43:57.544455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset and DataLoader",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Splitting the Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n    # If any of the hemorrhage indicators is 1, the label is 1, otherwise 0\n    patient_scan_labels['label'] = patient_scan_labels[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any(axis=1).astype(int)\n\n    # Extract the labels from the DataFrame\n    labels = patient_scan_labels['label']\n\n    # First, split off the test set\n    train_val_labels, test_labels = train_test_split(\n        patient_scan_labels, \n        test_size=test_size, \n        stratify=labels, \n        random_state=random_state\n    )\n\n    # Calculate the validation size relative to the train_val set\n    val_size_adjusted = val_size / (1 - test_size)\n\n    # Split the train_val set into train and validation sets\n    train_labels, val_labels = train_test_split(\n        train_val_labels, \n        test_size=val_size_adjusted, \n        stratify=train_val_labels['label'], \n        random_state=random_state\n    )\n\n    return train_labels, val_labels, test_labels",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.546717Z",
     "iopub.execute_input": "2024-09-26T10:43:57.547107Z",
     "iopub.status.idle": "2024-09-26T10:43:57.603110Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.547066Z",
     "shell.execute_reply": "2024-09-26T10:43:57.602208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Processing the Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def process_patient_data(dicom_dir, row, num_instances=12, depth=5):\n",
    "    patient_id = row['patient_id'].replace('ID_', '')\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n",
    "    \n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(dicom_dir, folder_name)\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        \n",
    "        # Ensure we have enough slices to create the specified instances\n",
    "        if len(slices) < depth * num_instances:\n",
    "            print(f\"Not enough slices for patient {patient_id}: found {len(slices)}, needed {depth * num_instances}\")\n",
    "            return None, None\n",
    "        \n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Stack preprocessed slices into an array\n",
    "        preprocessed_slices = np.stack(preprocessed_slices, axis=0)  # (num_slices, height, width, channels)\n",
    "        \n",
    "        # Reshape to (num_instances, depth, height, width, channels)\n",
    "        # reshaped_slices = preprocessed_slices[:num_instances * depth].reshape(num_instances, depth, *preprocessed_slices.shape[1:])  # (num_instances, depth, height, width, channels)\n",
    "        \n",
    "        # Labeling remains consistent  \n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        # return reshaped_slices, label\n",
    "        return preprocessed_slices, label\n",
    "    \n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.604365Z",
     "iopub.execute_input": "2024-09-26T10:43:57.604692Z",
     "iopub.status.idle": "2024-09-26T10:43:57.662122Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.604647Z",
     "shell.execute_reply": "2024-09-26T10:43:57.661159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class TrainDatasetGenerator(Dataset):\n    \"\"\"\n    A custom dataset class for training data.\n    \"\"\"\n    def __init__(self, data_dir, patient_scan_labels):\n        self.data_dir = data_dir\n        self.patient_scan_labels = patient_scan_labels\n\n    def __len__(self):\n        return len(self.patient_scan_labels)\n\n    def __getitem__(self, idx):\n        row = self.patient_scan_labels.iloc[idx]\n        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n        \n        if preprocessed_slices is not None:\n            # Convert the list of numpy arrays to a single numpy array\n            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n        else:\n            return None, None  # Handle the case where the folder is not found\n\nclass TestDatasetGenerator(Dataset):\n    \"\"\"\n    A custom dataset class for testing data.\n    \"\"\"\n    def __init__(self, data_dir, patient_scan_labels):\n        self.data_dir = data_dir\n        self.patient_scan_labels = patient_scan_labels\n\n    def __len__(self):\n        return len(self.patient_scan_labels)\n\n    def __getitem__(self, idx):\n        row = self.patient_scan_labels.iloc[idx]\n        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n        \n        if preprocessed_slices is not None:\n            # Convert the list of numpy arrays to a single numpy array\n            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n        else:\n            return None, None  # Handle the case where the folder is not found",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.663413Z",
     "iopub.execute_input": "2024-09-26T10:43:57.663797Z",
     "iopub.status.idle": "2024-09-26T10:43:57.722055Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.663744Z",
     "shell.execute_reply": "2024-09-26T10:43:57.721224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE, shuffle=True):\n    train_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels)\n    return DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n\ndef get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels)\n    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.723486Z",
     "iopub.execute_input": "2024-09-26T10:43:57.723799Z",
     "iopub.status.idle": "2024-09-26T10:43:57.776454Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.723766Z",
     "shell.execute_reply": "2024-09-26T10:43:57.775488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## CNN Feature Extractor",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Sparse Gaussian Process Regression",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SparseGPLayer(ApproximateGP):\n    def __init__(self, num_inputs, num_inducing=INDUCING_POINTS):\n        # Initialize inducing points\n        inducing_points = torch.randn(num_inducing, num_inputs)\n        \n        # Initialize variational parameters\n        variational_distribution = CholeskyVariationalDistribution(num_inducing)\n        variational_strategy = VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        \n        super(SparseGPLayer, self).__init__(variational_strategy)\n        \n        # Mean and covariance modules\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n        \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.777808Z",
     "iopub.execute_input": "2024-09-26T10:43:57.778187Z",
     "iopub.status.idle": "2024-09-26T10:43:57.831958Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.778144Z",
     "shell.execute_reply": "2024-09-26T10:43:57.831045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class SparseGPWrapper(nn.Module):\n    def __init__(self, num_inputs, num_inducing=INDUCING_POINTS):\n        super(SparseGPWrapper, self).__init__()\n        self.gp_layer = SparseGPLayer(num_inputs, num_inducing)\n        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        \n    def forward(self, x):\n        # Get the GP output\n        gp_output = self.gp_layer(x)\n        # Return the mean of the GP output\n        return gp_output.mean",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.833246Z",
     "iopub.execute_input": "2024-09-26T10:43:57.834173Z",
     "iopub.status.idle": "2024-09-26T10:43:57.884926Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.834129Z",
     "shell.execute_reply": "2024-09-26T10:43:57.883972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Attention Layer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class AttentionLayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, num_instances, feature_dim)\n        attention_weights = self.attention(x)\n        weights = F.softmax(attention_weights, dim=1)\n        \n        return (x * weights).sum(dim=1), weights.squeeze(-1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.886152Z",
     "iopub.execute_input": "2024-09-26T10:43:57.886776Z",
     "iopub.status.idle": "2024-09-26T10:43:57.939316Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.886731Z",
     "shell.execute_reply": "2024-09-26T10:43:57.938379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class GatedAttention(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(GatedAttention, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.gate = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n    def forward(self, x):\n        # x shape: (batch_size, num_instances, input_dim)\n        attention_weights = self.attention(x)\n        gate_weights = torch.sigmoid(self.gate(x))\n        \n        weights = attention_weights * gate_weights\n        weights = F.softmax(weights, dim=1)\n        \n        return (x * weights).sum(dim=1), weights.squeeze(-1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.940447Z",
     "iopub.execute_input": "2024-09-26T10:43:57.940756Z",
     "iopub.status.idle": "2024-09-26T10:43:57.995981Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.940716Z",
     "shell.execute_reply": "2024-09-26T10:43:57.995045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Sparse Gaussian Process Layer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SparseGPlayer(gpytorch.models.ApproximateGP):\n    def __init__(self, inducing_points):\n        inducing_points = inducing_points.to(device)\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            inducing_points.size(0)\n        )\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(SparseGPlayer, self).__init__(variational_strategy)\n        \n        self.mean_module = gpytorch.means.ConstantMean().to(device)\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()).to(device)\n    \n    def forward(self, x):\n        x = x.to(device)\n        \n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:57.997315Z",
     "iopub.execute_input": "2024-09-26T10:43:57.997655Z",
     "iopub.status.idle": "2024-09-26T10:43:58.052019Z",
     "shell.execute_reply.started": "2024-09-26T10:43:57.997621Z",
     "shell.execute_reply": "2024-09-26T10:43:58.051012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### ResNet2D Model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "#### Basic Block",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class BasicBlock2D(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        expansion: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "        dropout_rate: float = 0.25\n",
    "    ) -> None:\n",
    "        super(BasicBlock2D, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=3, \n",
    "            stride=stride, \n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, \n",
    "            out_channels*self.expansion, \n",
    "            kernel_size=3, \n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "        \n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:58.053211Z",
     "iopub.execute_input": "2024-09-26T10:43:58.053543Z",
     "iopub.status.idle": "2024-09-26T10:43:58.112806Z",
     "shell.execute_reply.started": "2024-09-26T10:43:58.053503Z",
     "shell.execute_reply": "2024-09-26T10:43:58.111852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "#### ResNet2D Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ResNet2D_MIL(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_channels: int,\n",
    "        num_layers: int,\n",
    "        block: Type[BasicBlock2D],\n",
    "        num_classes: int = 1,\n",
    "        dropout_rate: float = 0.25\n",
    "    ) -> None:\n",
    "        super(ResNet2D_MIL, self).__init__()\n",
    "        \n",
    "        if num_layers == 18:\n",
    "            layers = [2, 2, 2, 2]\n",
    "            self.expansion = 1\n",
    "        elif num_layers == 34:\n",
    "            layers = [3, 4, 6, 3]\n",
    "            self.expansion = 1\n",
    "        elif num_layers == 50:\n",
    "            layers = [3, 4, 6, 3]\n",
    "            self.expansion = 4\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported number of layers\")\n",
    "        \n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=img_channels,\n",
    "            out_channels=self.in_channels,\n",
    "            kernel_size=7, \n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.meanpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1, dropout_rate=dropout_rate)\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2, dropout_rate=dropout_rate)\n",
    "        self.layer3 = self._make_layer(block, 32, layers[2], stride=2, dropout_rate=dropout_rate)\n",
    "        self.layer4 = self._make_layer(block, 32, layers[3], stride=2, dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.attention = AttentionLayer(32*self.expansion, 64)\n",
    "        self.fc = nn.Linear(32*self.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, \n",
    "        block: Type[BasicBlock2D],\n",
    "        out_channels: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dropout_rate: float = 0.25\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * self.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels, \n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False \n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels, out_channels, stride, self.expansion, downsample, dropout_rate\n",
    "            )\n",
    "        )\n",
    "        self.in_channels = out_channels * self.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(\n",
    "                self.in_channels,\n",
    "                out_channels,\n",
    "                expansion=self.expansion,\n",
    "                dropout_rate=dropout_rate\n",
    "            ))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Input shape assertion\n",
    "        assert x.dim() == 5, f\"Expected 5D input (batch_size, num_instances, height, width, channels), but got {x.dim()}D input; x.shape={x.shape}\"\n",
    "        batch_size, num_instances, h, w, c = x.size()\n",
    "        \n",
    "        # Reshape to (batch_size * num_instances, channels, height, width)\n",
    "        out = x.view(batch_size * num_instances, c, h, w)\n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # out = self.maxpool(out)\n",
    "        out = self.meanpool(out)\n",
    "    \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        \n",
    "        # Reshape back to (batch_size, num_instances, features)\n",
    "        out = out.view(batch_size, num_instances, -1)\n",
    "        \n",
    "        # out = torch.max(out, dim=1)[0]  # Take max across instances\n",
    "        \n",
    "        # Take mean across instances\n",
    "        out = out.mean(dim=1)\n",
    "        \n",
    "        # out, _ = self.attention(out)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Assert that the output shape is correct\n",
    "        assert out.shape == (batch_size, self.fc.out_features), f\"Expected output shape (batch_size, {self.fc.out_features}), but got {out.shape}\"\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet2D18_MIL(img_channels: int = 3, num_classes: int = 1, dropout_rate: float = 0.25):\n",
    "    return ResNet2D_MIL(img_channels, 18, BasicBlock2D, num_classes, dropout_rate)\n",
    "\n",
    "def ResNet2D34_MIL(img_channels: int = 3, num_classes: int = 1, dropout_rate: float = 0.25):\n",
    "    return ResNet2D_MIL(img_channels, 34, BasicBlock2D, num_classes, dropout_rate)\n",
    "\n",
    "def ResNet2D50_MIL(img_channels: int = 3, num_classes: int = 1, dropout_rate: float = 0.25):\n",
    "    return ResNet2D_MIL(img_channels, 50, BasicBlock2D, num_classes, dropout_rate)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:58.114103Z",
     "iopub.execute_input": "2024-09-26T10:43:58.114385Z",
     "iopub.status.idle": "2024-09-26T10:43:58.191440Z",
     "shell.execute_reply.started": "2024-09-26T10:43:58.114355Z",
     "shell.execute_reply": "2024-09-26T10:43:58.190414Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Training and Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for batch_data, batch_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data).squeeze()\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions.extend((outputs > 0).cpu().numpy())\n",
    "        labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            predictions.extend((outputs > 0).cpu().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, learning_rate, device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "                                              steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, val_loader, criterion, device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:58.193052Z",
     "iopub.execute_input": "2024-09-26T10:43:58.193478Z",
     "iopub.status.idle": "2024-09-26T10:43:58.260233Z",
     "shell.execute_reply.started": "2024-09-26T10:43:58.193435Z",
     "shell.execute_reply": "2024-09-26T10:43:58.259233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Evaluation Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Model Evaluation Functions\ndef evaluate_model(model, data_loader, device='cuda'):\n    \"\"\"Evaluate the model on the given data loader.\"\"\"\n    model = model.to(device)\n    model.eval()\n    predictions = []\n    labels = []\n\n    with torch.no_grad():\n        for batch_data, batch_labels in data_loader:\n            batch_data = batch_data.to(device)\n            batch_labels = batch_labels.float().to(device)\n\n            outputs = model(batch_data).squeeze()\n            predictions.extend((outputs > 0).cpu().numpy())\n            labels.extend(batch_labels.cpu().numpy())\n\n    return np.array(predictions), np.array(labels)\n\ndef print_metrics(metrics):\n    \"\"\"Print the calculated metrics.\"\"\"\n    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n          f\"Precision: {metrics['precision']:.4f}, \"\n          f\"Recall: {metrics['recall']:.4f}, \"\n          f\"F1: {metrics['f1']:.4f}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:58.261327Z",
     "iopub.execute_input": "2024-09-26T10:43:58.261641Z",
     "iopub.status.idle": "2024-09-26T10:43:58.317367Z",
     "shell.execute_reply.started": "2024-09-26T10:43:58.261598Z",
     "shell.execute_reply": "2024-09-26T10:43:58.316339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Visualization Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    # predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs = model(batch_data).squeeze()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:58.318685Z",
     "iopub.execute_input": "2024-09-26T10:43:58.319066Z",
     "iopub.status.idle": "2024-09-26T10:43:58.376343Z",
     "shell.execute_reply.started": "2024-09-26T10:43:58.319030Z",
     "shell.execute_reply": "2024-09-26T10:43:58.375323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Data Processing Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Data Processing Functions\ndef load_model(model_class, model_path):\n    \"\"\"Load a trained model from a file.\"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n\n    model = model_class()\n    try:\n        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n        if not state_dict:\n            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n        model.load_state_dict(state_dict)\n    except Exception as e:\n        print(f\"Error loading model from {model_path}: {str(e)}\")\n        print(\"Initializing model with random weights instead.\")\n        return model  # Return the model with random initialization\n\n    return model.eval()\n\n\ndef get_test_results(model, test_loader, test_labels):\n    \"\"\"Get test results including patient information.\"\"\"\n    predictions, _ = evaluate_model(model, test_loader)\n    \n    results = []\n    for i, row in enumerate(test_labels.itertuples(index=False)):\n        result = {col: getattr(row, col) for col in test_labels.columns}\n        result['prediction'] = predictions[i]\n        results.append(result)\n    \n    return pd.DataFrame(results)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:58.377583Z",
     "iopub.execute_input": "2024-09-26T10:43:58.377946Z",
     "iopub.status.idle": "2024-09-26T10:43:58.434538Z",
     "shell.execute_reply.started": "2024-09-26T10:43:58.377901Z",
     "shell.execute_reply": "2024-09-26T10:43:58.433610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Main",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "## Main Execution\n",
    "def main():\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = ResNet2D18_MIL()  # Assuming this is your model class\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, LEARNING_RATE, DEVICE)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # Load best model\n",
    "    trained_model = load_model(ResNet2D18_MIL, MODEL_PATH)\n",
    "\n",
    "    # Evaluate model\n",
    "    predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    metrics = calculate_metrics(predictions, labels)\n",
    "    print_metrics(metrics)\n",
    "    \n",
    "    # Visualizations\n",
    "    plot_roc_curve(trained_model, test_loader, DEVICE)\n",
    "    plot_confusion_matrix(trained_model, test_loader, DEVICE)\n",
    "\n",
    "    # Select only the required columns\n",
    "    required_columns = ['patient_id', 'study_instance_uid', 'label']\n",
    "    test_labels = test_labels[required_columns]\n",
    "    \n",
    "    # Save results\n",
    "    results_df = get_test_results(trained_model, test_loader, test_labels)\n",
    "    results_df.to_csv('results.csv', index=False)\n",
    "    print(results_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T10:43:58.435918Z",
     "iopub.execute_input": "2024-09-26T10:43:58.436384Z",
     "iopub.status.idle": "2024-09-26T13:09:54.715381Z",
     "shell.execute_reply.started": "2024-09-26T10:43:58.436337Z",
     "shell.execute_reply": "2024-09-26T13:09:54.714281Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Visualizing Attention Weights and Images",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Show Attention Weights and Images for a Single Patient",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Function to test a single patient\ndef test_single_patient(model, patient_data):\n    model.eval()\n    with torch.no_grad():\n        for batch_data, batch_labels in patient_data:\n            batch_data = batch_data.to(device)  # Move data to device\n            outputs, gated_attention = model(batch_data)  # Get outputs and attention weights\n            outputs = outputs.squeeze()  # Remove extra dimension\n            \n            predictions = (outputs > 0).cpu().numpy()\n            \n            print(\"Predictions:\", predictions)\n            print(\"Labels: \", batch_labels.cpu().numpy())\n            # Print attention weights\n            attention_weights_np = gated_attention.cpu().numpy()  # Move to CPU for easier handling\n            print(\"Attention Weights for Slides:\")\n            \n            for i in range(attention_weights_np.shape[1]):  # Iterate over instances/slides\n                print(f'Slide {i+1}: Attention Weight: {attention_weights_np[0][i][0]}')  # Print weight for each slide\n                \n            # Plot the images\n            plot_images(batch_data[0].cpu(), batch_labels[0].cpu())\n            \n            break  # Only test one patient",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T13:09:54.717869Z",
     "iopub.execute_input": "2024-09-26T13:09:54.718216Z",
     "iopub.status.idle": "2024-09-26T13:09:54.785965Z",
     "shell.execute_reply.started": "2024-09-26T13:09:54.718179Z",
     "shell.execute_reply": "2024-09-26T13:09:54.785009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def plot_images(images, labels):\n    print(images.shape)\n    batch_size, num_images_per_batch, height, width, channels = images.shape\n    images = images.view(batch_size * num_images_per_batch, height, width, channels)\n    \n    print(f'Images shape: {images.shape}')\n    num_images = images.size(0) \n    images = images.numpy()\n    \n    plt.figure(figsize=(15, 15))\n    \n    for i in range(num_images):\n        plt.subplot(8, 8, i + 1)\n        plt.imshow(images[i], cmap='gray')\n        plt.title(f'Image {i + 1}')\n        plt.axis('off')\n    \n    plt.tight_layout() \n    plt.show()\n    \n# # Call the function with the test_loader\n# test_loader = get_test_loader(dicom_dir, test_labels, batch_size=2)\n# test_single_patient(trained_model, test_loader)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-26T13:09:54.790865Z",
     "iopub.execute_input": "2024-09-26T13:09:54.791158Z",
     "iopub.status.idle": "2024-09-26T13:09:54.849289Z",
     "shell.execute_reply.started": "2024-09-26T13:09:54.791127Z",
     "shell.execute_reply": "2024-09-26T13:09:54.848304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ]
}
