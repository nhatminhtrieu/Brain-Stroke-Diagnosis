{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 9652074,
     "sourceType": "datasetVersion",
     "datasetId": 5705276
    }
   ],
   "dockerImageVersionId": 30776,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25022.389189,
   "end_time": "2024-10-05T02:20:29.848670",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-04T19:23:27.459481",
   "version": "2.6.0"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Library",
   "metadata": {
    "papermill": {
     "duration": 0.015487,
     "end_time": "2024-10-04T19:23:30.290610",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.275123",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gpytorch\n",
    "!pip install wandb"
   ],
   "metadata": {
    "papermill": {
     "duration": 13.842163,
     "end_time": "2024-10-04T19:23:44.147775",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.305612",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:03:29.962323Z",
     "iopub.execute_input": "2024-11-09T12:03:29.962697Z",
     "iopub.status.idle": "2024-11-09T12:03:54.971291Z",
     "shell.execute_reply.started": "2024-11-09T12:03:29.962659Z",
     "shell.execute_reply": "2024-11-09T12:03:54.970161Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:14.383942Z",
     "start_time": "2024-11-15T14:20:13.054713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.24.3)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n",
      "Requirement already satisfied: wandb in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (0.18.5)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (5.28.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.17.0)\r\n",
      "Requirement already satisfied: setproctitle in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (72.1.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pydicom\nimport cv2\nfrom tqdm import tqdm\nfrom skimage.transform import resize\nfrom skimage.transform import rotate\nfrom scipy.ndimage import gaussian_filter\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\nimport torchvision.models as models\nfrom torchvision import transforms\n\nimport wandb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n\nimport gpytorch\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'",
   "metadata": {
    "papermill": {
     "duration": 11.793387,
     "end_time": "2024-10-04T19:23:55.956283",
     "exception": false,
     "start_time": "2024-10-04T19:23:44.162896",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:03:54.973321Z",
     "iopub.execute_input": "2024-11-09T12:03:54.973660Z",
     "iopub.status.idle": "2024-11-09T12:04:04.345572Z",
     "shell.execute_reply.started": "2024-11-09T12:03:54.973624Z",
     "shell.execute_reply": "2024-11-09T12:04:04.344540Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.011305Z",
     "start_time": "2024-11-15T14:20:14.388145Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "import warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)",
   "metadata": {
    "papermill": {
     "duration": 0.02447,
     "end_time": "2024-10-04T19:23:55.997833",
     "exception": false,
     "start_time": "2024-10-04T19:23:55.973363",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.346735Z",
     "iopub.execute_input": "2024-11-09T12:04:04.347247Z",
     "iopub.status.idle": "2024-11-09T12:04:04.352304Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.347210Z",
     "shell.execute_reply": "2024-11-09T12:04:04.351162Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.036030Z",
     "start_time": "2024-11-15T14:20:16.034346Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## Init GPU",
   "metadata": {
    "papermill": {
     "duration": 0.024749,
     "end_time": "2024-10-04T19:23:56.038412",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.013663",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# Initialize GPU Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\nelse:\n    print(\"No GPU available. Training will run on CPU.\")\n\nprint(device)",
   "metadata": {
    "papermill": {
     "duration": 0.105269,
     "end_time": "2024-10-04T19:23:56.159555",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.054286",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.354377Z",
     "iopub.execute_input": "2024-11-09T12:04:04.354684Z",
     "iopub.status.idle": "2024-11-09T12:04:04.410802Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.354653Z",
     "shell.execute_reply": "2024-11-09T12:04:04.409843Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.105098Z",
     "start_time": "2024-11-15T14:20:16.083569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2",
   "metadata": {
    "papermill": {
     "duration": 0.081958,
     "end_time": "2024-10-04T19:23:56.258405",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.176447",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.411954Z",
     "iopub.execute_input": "2024-11-09T12:04:04.412279Z",
     "iopub.status.idle": "2024-11-09T12:04:04.479489Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.412244Z",
     "shell.execute_reply": "2024-11-09T12:04:04.478787Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.165134Z",
     "start_time": "2024-11-15T14:20:16.145631Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "## Config Info",
   "metadata": {
    "papermill": {
     "duration": 0.016234,
     "end_time": "2024-10-04T19:23:56.291671",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.275437",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 1\n",
    "\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "TEST_BATCH_SIZE = 2\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "\n",
    "MAX_SLICES = 60\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "INDUCING_POINTS = 128\n",
    "\n",
    "# TARGET_LABELS = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "TARGET_LABELS = ['intraparenchymal']\n",
    "\n",
    "MODEL_PATH = 'results/trained_model.pth'\n",
    "DEVICE = 'cuda'"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.076674,
     "end_time": "2024-10-04T19:23:56.384583",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.307909",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.480520Z",
     "iopub.execute_input": "2024-11-09T12:04:04.480805Z",
     "iopub.status.idle": "2024-11-09T12:04:04.541310Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.480773Z",
     "shell.execute_reply": "2024-11-09T12:04:04.540405Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.220804Z",
     "start_time": "2024-11-15T14:20:16.193519Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# Kaggle and local switch\nKAGGLE = os.path.exists('/kaggle')\nprint(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\nROOT_DIR = '/kaggle/input/rsna-mil-training/' if KAGGLE else None\nDATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-mil-training/'\nDICOM_DIR = DATA_DIR\nCSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_1000_scan_subset.csv'\n# SLICE_LABEL_PATH = ROOT_DIR + \"sorted_training_dataset_with_labels.csv\" if KAGGLE else './data_analyze/sorted_training_dataset_with_labels.csv'\n\ndicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n# Load patient scan labels\npatient_scan_labels = pd.read_csv(CSV_PATH)\n# patient_slice_labels = pd.read_csv(SLICE_LABEL_PATH)\nos.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)",
   "metadata": {
    "papermill": {
     "duration": 4.085647,
     "end_time": "2024-10-04T19:24:00.487263",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.401616",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.542362Z",
     "iopub.execute_input": "2024-11-09T12:04:04.542646Z",
     "iopub.status.idle": "2024-11-09T12:04:04.649755Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.542615Z",
     "shell.execute_reply": "2024-11-09T12:04:04.648985Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.285656Z",
     "start_time": "2024-11-15T14:20:16.260136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret(\"Wandb key\")\n",
    "# \n",
    "# wandb.login(key=key, relogin=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.650858Z",
     "iopub.execute_input": "2024-11-09T12:04:04.651199Z",
     "iopub.status.idle": "2024-11-09T12:04:06.250597Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.651166Z",
     "shell.execute_reply": "2024-11-09T12:04:06.249864Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.327803Z",
     "start_time": "2024-11-15T14:20:16.312006Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "patient_scan_labels.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.251742Z",
     "iopub.execute_input": "2024-11-09T12:04:06.252251Z",
     "iopub.status.idle": "2024-11-09T12:04:06.331892Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.252216Z",
     "shell.execute_reply": "2024-11-09T12:04:06.331086Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.382655Z",
     "start_time": "2024-11-15T14:20:16.363562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    patient_id study_instance_uid  \\\n",
       "0  ID_2e010e33      ID_bda0f47e84   \n",
       "1  ID_43dd2890      ID_43af13416f   \n",
       "2  ID_93753256      ID_1ee74896bd   \n",
       "3  ID_2d49911c      ID_a71e7c6817   \n",
       "4  ID_cd2cd174      ID_d02e9abf35   \n",
       "\n",
       "                                              images  \\\n",
       "0  ['ID_b9035cb1e.dcm', 'ID_0713bed86.dcm', 'ID_5...   \n",
       "1  ['ID_23864d3ba.dcm', 'ID_f1c65b76e.dcm', 'ID_5...   \n",
       "2  ['ID_a3a37df48.dcm', 'ID_61dd5485f.dcm', 'ID_9...   \n",
       "3  ['ID_93d9cf661.dcm', 'ID_efd4e2780.dcm', 'ID_7...   \n",
       "4  ['ID_b437d2e19.dcm', 'ID_0a919428c.dcm', 'ID_d...   \n",
       "\n",
       "                                              labels  patient_label  length  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0      48  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0      33  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0      30  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0      40  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0      33  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_2e010e33</td>\n",
       "      <td>ID_bda0f47e84</td>\n",
       "      <td>['ID_b9035cb1e.dcm', 'ID_0713bed86.dcm', 'ID_5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_43dd2890</td>\n",
       "      <td>ID_43af13416f</td>\n",
       "      <td>['ID_23864d3ba.dcm', 'ID_f1c65b76e.dcm', 'ID_5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_93753256</td>\n",
       "      <td>ID_1ee74896bd</td>\n",
       "      <td>['ID_a3a37df48.dcm', 'ID_61dd5485f.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_2d49911c</td>\n",
       "      <td>ID_a71e7c6817</td>\n",
       "      <td>['ID_93d9cf661.dcm', 'ID_efd4e2780.dcm', 'ID_7...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_cd2cd174</td>\n",
       "      <td>ID_d02e9abf35</td>\n",
       "      <td>['ID_b437d2e19.dcm', 'ID_0a919428c.dcm', 'ID_d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "metadata": {
    "papermill": {
     "duration": 0.016012,
     "end_time": "2024-10-04T19:24:00.520265",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.504253",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "def correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef window_image(dcm, window_center, window_width):    \n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n    \n    # Resize\n    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n   \n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 40, 80)\n    subdural_img = window_image(dcm, 80, 200)\n    soft_img = window_image(dcm, 40, 380)\n    \n    brain_img = (brain_img - 0) / 80\n    subdural_img = (subdural_img - (-20)) / 200\n    soft_img = (soft_img - (-150)) / 380\n\n    if CHANNELS == 3:\n        bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n    else:\n        bsb_img = brain_img\n    return bsb_img.astype(np.float16)",
   "metadata": {
    "papermill": {
     "duration": 0.079596,
     "end_time": "2024-10-04T19:24:00.617176",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.537580",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.335428Z",
     "iopub.execute_input": "2024-11-09T12:04:06.335746Z",
     "iopub.status.idle": "2024-11-09T12:04:06.404940Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.335713Z",
     "shell.execute_reply": "2024-11-09T12:04:06.403960Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.450069Z",
     "start_time": "2024-11-15T14:20:16.433039Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n    # Check if type of slice is dicom or an empty numpy array\n    if (type(slice) == np.ndarray):\n        slice = resize(slice, target_size, anti_aliasing=True)\n        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n        if CHANNELS == 3:\n            return multichannel_slice.astype(np.float16)\n        else:\n            return slice.astype(np.float16)\n    else:\n        slice = bsb_window(slice)\n        return slice.astype(np.float16)",
   "metadata": {
    "papermill": {
     "duration": 0.076531,
     "end_time": "2024-10-04T19:24:00.710787",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.634256",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.406284Z",
     "iopub.execute_input": "2024-11-09T12:04:06.406824Z",
     "iopub.status.idle": "2024-11-09T12:04:06.470535Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.406773Z",
     "shell.execute_reply": "2024-11-09T12:04:06.469524Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.489262Z",
     "start_time": "2024-11-15T14:20:16.472531Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "def read_dicom_folder(folder_path):\n    slices = []\n    for filename in sorted(os.listdir(folder_path))[:MAX_SLICES]:  # Limit to MAX_SLICES\n        if filename.endswith(\".dcm\"):\n            file_path = os.path.join(folder_path, filename)\n            ds = pydicom.dcmread(file_path)\n            slices.append(ds)\n            \n    # Sort slices by images position (z-coordinate) in ascending order\n    slices = sorted(slices, key=lambda x: float(x.ImagePositionPatient[2]))\n    \n    # Pad with black images if necessary\n    while len(slices) < MAX_SLICES:\n        slices.append(np.zeros_like(slices[0].pixel_array))\n    \n    return slices[:MAX_SLICES]  # Ensure we return exactly MAX_SLICES",
   "metadata": {
    "papermill": {
     "duration": 0.076142,
     "end_time": "2024-10-04T19:24:00.803356",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.727214",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.471678Z",
     "iopub.execute_input": "2024-11-09T12:04:06.471972Z",
     "iopub.status.idle": "2024-11-09T12:04:06.536802Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.471940Z",
     "shell.execute_reply": "2024-11-09T12:04:06.535848Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.538228Z",
     "start_time": "2024-11-15T14:20:16.521527Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset and DataLoader",
   "metadata": {
    "papermill": {
     "duration": 0.016487,
     "end_time": "2024-10-04T19:24:00.836476",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.819989",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Splitting the Dataset",
   "metadata": {
    "papermill": {
     "duration": 0.015844,
     "end_time": "2024-10-04T19:24:00.868235",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.852391",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n    # Extract the labels from the DataFrame\n    labels = patient_scan_labels['patient_label']\n\n    # First, split off the test set\n    train_val_labels, test_labels = train_test_split(\n        patient_scan_labels, \n        test_size=test_size, \n        stratify=labels, \n        random_state=random_state\n    )\n\n    # Calculate the validation size relative to the train_val set\n    val_size_adjusted = val_size / (1 - test_size)\n\n    # Split the train_val set into train and validation sets\n    train_labels, val_labels = train_test_split(\n        train_val_labels, \n        test_size=val_size_adjusted, \n        stratify=train_val_labels['patient_label'], \n        random_state=random_state\n    )\n\n    return train_labels, val_labels, test_labels",
   "metadata": {
    "papermill": {
     "duration": 0.075616,
     "end_time": "2024-10-04T19:24:00.959889",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.884273",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.537885Z",
     "iopub.execute_input": "2024-11-09T12:04:06.538200Z",
     "iopub.status.idle": "2024-11-09T12:04:06.603427Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.538168Z",
     "shell.execute_reply": "2024-11-09T12:04:06.602545Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.584023Z",
     "start_time": "2024-11-15T14:20:16.565720Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "### Processing the Data",
   "metadata": {
    "papermill": {
     "duration": 0.017012,
     "end_time": "2024-10-04T19:24:00.993325",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.976313",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "def process_patient_data(dicom_dir, row, num_instances=12, depth=5):\n    patient_id = row['patient_id'].replace('ID_', '')\n    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n    \n    folder_name = f\"{patient_id}_{study_instance_uid}\"\n    folder_path = os.path.join(dicom_dir, folder_name)\n    \n    if os.path.exists(folder_path):\n        slices = read_dicom_folder(folder_path)\n        \n        # Ensure we have enough slices to create the specified instances\n        if len(slices) < depth * num_instances:\n            print(f\"Not enough slices for patient {patient_id}: found {len(slices)}, needed {depth * num_instances}\")\n            return None, None\n        \n        preprocessed_slices = [torch.tensor(preprocess_slice(slice), dtype=torch.float32) for slice in slices]  # Convert to tensor\n        \n        # Stack preprocessed slices into an array\n        preprocessed_slices = torch.stack(preprocessed_slices, dim=0)  # (num_slices, height, width, channels)\n    \n        # Labels are already in list form, so just convert them to a tensor\n        labels = torch.tensor(row['labels'], dtype=torch.long)\n        \n        # Fill labels with 0s if necessary\n        if len(preprocessed_slices) > len(labels):\n            padded_labels = torch.zeros(len(preprocessed_slices), dtype=torch.long)\n            padded_labels[:len(labels)] = labels\n        else:\n            padded_labels = labels[:len(preprocessed_slices)]\n        \n        return preprocessed_slices, padded_labels\n    \n    else:\n        print(f\"Folder not found: {folder_path}\")\n        return None, None",
   "metadata": {
    "papermill": {
     "duration": 0.076813,
     "end_time": "2024-10-04T19:24:01.086193",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.009380",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.604538Z",
     "iopub.execute_input": "2024-11-09T12:04:06.604898Z",
     "iopub.status.idle": "2024-11-09T12:04:06.673120Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.604856Z",
     "shell.execute_reply": "2024-11-09T12:04:06.672312Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.633452Z",
     "start_time": "2024-11-15T14:20:16.611396Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "### Augmentation",
   "metadata": {
    "papermill": {
     "duration": 0.016194,
     "end_time": "2024-10-04T19:24:01.118549",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.102355",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# class DatasetAugmentor:\n#     def __init__(self, height, width, seed=None):\n#         self.height = height\n#         self.width = width\n#         self.seed = seed\n        \n#         # Store parameters for consistency checks with non-overlapping ranges\n#         self.min_params = self._create_transform(5, (0.05, 0.05), (0.95, 1.05), \n#                                                   brightness_range=0.05, contrast_range=0.05,\n#                                                   blur_sigma_range=(0.1, 0.2), apply_elastic=False, level_name='min')\n#         self.med_params = self._create_transform(10, (0.1, 0.1), (0.9, 1.1), \n#                                                   brightness_range=0.1, contrast_range=0.1,\n#                                                   blur_sigma_range=(0.2, 0.5), apply_elastic=True, level_name='med')\n#         self.max_params = self._create_transform(15, (0.2, 0.2), (0.8, 1.2), \n#                                                   brightness_range=0.2, contrast_range=0.2,\n#                                                   blur_sigma_range=(0.5, 1.0), apply_elastic=True, level_name='max')\n\n#     def _sample_value(self, value_range):\n#         if isinstance(value_range, tuple):\n#             random.seed(self.seed)\n#             return random.uniform(value_range[0], value_range[1])\n#         return value_range\n\n#     def _create_transform(self, degrees, translate_range, scale_range,\n#                       brightness_range, contrast_range,\n#                       blur_sigma_range, apply_elastic, level_name):\n#         print(f\"Creating '{level_name}' transform with parameters:\")\n\n#         # Sample specific values and take the absolute value for positivity\n#         sampled_degrees = abs(self._sample_value((-degrees, degrees)) if isinstance(degrees, int) else degrees)\n#         sampled_translate_x = abs(self._sample_value(translate_range[0]))\n#         sampled_translate_y = abs(self._sample_value(translate_range[1]))\n#         sampled_scale = self._sample_value(scale_range)\n#         sampled_brightness = self._sample_value(brightness_range)\n#         sampled_contrast = self._sample_value(contrast_range)\n#         sampled_blur_sigma = self._sample_value(blur_sigma_range)\n\n#         # Print specific sampled values\n#         print(f\"  Degrees: {sampled_degrees}, Translate: ({sampled_translate_x}, {sampled_translate_y}), Scale: {sampled_scale}\")\n#         print(f\"  Brightness: {sampled_brightness}, Contrast: {sampled_contrast}\")\n#         print(f\"  Blur Sigma: {sampled_blur_sigma}, Apply Elastic: {apply_elastic}\")\n\n#         return {\n#             \"degrees\": sampled_degrees,\n#             \"translate\": (sampled_translate_x, sampled_translate_y),\n#             \"scale\": sampled_scale,\n#             \"brightness\": sampled_brightness,\n#             \"contrast\": sampled_contrast,\n#             \"blur_sigma\": sampled_blur_sigma,\n#             \"apply_elastic\": apply_elastic\n#         }\n\n#     def apply_transform(self, image, level):\n#         channels = image.shape[0]\n        \n#         if self.seed is not None:\n#             torch.manual_seed(self.seed)\n#             random.seed(self.seed)\n\n#         if level == 0:\n#             params = self.min_params\n#             transform = self._get_transform(params, channels)\n#         elif level == 1:\n#             params = self.med_params\n#             transform = self._get_transform(params, channels)\n#         else:\n#             params = self.max_params\n#             transform = self._get_transform(params, channels)\n\n#         transformed_image = transform(image)        \n#         return transformed_image\n\n#     def _get_transform(self, params, channels=3):\n#         transform_list = [\n#             transforms.ToPILImage(),\n#             transforms.RandomHorizontalFlip(p=0.5),\n#             transforms.RandomAffine(degrees=params[\"degrees\"], translate=params[\"translate\"], scale=(params[\"scale\"], params[\"scale\"])),\n#             transforms.ColorJitter(brightness=params[\"brightness\"], contrast=params[\"contrast\"]),\n#             transforms.GaussianBlur(kernel_size=(3, 3), sigma=params[\"blur_sigma\"]),\n#             transforms.RandomApply([transforms.ElasticTransform()] if params[\"apply_elastic\"] else [], p=0.3),\n#             transforms.Resize(256),\n#             transforms.CenterCrop(HEIGHT),\n#             transforms.ToTensor(),\n#         ]\n\n#         if channels == 3:\n#             transform_list.extend([\n#                 transforms.Normalize(mean=[0.16774411, 0.1360026, 0.19076315], std=[0.3101935, 0.27605791, 0.30469988]),\n#                 transforms.RandomApply([self._channel_shuffle], p=0.3)\n#             ])\n#         elif channels == 1:\n#             transform_list.append(transforms.Normalize(mean=[0.16774411], std=[0.3101935]))\n\n#         return transforms.Compose(transform_list)\n\n#     def _channel_shuffle(self, tensor):\n#         torch.manual_seed(self.seed)\n#         channels = tensor.shape[0]\n#         indices = torch.randperm(channels)\n#         return tensor[indices]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.674617Z",
     "iopub.execute_input": "2024-11-09T12:04:06.675050Z",
     "iopub.status.idle": "2024-11-09T12:04:06.741026Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.675004Z",
     "shell.execute_reply": "2024-11-09T12:04:06.740160Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.681705Z",
     "start_time": "2024-11-15T14:20:16.660049Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "class DatasetAugmentor:\n    def __init__(self, height, width, levels=2, seed=None):\n        self.height = height\n        self.width = width\n        self.levels = levels  # Dynamic number of levels\n        self.seed = seed\n        self.params = []\n\n        # Create different levels of transforms based on the number of levels specified\n        for i in range(levels):\n            factor = (i + 1) / levels\n            self.params.append(\n                self._create_transform(\n                    degrees=int(15 * factor), \n                    translate_range=(0.2 * factor, 0.2 * factor),\n                    scale_range=(1 - 0.2 * factor, 1 + 0.2 * factor),\n                    brightness_range=0.2 * factor,\n                    contrast_range=0.2 * factor,\n                    blur_sigma_range=(0.5 * factor, 1.0 * factor),\n                    apply_elastic=(i >= levels // 2),\n                    level_name=f'level_{i + 1}'\n                )\n            )\n\n    def _sample_value(self, value_range):\n        if isinstance(value_range, tuple):\n            random.seed(self.seed)\n            return random.uniform(value_range[0], value_range[1])\n        return value_range\n\n    def _create_transform(self, degrees, translate_range, scale_range, brightness_range, contrast_range, blur_sigma_range, apply_elastic, level_name):\n        print(f\"Creating '{level_name}' transform with parameters:\")\n        sampled_values = {\n            \"degrees\": abs(self._sample_value((-degrees, degrees))),\n            \"translate\": (abs(self._sample_value(translate_range[0])), abs(self._sample_value(translate_range[1]))),\n            \"scale\": self._sample_value(scale_range),\n            \"brightness\": self._sample_value(brightness_range),\n            \"contrast\": self._sample_value(contrast_range),\n            \"blur_sigma\": self._sample_value(blur_sigma_range),\n            \"apply_elastic\": apply_elastic\n        }\n        \n        print(sampled_values)\n        return sampled_values\n\n    def apply_transform(self, image, level):\n        params = self.params[level]\n        transform = self._get_transform(params, channels=image.shape[0])\n        return transform(image)\n\n    def _get_transform(self, params, channels=3):\n        transform_list = [\n            transforms.ToPILImage(),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomAffine(degrees=params[\"degrees\"], translate=params[\"translate\"], scale=(params[\"scale\"], params[\"scale\"])),\n            transforms.ColorJitter(brightness=params[\"brightness\"], contrast=params[\"contrast\"]),\n            transforms.GaussianBlur(kernel_size=(3, 3), sigma=params[\"blur_sigma\"]),\n            transforms.RandomApply([transforms.ElasticTransform()] if params[\"apply_elastic\"] else [], p=0.3),\n            transforms.Resize(256),\n            transforms.CenterCrop(self.height),\n            transforms.ToTensor(),\n        ]\n\n        if channels == 3:\n            transform_list.extend([\n                transforms.Normalize(mean=[0.16774411, 0.1360026, 0.19076315], std=[0.3101935, 0.27605791, 0.30469988]),\n                transforms.RandomApply([self._channel_shuffle], p=0.3)\n            ])\n        elif channels == 1:\n            transform_list.append(transforms.Normalize(mean=[0.16774411], std=[0.3101935]))\n\n        return transforms.Compose(transform_list)\n\n    def _channel_shuffle(self, tensor):\n        torch.manual_seed(self.seed)\n        channels = tensor.shape[0]\n        indices = torch.randperm(channels)\n        return tensor[indices]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.742336Z",
     "iopub.execute_input": "2024-11-09T12:04:06.742670Z",
     "iopub.status.idle": "2024-11-09T12:04:06.817331Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.742638Z",
     "shell.execute_reply": "2024-11-09T12:04:06.816387Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.728421Z",
     "start_time": "2024-11-15T14:20:16.706214Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "augmentor = DatasetAugmentor(224, 224, seed=42)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.818519Z",
     "iopub.execute_input": "2024-11-09T12:04:06.818878Z",
     "iopub.status.idle": "2024-11-09T12:04:06.881845Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.818836Z",
     "shell.execute_reply": "2024-11-09T12:04:06.881035Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.776128Z",
     "start_time": "2024-11-15T14:20:16.759971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'level_1' transform with parameters:\n",
      "{'degrees': 1.9519751784103718, 'translate': (0.1, 0.1), 'scale': 1.027885359691577, 'brightness': 0.1, 'contrast': 0.1, 'blur_sigma': 0.4098566996144709, 'apply_elastic': False}\n",
      "Creating 'level_2' transform with parameters:\n",
      "{'degrees': 4.182803953736514, 'translate': (0.2, 0.2), 'scale': 1.0557707193831534, 'brightness': 0.2, 'contrast': 0.2, 'blur_sigma': 0.8197133992289418, 'apply_elastic': True}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset Generator",
   "metadata": {
    "papermill": {
     "duration": 0.015494,
     "end_time": "2024-10-04T19:24:01.252123",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.236629",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# class MedicalScanDataset(Dataset):\n#     \"\"\"\n#     Base dataset class for processing medical scan data.\n#     Handles data loading, preprocessing, and normalization of medical scan images.\n#     \"\"\"\n#     def __init__(self, data_dir, patient_scan_labels, augmentor):\n#         self.data_dir = data_dir\n#         self.patient_scan_labels = self._parse_patient_scan_labels(patient_scan_labels)\n#         self.augmentor = augmentor\n\n#     def _parse_patient_scan_labels(self, patient_scan_labels):\n#         \"\"\"Parse and validate patient scan labels.\"\"\"\n#         patient_scan_labels['images'] = patient_scan_labels['images'].apply(\n#             lambda x: eval(x) if isinstance(x, str) else x\n#         )\n#         patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n#             lambda x: eval(x) if isinstance(x, str) else x\n#         )\n#         patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n#         return patient_scan_labels\n\n#     def __len__(self):\n#         \"\"\"Return the total number of samples in the dataset.\"\"\"\n#         return len(self.patient_scan_labels)\n\n#     def __getitem__(self, idx):\n#         \"\"\"Get a single item from the dataset.\"\"\"\n#         row = self.patient_scan_labels.iloc[idx]\n#         preprocessed_slices, labels = self._process_patient_data(row)\n        \n#         if preprocessed_slices is None:\n#             return None, None, None\n            \n#         preprocessed_slices = self._prepare_tensor(preprocessed_slices)\n#         patient_label = torch.tensor(bool(row['patient_label']), dtype=torch.uint8)\n        \n#         return preprocessed_slices, labels, patient_label\n\n#     def _process_patient_data(self, row):\n#         \"\"\"Process patient data to get preprocessed slices and labels.\"\"\"\n#         return process_patient_data(self.data_dir, row)\n\n#     def _prepare_tensor(self, preprocessed_slices):\n#         \"\"\"Convert preprocessed slices to normalized tensor.\"\"\"\n#         preprocessed_slices = np.array(preprocessed_slices)\n#         preprocessed_slices = torch.tensor(preprocessed_slices, dtype=torch.float32)\n\n#         if preprocessed_slices.ndim == 3:  # Grayscale\n#             preprocessed_slices = preprocessed_slices.unsqueeze(1)  # Add channel dimension\n#             return torch.stack([self.augmentor.apply_transform(img, level=0) for img in preprocessed_slices])\n#         elif preprocessed_slices.ndim == 4:  # RGB\n#             preprocessed_slices = preprocessed_slices.permute(0, 3, 1, 2) \n#             return torch.stack([self.augmentor.apply_transform(img, level=0) for img in preprocessed_slices])\n#         else:\n#             raise ValueError(f\"Unexpected number of dimensions: {preprocessed_slices.ndim}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.883210Z",
     "iopub.execute_input": "2024-11-09T12:04:06.883884Z",
     "iopub.status.idle": "2024-11-09T12:04:06.948390Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.883840Z",
     "shell.execute_reply": "2024-11-09T12:04:06.947409Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.827330Z",
     "start_time": "2024-11-15T14:20:16.811046Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "class MedicalScanDataset(Dataset):\n    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n        self.data_dir = data_dir\n        self.patient_scan_labels = self._parse_patient_scan_labels(patient_scan_labels)\n        self.augmentor = augmentor\n        \n    def _parse_patient_scan_labels(self, patient_scan_labels):\n        \"\"\"Parse and validate patient scan labels.\"\"\"\n        patient_scan_labels['images'] = patient_scan_labels['images'].apply(\n            lambda x: eval(x) if isinstance(x, str) else x\n        )\n        patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n            lambda x: eval(x) if isinstance(x, str) else x\n        )\n        patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n        return patient_scan_labels\n    \n    def _process_patient_data(self, row):\n        \"\"\"Process patient data to get preprocessed slices and labels.\"\"\"\n        return process_patient_data(self.data_dir, row)\n\n    def __len__(self):\n        return len(self.patient_scan_labels) * (self.augmentor.levels if self.augmentor else 1)\n\n    def __getitem__(self, idx):\n        patient_idx = idx // (self.augmentor.levels if self.augmentor else 1)\n        aug_level = idx % (self.augmentor.levels if self.augmentor else 1)\n\n        row = self.patient_scan_labels.iloc[patient_idx]\n        preprocessed_slices, labels = self._process_patient_data(row)\n\n        if preprocessed_slices is None:\n            return None, None, None\n\n        preprocessed_slices = self._prepare_tensor(preprocessed_slices, aug_level if self.augmentor else None)\n        patient_label = torch.tensor(bool(row['patient_label']), dtype=torch.uint8)\n\n        return preprocessed_slices, labels, patient_label\n\n#     def _prepare_tensor(self, preprocessed_slices, aug_level):\n#         preprocessed_slices = np.array(preprocessed_slices)\n#         preprocessed_slices = torch.tensor(preprocessed_slices, dtype=torch.float32)\n\n#         if self.augmentor and aug_level is not None:\n#             if preprocessed_slices.ndim == 3:\n#                 preprocessed_slices = preprocessed_slices.unsqueeze(1)\n#                 return torch.stack([self.augmentor.apply_transform(img, aug_level) for img in preprocessed_slices])\n#             elif preprocessed_slices.ndim == 4:\n#                 preprocessed_slices = preprocessed_slices.permute(0, 3, 1, 2)\n#                 return torch.stack([self.augmentor.apply_transform(img, aug_level) for img in preprocessed_slices])\n#         else:\n#             return preprocessed_slices\n\n    def _prepare_tensor(self, preprocessed_slices, aug_level):\n        # Convert to numpy array and then to torch tensor\n        preprocessed_slices = np.array(preprocessed_slices)\n        preprocessed_slices = torch.tensor(preprocessed_slices, dtype=torch.float32)\n\n        # Add an additional dimension for channel if it's missing (no augmentor)\n        if preprocessed_slices.ndim == 3:\n            preprocessed_slices = preprocessed_slices.unsqueeze(1)  # shape: [slices, 1, H, W]\n\n        # Apply augmentation if augmentor is specified\n        if self.augmentor and aug_level is not None:\n            if preprocessed_slices.ndim == 4:  # Ensure it has the [slices, channels, H, W] format\n                return torch.stack([self.augmentor.apply_transform(img, aug_level) for img in preprocessed_slices])\n\n        return preprocessed_slices  # Return without augmentation if augmentor is None",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.949800Z",
     "iopub.execute_input": "2024-11-09T12:04:06.950129Z",
     "iopub.status.idle": "2024-11-09T12:04:07.020925Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.950072Z",
     "shell.execute_reply": "2024-11-09T12:04:07.020108Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.881730Z",
     "start_time": "2024-11-15T14:20:16.857832Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.022171Z",
     "iopub.execute_input": "2024-11-09T12:04:07.022528Z",
     "iopub.status.idle": "2024-11-09T12:04:07.085840Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.022481Z",
     "shell.execute_reply": "2024-11-09T12:04:07.084942Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:16.929872Z",
     "start_time": "2024-11-15T14:20:16.908462Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.086910Z",
     "iopub.execute_input": "2024-11-09T12:04:07.087252Z",
     "iopub.status.idle": "2024-11-09T12:04:07.255534Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.087214Z",
     "shell.execute_reply": "2024-11-09T12:04:07.254706Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.010802Z",
     "start_time": "2024-11-15T14:20:16.956586Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "len(original_dataset)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.256674Z",
     "iopub.execute_input": "2024-11-09T12:04:07.256977Z",
     "iopub.status.idle": "2024-11-09T12:04:07.320972Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.256945Z",
     "shell.execute_reply": "2024-11-09T12:04:07.320056Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.032373Z",
     "start_time": "2024-11-15T14:20:17.016412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "x,y,z = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.322156Z",
     "iopub.execute_input": "2024-11-09T12:04:07.322491Z",
     "iopub.status.idle": "2024-11-09T12:04:08.712840Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.322458Z",
     "shell.execute_reply": "2024-11-09T12:04:08.711911Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.221903Z",
     "start_time": "2024-11-15T14:20:17.072941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 224, 224]) torch.Size([60]) torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "# # Check if the returned data is valid\n",
    "# if x is not None:\n",
    "#     # Convert the tensor to a numpy array\n",
    "#     x_np = x.numpy()\n",
    "#\n",
    "#     # Check the number of dimensions and squeeze if necessary\n",
    "#     if x_np.ndim == 4:  # RGB images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i].transpose(1, 2, 0))  # Convert CHW to HWC\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     elif x_np.ndim == 3:  # Grayscale images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i], cmap='gray')\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unexpected number of dimensions: {x_np.ndim}\")\n",
    "# else:\n",
    "#     print(\"No data available for this patient.\")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:08.713992Z",
     "iopub.execute_input": "2024-11-09T12:04:08.714306Z",
     "iopub.status.idle": "2024-11-09T12:04:10.720210Z",
     "shell.execute_reply.started": "2024-11-09T12:04:08.714273Z",
     "shell.execute_reply": "2024-11-09T12:04:10.719091Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.244321Z",
     "start_time": "2024-11-15T14:20:17.228546Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.074769,
     "end_time": "2024-10-04T19:24:01.438666",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.363897",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:10.721483Z",
     "iopub.execute_input": "2024-11-09T12:04:10.721855Z",
     "iopub.status.idle": "2024-11-09T12:04:10.790322Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.721821Z",
     "shell.execute_reply": "2024-11-09T12:04:10.789279Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.291393Z",
     "start_time": "2024-11-15T14:20:17.272780Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": "## CNN Feature Extractor",
   "metadata": {
    "papermill": {
     "duration": 0.015548,
     "end_time": "2024-10-04T19:24:01.470464",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.454916",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "### GP Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GPModel(gpytorch.models.ApproximateGP):\n    def __init__(self, inducing_points):\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(GPModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:10.791523Z",
     "iopub.execute_input": "2024-11-09T12:04:10.791820Z",
     "iopub.status.idle": "2024-11-09T12:04:10.858127Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.791789Z",
     "shell.execute_reply": "2024-11-09T12:04:10.857159Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.340529Z",
     "start_time": "2024-11-15T14:20:17.319609Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DeepGP Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.389468Z",
     "start_time": "2024-11-15T14:20:17.366398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gpytorch.models.deep_gps import DeepGPLayer, DeepGP\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "\n",
    "class ToyDeepGPHiddenLayer(DeepGPLayer):\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=32, mean_type='constant'):\n",
    "        if output_dims is None:\n",
    "            inducing_points = torch.randn(num_inducing, input_dims)\n",
    "            batch_shape = torch.Size([])\n",
    "        else:\n",
    "            inducing_points = torch.randn(output_dims, num_inducing, input_dims)\n",
    "            batch_shape = torch.Size([output_dims])\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_shape=batch_shape\n",
    "        )\n",
    "\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super(ToyDeepGPHiddenLayer, self).__init__(variational_strategy, input_dims, output_dims)\n",
    "\n",
    "        if mean_type == 'constant':\n",
    "            self.mean_module = ConstantMean(batch_shape=batch_shape)\n",
    "        else:\n",
    "            self.mean_module = LinearMean(input_dims)\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=batch_shape, ard_num_dims=input_dims),\n",
    "            batch_shape=batch_shape, ard_num_dims=None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    def __call__(self, x, *other_inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Overriding __call__ isn't strictly necessary, but it lets us add concatenation based skip connections\n",
    "        easily. For example, hidden_layer2(hidden_layer1_outputs, inputs) will pass the concatenation of the first\n",
    "        hidden layer's outputs and the input data to hidden_layer2.\n",
    "        \"\"\"\n",
    "        if len(other_inputs):\n",
    "            if isinstance(x, gpytorch.distributions.MultitaskMultivariateNormal):\n",
    "                x = x.rsample()\n",
    "\n",
    "            processed_inputs = [\n",
    "                inp.unsqueeze(0).expand(gpytorch.settings.num_likelihood_samples.value(), *inp.shape)\n",
    "                for inp in other_inputs\n",
    "            ]\n",
    "\n",
    "            x = torch.cat([x] + processed_inputs, dim=-1)\n",
    "\n",
    "        return super().__call__(x, are_samples=bool(len(other_inputs)))\n",
    "    \n",
    "    \n",
    "# class DeepGP(DeepGP):\n",
    "#     def __init__(self, input_dims=512, hidden_dims=128, output_dims=10):\n",
    "#         hidden_layer = ToyDeepGPHiddenLayer(\n",
    "#             input_dims=input_dims,\n",
    "#             output_dims=hidden_dims,\n",
    "#             mean_type='linear',\n",
    "#         )\n",
    "#         last_layer = ToyDeepGPHiddenLayer(\n",
    "#             input_dims=hidden_layer.output_dims,\n",
    "#             output_dims=output_dims,\n",
    "#             mean_type='constant',\n",
    "#         )\n",
    "# \n",
    "#         super().__init__()\n",
    "# \n",
    "#         self.hidden_layer = hidden_layer\n",
    "#         self.last_layer = last_layer\n",
    "#         self.likelihood = GaussianLikelihood()\n",
    "# \n",
    "#     def forward(self, inputs):\n",
    "#         hidden_rep1 = self.hidden_layer(inputs)\n",
    "#         output = self.last_layer(hidden_rep1)\n",
    "#         return output\n",
    "# \n",
    "#     # def predict(self, test_loader):\n",
    "#     #     with torch.no_grad():\n",
    "#     #         mus = []\n",
    "#     #         variances = []\n",
    "#     #         lls = []\n",
    "#     #         for x_batch, y_batch in test_loader:\n",
    "#     #             preds = self.likelihood(self(x_batch))\n",
    "#     #             mean = preds.mean\n",
    "#     #             var = preds.variance\n",
    "#     #             ll = model.likelihood.log_marginal(y_batch, model(x_batch))\n",
    "#     #             \n",
    "#     #             print(f'Means {mean}')\n",
    "#     #             print(f'Variances {var}')\n",
    "#     #             print(f'LLs {ll}')\n",
    "#     # \n",
    "#     #             mus.append(preds.mean)\n",
    "#     #             variances.append(preds.variance)\n",
    "#     #             lls.append(model.likelihood.log_marginal(y_batch, model(x_batch)))\n",
    "#     # \n",
    "#     #     return torch.cat(mus, dim=-1), torch.cat(variances, dim=-1), torch.cat(lls, dim=-1)\n",
    "# \n",
    "class DeepGP(DeepGP):\n",
    "    def __init__(self, input_dims=512, hidden_dims=[128], output_dims=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "\n",
    "        # Create multiple hidden layers\n",
    "        for i in range(len(hidden_dims)):\n",
    "            if i == 0:\n",
    "                input_dim = input_dims\n",
    "            else:\n",
    "                input_dim = hidden_dims[i - 1]  # Output dims of previous layer\n",
    "\n",
    "            hidden_layer = ToyDeepGPHiddenLayer(\n",
    "                input_dims=input_dim,\n",
    "                output_dims=hidden_dims[i],\n",
    "                mean_type='linear' if i < len(hidden_dims) - 1 else 'constant'\n",
    "            )\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "\n",
    "        # Last layer\n",
    "        self.last_layer = ToyDeepGPHiddenLayer(\n",
    "            input_dims=hidden_dims[-1],\n",
    "            output_dims=output_dims,\n",
    "            mean_type='constant'\n",
    "        )\n",
    "\n",
    "        self.likelihood = GaussianLikelihood()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)  # Pass through each hidden layer\n",
    "        output = self.last_layer(x)\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": "### Attention Layer",
   "metadata": {
    "papermill": {
     "duration": 0.015987,
     "end_time": "2024-10-04T19:24:01.719520",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.703533",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "class AttentionLayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            # nn.Tanh(),\n            # nn.ReLU(),\n            nn.PReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, num_instances, feature_dim)\n        attention_weights = self.attention(x)\n        weights = F.softmax(attention_weights, dim=1)\n\n        return (x * weights).sum(dim=1), weights.squeeze(-1)",
   "metadata": {
    "papermill": {
     "duration": 0.077274,
     "end_time": "2024-10-04T19:24:01.812862",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.735588",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:10.859388Z",
     "iopub.execute_input": "2024-11-09T12:04:10.859734Z",
     "iopub.status.idle": "2024-11-09T12:04:10.926160Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.859698Z",
     "shell.execute_reply": "2024-11-09T12:04:10.925167Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.434505Z",
     "start_time": "2024-11-15T14:20:17.418052Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": "class GatedAttention(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(GatedAttention, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            # nn.Tanh(),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.gate = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            # nn.Tanh(),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n    def forward(self, x):\n        # x shape: (batch_size, num_instances, input_dim)\n        attention_weights = self.attention(x)\n        gate_weights = torch.sigmoid(self.gate(x))\n        \n        weights = attention_weights * gate_weights\n        weights = F.softmax(weights, dim=1)\n        \n        return (x * weights).sum(dim=1), weights.squeeze(-1)",
   "metadata": {
    "papermill": {
     "duration": 0.079025,
     "end_time": "2024-10-04T19:24:01.908938",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.829913",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:10.931368Z",
     "iopub.execute_input": "2024-11-09T12:04:10.931690Z",
     "iopub.status.idle": "2024-11-09T12:04:11.000139Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.931654Z",
     "shell.execute_reply": "2024-11-09T12:04:10.999075Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.479710Z",
     "start_time": "2024-11-15T14:20:17.463173Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": "### Mixed Pooling",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class MixedPooling(nn.Module):\n    def __init__(self, in_channels: int, alpha: float=0.5):\n        super(MixedPooling, self).__init__()\n        self.alpha = alpha\n        \n        # Convolution layer\n        self.conv = nn.Conv2d(in_channels, 16, kernel_size=1)  # Adjust output channels as needed\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))\n\n    def forward(self, x):\n        # Perform average pooling and apply weight\n        avg_out = self.avgpool(x)\n        avg_out_weighted = avg_out * self.alpha\n        \n        # Perform max pooling and apply weight\n        max_out = self.maxpool(x)\n        max_out_weighted = max_out * (1 - self.alpha)\n\n        # Add the weighted outputs together\n        # combined = avg_out_weighted + max_out_weighted\n        combined = torch.cat((avg_out_weighted, max_out_weighted), dim=1)  # Concatenate along channel dimension\n        combined = self.conv(combined)  # Reduce channels\n        return combined  # Output with combined features",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.001398Z",
     "iopub.execute_input": "2024-11-09T12:04:11.001723Z",
     "iopub.status.idle": "2024-11-09T12:04:11.070134Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.001690Z",
     "shell.execute_reply": "2024-11-09T12:04:11.069154Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.542923Z",
     "start_time": "2024-11-15T14:20:17.526020Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": "### ResNet2D Model",
   "metadata": {
    "papermill": {
     "duration": 0.015805,
     "end_time": "2024-10-04T19:24:02.065521",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.049716",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.353086",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.337113",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels=CHANNELS, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.attention = AttentionLayer(input_dim=144, hidden_dim=512)\n",
    "        \n",
    "        # self.classifier = nn.Linear(572, 1) # Use for GP before Att\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Define inducing points for the GP layer\n",
    "        inducing_points = torch.randn(32, 144) # \n",
    "        self.gp_layer = GPModel(inducing_points)\n",
    "        self.classifier_v0 = nn.Linear(144 + 1, 1) # Use for minimal model\n",
    "        # self.classifier = nn.Linear(512 + 1, 1) # Use for GP after Att\n",
    "        \n",
    "        # self.gp_layer = DeepGP(input_dims=512, hidden_dims=512, output_dims=None)\n",
    "        # self.classifier_v3 = nn.Linear(512 + 10, 1) # Use for Single Deep GP\n",
    "        \n",
    "        # Use for multiple Deep GP layers (optional)\n",
    "        # n_layers = 6  # Specify the number of hidden layers you want\n",
    "        # hidden_dimensions = [128] * n_layers  # You can customize this as needed\n",
    "        # self.gp_layer = DeepGP(input_dims=512, hidden_dims=hidden_dimensions, output_dims=10)\n",
    "        # self.classifier_v3 = nn.Linear(512 + 10 * 10, 1)\n",
    "        \n",
    "        \n",
    "        # First convolutional layer: 1 input channel, 32 output channels, kernel size (16, 16)\n",
    "        self.conv0 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))  # Max pooling layer\n",
    "        # Second convolutional layer: 32 input channels, 16 output channels, kernel size (5, 5)\n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(5,5))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(5, 5))\n",
    "        self.conv_layers = nn.ModuleList([self.conv2 for _ in range(3)])\n",
    "        \n",
    "    def forward(self, bags):\n",
    "        if CHANNELS == 1:\n",
    "            batch_size, num_instances, c, h, w = bags.size()\n",
    "        else:\n",
    "            batch_size, num_instances, h, w, c = bags.size()\n",
    "\n",
    "        bags_flattened = bags.view(batch_size * num_instances, c, h, w)\n",
    "        \n",
    "        # Version 0: Conv + Att + GP\n",
    "        # Apply first conv layer and pooling\n",
    "        x = self.pool(self.conv0(bags_flattened))\n",
    "        x = self.pool(self.conv1(x))\n",
    "        # Apply repeated conv layers and pooling\n",
    "        for conv in self.conv_layers:\n",
    "            x = self.pool(conv(x))\n",
    "        features = x.view(batch_size, num_instances, -1)\n",
    "        \n",
    "        # Version 1: CNN-ResNet + Att + GP\n",
    "        # features = self.resnet(bags_flattened)\n",
    "        # features = self.dropout(features)\n",
    "        # features = features.view(batch_size, num_instances, -1)\n",
    "        \n",
    "        attended_features, attended_weights = self.attention(features)\n",
    "        # attended_features_reshaped = attended_features.view(-1, 512)\n",
    "        attended_features_reshaped = attended_features.view(batch_size, -1)\n",
    "        \n",
    "        # CNN_ATT_GP\n",
    "        gp_output = self.gp_layer(attended_features_reshaped)\n",
    "        gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "\n",
    "        combine_features = torch.cat((attended_features, gp_mean), dim=1)\n",
    "        combine_features = self.dropout(combine_features)\n",
    "\n",
    "        outputs = torch.sigmoid(self.classifier_v0(combine_features))\n",
    "        return outputs, attended_weights, gp_output\n",
    "        \n",
    "        # CNN_GP_ATT\n",
    "        # gp_output_v2 = self.gp_layer(features)\n",
    "        # gp_mean_v2 = gp_output_v2.mean\n",
    "        # \n",
    "        # # print(f'Shape of gp_output_v2: {gp_output_v2.sample()}')\n",
    "        # # print(f'Shape of gp_mean_v2: {gp_mean_v2.shape}')\n",
    "        # \n",
    "        # combine_features_v2 = torch.cat((attended_features, gp_mean_v2), dim=1)\n",
    "        # combine_features_v2 = self.dropout(combine_features_v2)\n",
    "        # \n",
    "        # outputs_v2 = torch.sigmoid(self.classifier(combine_features_v2))\n",
    "        # return outputs_v2, attended_weights, gp_output_v2\n",
    "        \n",
    "        # # CNN_ATT_DeepGP\n",
    "        # gp_output_v3 = self.gp_layer(attended_features_reshaped)\n",
    "        # gp_mean_v3 = gp_output_v3.mean.view(batch_size, -1)\n",
    "        # \n",
    "        # # print(f'Shape of gp_output_v3: {gp_output_v3.sample()}')\n",
    "        # # print(f'Shape of gp_mean_v3: {gp_mean_v3.shape}')\n",
    "        # \n",
    "        # combine_features_v3 = torch.cat((attended_features, gp_mean_v3), dim=1)\n",
    "        # combine_features_v3 = self.dropout(combine_features_v3)\n",
    "        # \n",
    "        # outputs_v3 = torch.sigmoid(self.classifier_v3(combine_features_v3))\n",
    "        # return outputs_v3, attended_weights, gp_output_v3"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.071536Z",
     "iopub.execute_input": "2024-11-09T12:04:11.071923Z",
     "iopub.status.idle": "2024-11-09T12:04:11.143565Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.071889Z",
     "shell.execute_reply": "2024-11-09T12:04:11.142556Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.633336Z",
     "start_time": "2024-11-15T14:20:17.601443Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": "## Training and Evaluation",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.496444",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.480471",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Loss Function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n    # Cross-Entropy Loss for CNN outputs\n    bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n    kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n    total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n    \n    return total_loss",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.144744Z",
     "iopub.execute_input": "2024-11-09T12:04:11.145137Z",
     "iopub.status.idle": "2024-11-09T12:04:11.209993Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.145066Z",
     "shell.execute_reply": "2024-11-09T12:04:11.209156Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.679535Z",
     "start_time": "2024-11-15T14:20:17.663017Z"
    }
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.729361Z",
     "start_time": "2024-11-15T14:20:17.708493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss_v2(outputs, gp_output, mll, target, alpha=0.5):\n",
    "    # Cross-Entropy Loss for CNN outputs\n",
    "    bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n",
    "    gp_loss = -mll(gp_output, target)\n",
    "    total_loss = (1 - alpha) * bce_loss + alpha * gp_loss\n",
    "\n",
    "    return total_loss"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": "### Training",
   "metadata": {
    "papermill": {
     "duration": 0.016325,
     "end_time": "2024-10-04T19:24:02.529050",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.512725",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device, variational_optimizer=None, mll=None, likelihood=None):\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        variational_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, attention_weights, gp_output = model(batch_data)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = combined_loss(outputs, model.gp_layer, batch_patient_labels, alpha)\n",
    "        # loss = combined_loss_v2(outputs, gp_output, mll, batch_patient_labels, alpha)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        variational_optimizer.step()\n",
    "\n",
    "        # Store predictions and labels\n",
    "        predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "        labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def validate(model, data_loader, criterion, device, variational_optimizer=None, mll=None, likelihood=None):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, attention_weights, gp_output = model(batch_data)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = combined_loss(outputs, model.gp_layer, batch_patient_labels, alpha)\n",
    "            # loss = combined_loss_v2(outputs, gp_output, mll, batch_patient_labels, alpha)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            # Store predictions and labels\n",
    "            predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer,num_epochs, learning_rate, device='cuda', variational_optimizer=None, mll=None, likelihood=None):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "                                              steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, train_loader, criterion, optimizer, scheduler, device, variational_optimizer, mll, likelihood)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\"train/loss\": train_loss / len(train_loader), **train_metrics})\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, val_loader, criterion, device, variational_optimizer, mll, likelihood)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\"val/loss\": val_loss / len(val_loader), **val_metrics})\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.089765,
     "end_time": "2024-10-04T19:24:02.636485",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.546720",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.211419Z",
     "iopub.execute_input": "2024-11-09T12:04:11.212295Z",
     "iopub.status.idle": "2024-11-09T12:04:11.290690Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.212259Z",
     "shell.execute_reply": "2024-11-09T12:04:11.289598Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.781005Z",
     "start_time": "2024-11-15T14:20:17.761362Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": "### Evaluation Functions",
   "metadata": {
    "papermill": {
     "duration": 0.016054,
     "end_time": "2024-10-04T19:24:02.668925",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.652871",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, data_loader, device='cuda', likelihood=None):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    likelihood = likelihood.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad(): # Torch.inference_mode() is equivalent to torch.no_grad(), but it's faster and consumes less memory!!!\n",
    "        for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "\n",
    "            outputs, attention_weights, _ = model(batch_data)\n",
    "            outputs = outputs.squeeze()\n",
    "            predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.079611,
     "end_time": "2024-10-04T19:24:02.764721",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.685110",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.291769Z",
     "iopub.execute_input": "2024-11-09T12:04:11.292052Z",
     "iopub.status.idle": "2024-11-09T12:04:11.359152Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.292017Z",
     "shell.execute_reply": "2024-11-09T12:04:11.358128Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.825627Z",
     "start_time": "2024-11-15T14:20:17.808907Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": "### Visualization Functions",
   "metadata": {
    "papermill": {
     "duration": 0.01814,
     "end_time": "2024-10-04T19:24:02.800208",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.782068",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, data_loader, device, likelihood=None):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    # predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "\n",
    "            outputs, attention_weights, _ = model(batch_data)\n",
    "            outputs = outputs.squeeze()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            \n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, device, likelihood=None):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, data_loader, device, likelihood)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.092339,
     "end_time": "2024-10-04T19:24:02.911187",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.818848",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.360394Z",
     "iopub.execute_input": "2024-11-09T12:04:11.360796Z",
     "iopub.status.idle": "2024-11-09T12:04:11.429794Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.360761Z",
     "shell.execute_reply": "2024-11-09T12:04:11.428871Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.874045Z",
     "start_time": "2024-11-15T14:20:17.855576Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": "### Data Processing Functions",
   "metadata": {
    "papermill": {
     "duration": 0.018543,
     "end_time": "2024-10-04T19:24:02.948830",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.930287",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class()\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device, likelihood=None):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, DEVICE, likelihood)\n",
    "    \n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.079014,
     "end_time": "2024-10-04T19:24:03.046531",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.967517",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.430912Z",
     "iopub.execute_input": "2024-11-09T12:04:11.431237Z",
     "iopub.status.idle": "2024-11-09T12:04:11.497859Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.431205Z",
     "shell.execute_reply": "2024-11-09T12:04:11.496987Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.919032Z",
     "start_time": "2024-11-15T14:20:17.901696Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": "## Visualizing Attention Weights and Images",
   "metadata": {
    "papermill": {
     "duration": 0.01837,
     "end_time": "2024-10-04T19:24:03.082997",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.064627",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_label_attention_weights(model, data_loader, device='cuda', likelihood=None):\n",
    "    \"\"\"\n",
    "    Plot images with their labels and attention values in a single large plot.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model\n",
    "    - data_loader: DataLoader containing test dataset\n",
    "    - device: Device to run the model on ('cuda' or 'cpu')\n",
    "    - CHANNELS: Number of channels in the image (e.g., 1 for grayscale, 3 for RGB)\n",
    "\n",
    "    Expected shapes:\n",
    "    - 1-channel images: (batch_size, num_images, 224, 224)\n",
    "    - 3-channel images: (batch_size, num_images, 3, 224, 224)\n",
    "    - attention: float value per image indicating attention weight\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    likelihood = likelihood.to(device)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    num_images = 60\n",
    "    rows, cols = 10, 6  # Adjust to fit 60 images in a single plot\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patients_label in data_loader:\n",
    "            # Move data to the appropriate device\n",
    "            batch_data = batch_data.to(device)\n",
    "            outputs, attention_weight_batch, _ = model(batch_data)\n",
    "\n",
    "            # Process each patient in the batch\n",
    "            for patient_idx in range(batch_data.size(0)):\n",
    "                if batch_patients_label[patient_idx].item() == 1:  # Check if patient has positive label\n",
    "                    # Create a new figure for this patient\n",
    "                    fig = plt.figure(figsize=(cols * 4, rows * 4 + 2))  # Increased height for suptitle\n",
    "\n",
    "                    for img_idx in range(num_images):\n",
    "                        # Get the image and its label\n",
    "                        img = batch_data[patient_idx, img_idx].cpu().numpy()\n",
    "                        img_label = batch_labels[patient_idx, img_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Get attention value\n",
    "                        if attention_weight_batch.size(1) == batch_data.size(1):\n",
    "                            attention_value = attention_weight_batch[patient_idx, img_idx].cpu().item()\n",
    "                        else:\n",
    "                            attention_value = attention_weight_batch[patient_idx].cpu().item()\n",
    "                        \n",
    "                        # Plot image\n",
    "                        plt.subplot(rows, cols, img_idx + 1)\n",
    "                        if CHANNELS == 3:  # RGB image\n",
    "                            plt.imshow(img)\n",
    "                        else:  # Grayscale image\n",
    "                            if img.ndim == 3:  # If shape is (1, H, W)\n",
    "                                img = np.squeeze(img)  # Convert to (H, W)\n",
    "                            plt.imshow(img, cmap='gray')\n",
    "                        \n",
    "                        plt.title(f'Label: {img_label}\\nAttention: {attention_value:.4f}', fontsize=12)\n",
    "                        plt.axis('off')\n",
    "\n",
    "                    # Add overall title for the patient\n",
    "                    plt.suptitle(f'Patient Images (Patient Label: {batch_patients_label[patient_idx].cpu().numpy()})', fontsize=16)\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust rect to make space for suptitle\n",
    "                    plt.show()\n",
    "                                      \n",
    "                    # Since we are plotting only for one patient, return after the first plot\n",
    "                    return"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.095451,
     "end_time": "2024-10-04T19:24:03.197071",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.101620",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.499019Z",
     "iopub.execute_input": "2024-11-09T12:04:11.499350Z",
     "iopub.status.idle": "2024-11-09T12:04:11.569343Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.499317Z",
     "shell.execute_reply": "2024-11-09T12:04:11.568408Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:17.968878Z",
     "start_time": "2024-11-15T14:20:17.948982Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": "## Main",
   "metadata": {
    "papermill": {
     "duration": 0.016248,
     "end_time": "2024-10-04T19:24:03.231225",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.214977",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "def set_seed(seed=42):\n    \"\"\"Set seed for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.570677Z",
     "iopub.execute_input": "2024-11-09T12:04:11.571424Z",
     "iopub.status.idle": "2024-11-09T12:04:11.634204Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.571366Z",
     "shell.execute_reply": "2024-11-09T12:04:11.633349Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T14:20:18.014867Z",
     "start_time": "2024-11-15T14:20:17.998396Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "def main(mode='train'):\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=\"your_project_name\")\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    set_seed()\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    set_seed()\n",
    "    \n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = MILResNet18()\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    likelihood = likelihood.to(device) # Move likelihood to device before passing to mll\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    # mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_loader.__len__())\n",
    "    mll = gpytorch.mlls.DeepApproximateMLL(gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_loader.__len__()))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    variational_optimizer = gpytorch.optim.NGD(model.gp_layer.variational_parameters(), num_data=train_loader.__len__(), lr=0.01)\n",
    "\n",
    "    if mode == 'train':\n",
    "        # Watch the model to log gradients and parameters\n",
    "        wandb.watch(model)\n",
    "\n",
    "        # Train model\n",
    "        trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, config.num_epochs, config.learning_rate, DEVICE, variational_optimizer, mll, likelihood)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    # Load best model\n",
    "    trained_model = load_model(MILResNet18, MODEL_PATH)\n",
    "\n",
    "    # Evaluate model\n",
    "    predictions, labels = evaluate_model(trained_model, test_loader, DEVICE, likelihood)\n",
    "    metrics = calculate_metrics(predictions, labels)\n",
    "    \n",
    "    # Log metrics to W&B\n",
    "    wandb.log(metrics)\n",
    "\n",
    "    print_metrics(metrics)\n",
    "\n",
    "    # Visualizations\n",
    "    plot_roc_curve(trained_model, test_loader, DEVICE, likelihood)\n",
    "    plot_confusion_matrix(trained_model, test_loader, DEVICE, likelihood)\n",
    "\n",
    "    if mode == 'train':\n",
    "        required_columns = ['patient_id', 'study_instance_uid', 'patient_label']\n",
    "        temp_test_labels = test_labels[required_columns]\n",
    "        \n",
    "        # Save results\n",
    "        results_df = get_test_results(trained_model, test_loader, temp_test_labels, device, likelihood)\n",
    "        results_df.to_csv('results/results.csv', index=False)\n",
    "        print(results_df.head())\n",
    "\n",
    "        # Log results DataFrame as a table in W&B (optional)\n",
    "        wandb.log({\"results\": wandb.Table(dataframe=results_df)})\n",
    "\n",
    "    # Call the function with the test_loader\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "    plot_label_attention_weights(trained_model, test_loader, device, likelihood)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ],
   "metadata": {
    "papermill": {
     "duration": 24983.553969,
     "end_time": "2024-10-05T02:20:26.801619",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.247650",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.635549Z",
     "iopub.execute_input": "2024-11-09T12:04:11.635991Z",
     "iopub.status.idle": "2024-11-09T12:18:52.781211Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.635946Z",
     "shell.execute_reply": "2024-11-09T12:18:52.780156Z"
    },
    "trusted": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-15T14:20:18.044863Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train:\n",
      "Loss: 60.6952, Accuracy: 0.4986, Precision: 0.3333, Recall: 0.0029, F1: 0.0057\n",
      "Epoch 1/10 - Validation:\n",
      "Loss: 26.0054, Accuracy: 0.5000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Results",
   "metadata": {}
  }
 ]
}
