{
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 9652074,
     "sourceType": "datasetVersion",
     "datasetId": 5705276
    }
   ],
   "dockerImageVersionId": 30776,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25022.389189,
   "end_time": "2024-10-05T02:20:29.848670",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-04T19:23:27.459481",
   "version": "2.6.0"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Library",
   "metadata": {
    "papermill": {
     "duration": 0.015487,
     "end_time": "2024-10-04T19:23:30.290610",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.275123",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gpytorch\n",
    "!pip install wandb"
   ],
   "metadata": {
    "papermill": {
     "duration": 13.842163,
     "end_time": "2024-10-04T19:23:44.147775",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.305612",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:03:29.962323Z",
     "iopub.execute_input": "2024-11-09T12:03:29.962697Z",
     "iopub.status.idle": "2024-11-09T12:03:54.971291Z",
     "shell.execute_reply.started": "2024-11-09T12:03:29.962659Z",
     "shell.execute_reply": "2024-11-09T12:03:54.970161Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:20.194581Z",
     "start_time": "2024-12-26T16:32:18.982247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n",
      "Requirement already satisfied: wandb in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (0.18.5)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (5.28.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.17.0)\r\n",
      "Requirement already satisfied: setproctitle in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (72.1.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "\n",
    "from torch.cpu.amp import GradScaler\n",
    "\n",
    "import time\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "metadata": {
    "papermill": {
     "duration": 11.793387,
     "end_time": "2024-10-04T19:23:55.956283",
     "exception": false,
     "start_time": "2024-10-04T19:23:44.162896",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:03:54.973321Z",
     "iopub.execute_input": "2024-11-09T12:03:54.973660Z",
     "iopub.status.idle": "2024-11-09T12:04:04.345572Z",
     "shell.execute_reply.started": "2024-11-09T12:03:54.973624Z",
     "shell.execute_reply": "2024-11-09T12:04:04.344540Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:21.831261Z",
     "start_time": "2024-12-26T16:32:20.202897Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "import warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)",
   "metadata": {
    "papermill": {
     "duration": 0.02447,
     "end_time": "2024-10-04T19:23:55.997833",
     "exception": false,
     "start_time": "2024-10-04T19:23:55.973363",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.346735Z",
     "iopub.execute_input": "2024-11-09T12:04:04.347247Z",
     "iopub.status.idle": "2024-11-09T12:04:04.352304Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.347210Z",
     "shell.execute_reply": "2024-11-09T12:04:04.351162Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:21.855534Z",
     "start_time": "2024-12-26T16:32:21.853946Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## Init GPU",
   "metadata": {
    "papermill": {
     "duration": 0.024749,
     "end_time": "2024-10-04T19:23:56.038412",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.013663",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "# Initialize GPU Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\nelse:\n    print(\"No GPU available. Training will run on CPU.\")\n\nprint(device)",
   "metadata": {
    "papermill": {
     "duration": 0.105269,
     "end_time": "2024-10-04T19:23:56.159555",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.054286",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.354377Z",
     "iopub.execute_input": "2024-11-09T12:04:04.354684Z",
     "iopub.status.idle": "2024-11-09T12:04:04.410802Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.354653Z",
     "shell.execute_reply": "2024-11-09T12:04:04.409843Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:21.923875Z",
     "start_time": "2024-12-26T16:32:21.898845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2",
   "metadata": {
    "papermill": {
     "duration": 0.081958,
     "end_time": "2024-10-04T19:23:56.258405",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.176447",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.411954Z",
     "iopub.execute_input": "2024-11-09T12:04:04.412279Z",
     "iopub.status.idle": "2024-11-09T12:04:04.479489Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.412244Z",
     "shell.execute_reply": "2024-11-09T12:04:04.478787Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:21.993345Z",
     "start_time": "2024-12-26T16:32:21.964935Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seed Everything"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.042016Z",
     "start_time": "2024-12-26T16:32:22.014431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "## Config Info",
   "metadata": {
    "papermill": {
     "duration": 0.016234,
     "end_time": "2024-10-04T19:23:56.291671",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.275437",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 1\n",
    "\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "TEST_BATCH_SIZE = 2\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "\n",
    "MAX_SLICES = 60\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "INDUCING_POINTS = 128\n",
    "THRESHOLD = 0.4\n",
    "\n",
    "# TARGET_LABELS = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "TARGET_LABELS = ['intraparenchymal']\n",
    "\n",
    "MODEL_PATH = 'results/trained_model.pth'\n",
    "DEVICE = 'cuda'"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.076674,
     "end_time": "2024-10-04T19:23:56.384583",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.307909",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.480520Z",
     "iopub.execute_input": "2024-11-09T12:04:04.480805Z",
     "iopub.status.idle": "2024-11-09T12:04:04.541310Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.480773Z",
     "shell.execute_reply": "2024-11-09T12:04:04.540405Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.085830Z",
     "start_time": "2024-12-26T16:32:22.060793Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "ROOT_DIR = '/kaggle/input/rsna-mil-training/' if KAGGLE else None\n",
    "# DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-mil-training/'\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "DICOM_DIR = DATA_DIR\n",
    "# CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_1000_scan_subset.csv'\n",
    "CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_1150.csv'\n",
    "# SLICE_LABEL_PATH = ROOT_DIR + \"sorted_training_dataset_with_labels.csv\" if KAGGLE else './data_analyze/sorted_training_dataset_with_labels.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "# Load patient scan labels\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "\n",
    "# patient_slice_labels = pd.read_csv(SLICE_LABEL_PATH)\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)"
   ],
   "metadata": {
    "papermill": {
     "duration": 4.085647,
     "end_time": "2024-10-04T19:24:00.487263",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.401616",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.542362Z",
     "iopub.execute_input": "2024-11-09T12:04:04.542646Z",
     "iopub.status.idle": "2024-11-09T12:04:04.649755Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.542615Z",
     "shell.execute_reply": "2024-11-09T12:04:04.648985Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.148491Z",
     "start_time": "2024-12-26T16:32:22.109626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret(\"Wandb key\")\n",
    "# \n",
    "# wandb.login(key=key, relogin=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:04.650858Z",
     "iopub.execute_input": "2024-11-09T12:04:04.651199Z",
     "iopub.status.idle": "2024-11-09T12:04:06.250597Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.651166Z",
     "shell.execute_reply": "2024-11-09T12:04:06.249864Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.173744Z",
     "start_time": "2024-12-26T16:32:22.157846Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "patient_scan_labels.head()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.251742Z",
     "iopub.execute_input": "2024-11-09T12:04:06.252251Z",
     "iopub.status.idle": "2024-11-09T12:04:06.331892Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.252216Z",
     "shell.execute_reply": "2024-11-09T12:04:06.331086Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.233488Z",
     "start_time": "2024-12-26T16:32:22.206489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_e29b63250.dcm', 'ID_766be7451.dcm', 'ID_b...   \n",
       "1  ['ID_b2e23d464.dcm', 'ID_0d5c28287.dcm', 'ID_3...   \n",
       "2  ['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...   \n",
       "3  ['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...   \n",
       "4  ['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_005f241d   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0075b28c   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...  ID_00760731   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00859e11   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00a3b735   \n",
       "\n",
       "  study_instance_uid                                     image_position  \\\n",
       "0      ID_07e2cf7b4b  ['[-125.000, -131.252, 63.879]', '[-125.000, -...   \n",
       "1      ID_0373dbdd02  ['[-125.000000, -130.318451, 38.952644]', '[-1...   \n",
       "2      ID_006a2c59e4  ['[-125.000, -146.200, 5.250]', '[-125.000, -1...   \n",
       "3      ID_01f49be39f  ['[-125.000, -121.018, -1.452]', '[-125.000, -...   \n",
       "4      ID_065682422f  ['[-125, -42, 69.9000244]', '[-125, -42, 74.90...   \n",
       "\n",
       "                                   samples_per_pixel  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                       pixel_spacing  \\\n",
       "0  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "1  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "2  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "3  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "4  ['[0.48828125, 0.48828125]', '[0.48828125, 0.4...   \n",
       "\n",
       "                                pixel_representation  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       window_center  \\\n",
       "0  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "1  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "2  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "3  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "4  ['[00036, 00036]', '[00036, 00036]', '[00036, ...   \n",
       "\n",
       "                                        window_width  \\\n",
       "0  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "1  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "2  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "3  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "4  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "\n",
       "                                   rescale_intercept  \\\n",
       "0  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "1  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "2  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "3  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "4  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "\n",
       "                                       rescale_slope  patient_label  \n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              0  \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              0  \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              1  \n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              1  \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>image_position</th>\n",
       "      <th>samples_per_pixel</th>\n",
       "      <th>pixel_spacing</th>\n",
       "      <th>pixel_representation</th>\n",
       "      <th>window_center</th>\n",
       "      <th>window_width</th>\n",
       "      <th>rescale_intercept</th>\n",
       "      <th>rescale_slope</th>\n",
       "      <th>patient_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_e29b63250.dcm', 'ID_766be7451.dcm', 'ID_b...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_005f241d</td>\n",
       "      <td>ID_07e2cf7b4b</td>\n",
       "      <td>['[-125.000, -131.252, 63.879]', '[-125.000, -...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_b2e23d464.dcm', 'ID_0d5c28287.dcm', 'ID_3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0075b28c</td>\n",
       "      <td>ID_0373dbdd02</td>\n",
       "      <td>['[-125.000000, -130.318451, 38.952644]', '[-1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>ID_00760731</td>\n",
       "      <td>ID_006a2c59e4</td>\n",
       "      <td>['[-125.000, -146.200, 5.250]', '[-125.000, -1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00859e11</td>\n",
       "      <td>ID_01f49be39f</td>\n",
       "      <td>['[-125.000, -121.018, -1.452]', '[-125.000, -...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00a3b735</td>\n",
       "      <td>ID_065682422f</td>\n",
       "      <td>['[-125, -42, 69.9000244]', '[-125, -42, 74.90...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.48828125, 0.48828125]', '[0.48828125, 0.4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['[00036, 00036]', '[00036, 00036]', '[00036, ...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "metadata": {
    "papermill": {
     "duration": 0.016012,
     "end_time": "2024-10-04T19:24:00.520265",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.504253",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def create_bone_mask(dcm):\n",
    "    # Assuming dcm.pixel_array contains the HU values\n",
    "    hu_values = dcm.pixel_array\n",
    "\n",
    "    # Create a mask for bone regions\n",
    "    # bone_mask = (hu_values >= 1000) & (hu_values <= 1200)\n",
    "    bone_mask = (hu_values >= 1000) & (hu_values <= 1200)\n",
    "    return bone_mask\n",
    "\n",
    "def extract_bone_mask(dcm):\n",
    "    # Create the bone mask\n",
    "    bone_mask = create_bone_mask(dcm)\n",
    "\n",
    "    # Extract the bone mask from the image\n",
    "    hu_values = dcm.pixel_array.copy()\n",
    "    # hu_values[bone_mask] = 0\n",
    "    hu_values[~bone_mask] = 0\n",
    "\n",
    "    # Update the DICOM pixel data\n",
    "    dcm.PixelData = hu_values.tobytes()\n",
    "\n",
    "\n",
    "def window_image(dcm, window_center, window_width):\n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    # extract_bone_mask(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "\n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 90)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "\n",
    "    brain_img = (brain_img - 0) / 90\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "\n",
    "    if CHANNELS == 3:\n",
    "        bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n",
    "    else:\n",
    "        bsb_img = brain_img\n",
    "    return bsb_img.astype(np.float16)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.079596,
     "end_time": "2024-10-04T19:24:00.617176",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.537580",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.335428Z",
     "iopub.execute_input": "2024-11-09T12:04:06.335746Z",
     "iopub.status.idle": "2024-11-09T12:04:06.404940Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.335713Z",
     "shell.execute_reply": "2024-11-09T12:04:06.403960Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.288979Z",
     "start_time": "2024-12-26T16:32:22.271737Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n    # Check if type of slice is dicom or an empty numpy array\n    if (type(slice) == np.ndarray):\n        slice = resize(slice, target_size, anti_aliasing=True)\n        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n        if CHANNELS == 3:\n            return multichannel_slice.astype(np.float16)\n        else:\n            return slice.astype(np.float16)\n    else:\n        slice = bsb_window(slice)\n        return slice.astype(np.float16)",
   "metadata": {
    "papermill": {
     "duration": 0.076531,
     "end_time": "2024-10-04T19:24:00.710787",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.634256",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.406284Z",
     "iopub.execute_input": "2024-11-09T12:04:06.406824Z",
     "iopub.status.idle": "2024-11-09T12:04:06.470535Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.406773Z",
     "shell.execute_reply": "2024-11-09T12:04:06.469524Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.336360Z",
     "start_time": "2024-12-26T16:32:22.314861Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "def read_dicom_folder(folder_path, max_slices=MAX_SLICES):\n",
    "    # Filter and sort DICOM files directly based on ImagePositionPatient\n",
    "    dicom_files = sorted(\n",
    "        [f for f in os.listdir(folder_path) if f.endswith(\".dcm\")],\n",
    "        key=lambda f: float(pydicom.dcmread(os.path.join(folder_path, f)).ImagePositionPatient[2])\n",
    "    )[:max_slices]\n",
    "\n",
    "    # Read and store slices\n",
    "    slices = [pydicom.dcmread(os.path.join(folder_path, f)) for f in dicom_files]\n",
    "\n",
    "    # Pad with black images if necessary\n",
    "    if len(slices) < max_slices:\n",
    "        black_image = np.zeros_like(slices[0].pixel_array)\n",
    "        slices += [black_image] * (max_slices - len(slices))\n",
    "\n",
    "    return slices[:max_slices]\n"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.076142,
     "end_time": "2024-10-04T19:24:00.803356",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.727214",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.471678Z",
     "iopub.execute_input": "2024-11-09T12:04:06.471972Z",
     "iopub.status.idle": "2024-11-09T12:04:06.536802Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.471940Z",
     "shell.execute_reply": "2024-11-09T12:04:06.535848Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.370298Z",
     "start_time": "2024-12-26T16:32:22.353508Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset and DataLoader",
   "metadata": {
    "papermill": {
     "duration": 0.016487,
     "end_time": "2024-10-04T19:24:00.836476",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.819989",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Splitting the Dataset",
   "metadata": {
    "papermill": {
     "duration": 0.015844,
     "end_time": "2024-10-04T19:24:00.868235",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.852391",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.075616,
     "end_time": "2024-10-04T19:24:00.959889",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.884273",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.537885Z",
     "iopub.execute_input": "2024-11-09T12:04:06.538200Z",
     "iopub.status.idle": "2024-11-09T12:04:06.603427Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.538168Z",
     "shell.execute_reply": "2024-11-09T12:04:06.602545Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.413067Z",
     "start_time": "2024-12-26T16:32:22.396591Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "### Processing the Data",
   "metadata": {
    "papermill": {
     "duration": 0.017012,
     "end_time": "2024-10-04T19:24:00.993325",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.976313",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def process_patient_data(dicom_dir, row, num_instances=12, depth=5):\n",
    "    patient_id = row['patient_id'].replace('ID_', '')\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n",
    "\n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(dicom_dir, folder_name)\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "\n",
    "        preprocessed_slices = [torch.tensor(preprocess_slice(slice), dtype=torch.float32) for slice in slices]  # Convert to tensor\n",
    "\n",
    "        # Stack preprocessed slices into an array\n",
    "        preprocessed_slices = torch.stack(preprocessed_slices, dim=0)  # (num_slices, height, width, channels)\n",
    "\n",
    "        # Labels are already in list form, so just convert them to a tensor\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.long)\n",
    "\n",
    "        # Fill labels with 0s if necessary\n",
    "        if len(preprocessed_slices) > len(labels):\n",
    "            padded_labels = torch.zeros(len(preprocessed_slices), dtype=torch.long)\n",
    "            padded_labels[:len(labels)] = labels\n",
    "        else:\n",
    "            padded_labels = labels[:len(preprocessed_slices)]\n",
    "\n",
    "        return preprocessed_slices, padded_labels\n",
    "\n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_name}\")\n",
    "        return None, None"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.076813,
     "end_time": "2024-10-04T19:24:01.086193",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.009380",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.604538Z",
     "iopub.execute_input": "2024-11-09T12:04:06.604898Z",
     "iopub.status.idle": "2024-11-09T12:04:06.673120Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.604856Z",
     "shell.execute_reply": "2024-11-09T12:04:06.672312Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.461982Z",
     "start_time": "2024-12-26T16:32:22.441995Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": "### Augmentation",
   "metadata": {
    "papermill": {
     "duration": 0.016194,
     "end_time": "2024-10-04T19:24:01.118549",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.102355",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "class DatasetAugmentor:\n    def __init__(self, height, width, levels=2, seed=None):\n        self.height = height\n        self.width = width\n        self.levels = levels  # Dynamic number of levels\n        self.seed = seed\n        self.params = []\n\n        # Create different levels of transforms based on the number of levels specified\n        for i in range(levels):\n            factor = (i + 1) / levels\n            self.params.append(\n                self._create_transform(\n                    degrees=int(15 * factor), \n                    translate_range=(0.2 * factor, 0.2 * factor),\n                    scale_range=(1 - 0.2 * factor, 1 + 0.2 * factor),\n                    brightness_range=0.2 * factor,\n                    contrast_range=0.2 * factor,\n                    blur_sigma_range=(0.5 * factor, 1.0 * factor),\n                    apply_elastic=(i >= levels // 2),\n                    level_name=f'level_{i + 1}'\n                )\n            )\n\n    def _sample_value(self, value_range):\n        if isinstance(value_range, tuple):\n            random.seed(self.seed)\n            return random.uniform(value_range[0], value_range[1])\n        return value_range\n\n    def _create_transform(self, degrees, translate_range, scale_range, brightness_range, contrast_range, blur_sigma_range, apply_elastic, level_name):\n        print(f\"Creating '{level_name}' transform with parameters:\")\n        sampled_values = {\n            \"degrees\": abs(self._sample_value((-degrees, degrees))),\n            \"translate\": (abs(self._sample_value(translate_range[0])), abs(self._sample_value(translate_range[1]))),\n            \"scale\": self._sample_value(scale_range),\n            \"brightness\": self._sample_value(brightness_range),\n            \"contrast\": self._sample_value(contrast_range),\n            \"blur_sigma\": self._sample_value(blur_sigma_range),\n            \"apply_elastic\": apply_elastic\n        }\n        \n        print(sampled_values)\n        return sampled_values\n\n    def apply_transform(self, image, level):\n        params = self.params[level]\n        transform = self._get_transform(params, channels=image.shape[0])\n        return transform(image)\n\n    def _get_transform(self, params, channels=3):\n        transform_list = [\n            transforms.ToPILImage(),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomAffine(degrees=params[\"degrees\"], translate=params[\"translate\"], scale=(params[\"scale\"], params[\"scale\"])),\n            transforms.ColorJitter(brightness=params[\"brightness\"], contrast=params[\"contrast\"]),\n            transforms.GaussianBlur(kernel_size=(3, 3), sigma=params[\"blur_sigma\"]),\n            transforms.RandomApply([transforms.ElasticTransform()] if params[\"apply_elastic\"] else [], p=0.3),\n            transforms.Resize(256),\n            transforms.CenterCrop(self.height),\n            transforms.ToTensor(),\n        ]\n\n        if channels == 3:\n            transform_list.extend([\n                transforms.Normalize(mean=[0.16774411, 0.1360026, 0.19076315], std=[0.3101935, 0.27605791, 0.30469988]),\n                transforms.RandomApply([self._channel_shuffle], p=0.3)\n            ])\n        elif channels == 1:\n            transform_list.append(transforms.Normalize(mean=[0.16774411], std=[0.3101935]))\n\n        return transforms.Compose(transform_list)\n\n    def _channel_shuffle(self, tensor):\n        torch.manual_seed(self.seed)\n        channels = tensor.shape[0]\n        indices = torch.randperm(channels)\n        return tensor[indices]",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.742336Z",
     "iopub.execute_input": "2024-11-09T12:04:06.742670Z",
     "iopub.status.idle": "2024-11-09T12:04:06.817331Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.742638Z",
     "shell.execute_reply": "2024-11-09T12:04:06.816387Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.521682Z",
     "start_time": "2024-12-26T16:32:22.490255Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "augmentor = DatasetAugmentor(224, 224, seed=42)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.818519Z",
     "iopub.execute_input": "2024-11-09T12:04:06.818878Z",
     "iopub.status.idle": "2024-11-09T12:04:06.881845Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.818836Z",
     "shell.execute_reply": "2024-11-09T12:04:06.881035Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.558262Z",
     "start_time": "2024-12-26T16:32:22.540835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'level_1' transform with parameters:\n",
      "{'degrees': 1.9519751784103718, 'translate': (0.1, 0.1), 'scale': 1.027885359691577, 'brightness': 0.1, 'contrast': 0.1, 'blur_sigma': 0.4098566996144709, 'apply_elastic': False}\n",
      "Creating 'level_2' transform with parameters:\n",
      "{'degrees': 4.182803953736514, 'translate': (0.2, 0.2), 'scale': 1.0557707193831534, 'brightness': 0.2, 'contrast': 0.2, 'blur_sigma': 0.8197133992289418, 'apply_elastic': True}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "### Dataset Generator",
   "metadata": {
    "papermill": {
     "duration": 0.015494,
     "end_time": "2024-10-04T19:24:01.252123",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.236629",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# class MedicalScanDataset(Dataset):\n",
    "#     def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "#         self.data_dir = data_dir\n",
    "#         self.patient_scan_labels = self._parse_patient_scan_labels(patient_scan_labels)\n",
    "#         self.augmentor = augmentor\n",
    "#\n",
    "#     def _parse_patient_scan_labels(self, patient_scan_labels):\n",
    "#         \"\"\"Parse and validate patient scan labels.\"\"\"\n",
    "#         patient_scan_labels['images'] = patient_scan_labels['images'].apply(\n",
    "#             lambda x: eval(x) if isinstance(x, str) else x\n",
    "#         )\n",
    "#         patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
    "#             lambda x: eval(x) if isinstance(x, str) else x\n",
    "#         )\n",
    "#         patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
    "#         return patient_scan_labels\n",
    "#\n",
    "#     def _process_patient_data(self, row):\n",
    "#         \"\"\"Process patient data to get preprocessed slices and labels.\"\"\"\n",
    "#         return process_patient_data(self.data_dir, row)\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.patient_scan_labels) * (self.augmentor.levels if self.augmentor else 1)\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         patient_idx = idx // (self.augmentor.levels if self.augmentor else 1)\n",
    "#         aug_level = idx % (self.augmentor.levels if self.augmentor else 1)\n",
    "#\n",
    "#         row = self.patient_scan_labels.iloc[patient_idx]\n",
    "#         preprocessed_slices, labels = self._process_patient_data(row)\n",
    "#\n",
    "#         if preprocessed_slices is None:\n",
    "#             return None, None, None\n",
    "#\n",
    "#         preprocessed_slices = self._prepare_tensor(preprocessed_slices, aug_level if self.augmentor else None)\n",
    "#         patient_label = torch.tensor(bool(row['patient_label']), dtype=torch.uint8)\n",
    "#\n",
    "#         return preprocessed_slices, labels, patient_label\n",
    "#\n",
    "#     def _prepare_tensor(self, preprocessed_slices, aug_level):\n",
    "#         # Convert to numpy array and then to torch tensor\n",
    "#         preprocessed_slices = np.array(preprocessed_slices)\n",
    "#         preprocessed_slices = torch.tensor(preprocessed_slices, dtype=torch.float32)\n",
    "#\n",
    "#         # Add an additional dimension for channel if it's missing (no augmentor)\n",
    "#         if preprocessed_slices.ndim == 3:\n",
    "#             preprocessed_slices = preprocessed_slices.unsqueeze(1)  # shape: [slices, 1, H, W]\n",
    "#\n",
    "#         # Apply augmentation if augmentor is specified\n",
    "#         if self.augmentor and aug_level is not None:\n",
    "#             if preprocessed_slices.ndim == 4:  # Ensure it has the [slices, channels, H, W] format\n",
    "#                 return torch.stack([self.augmentor.apply_transform(img, aug_level) for img in preprocessed_slices])\n",
    "#\n",
    "#         return preprocessed_slices  # Return without augmentation if augmentor is None"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:06.949800Z",
     "iopub.execute_input": "2024-11-09T12:04:06.950129Z",
     "iopub.status.idle": "2024-11-09T12:04:07.020925Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.950072Z",
     "shell.execute_reply": "2024-11-09T12:04:07.020108Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.609962Z",
     "start_time": "2024-12-26T16:32:22.588212Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.662255Z",
     "start_time": "2024-12-26T16:32:22.636896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full Dataset\n",
    "class MedicalScanDataset:\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.dataset = self._parse_patient_scan_labels(patient_scan_labels)\n",
    "        self.augmentor = augmentor\n",
    "\n",
    "    def _parse_patient_scan_labels(self, patient_scan_labels):\n",
    "        \"\"\"Parse and validate patient scan labels.\"\"\"\n",
    "        patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
    "            lambda x: eval(x) if isinstance(x, str) else x\n",
    "        )\n",
    "        patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
    "            lambda x: eval(x) if isinstance(x, str) else x\n",
    "        )\n",
    "        patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
    "        return patient_scan_labels\n",
    "\n",
    "    def _process_patient_data(self, row):\n",
    "        \"\"\"Process patient data to get preprocessed slices and labels.\"\"\"\n",
    "        return process_patient_data(self.data_dir, row)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * (self.augmentor.levels if self.augmentor else 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_idx = idx // (self.augmentor.levels if self.augmentor else 1)\n",
    "        aug_level = idx % (self.augmentor.levels if self.augmentor else 1)\n",
    "\n",
    "        row = self.dataset.iloc[patient_idx]\n",
    "        preprocessed_slices, labels = self._process_patient_data(row)\n",
    "\n",
    "        preprocessed_slices = self._prepare_tensor(preprocessed_slices, aug_level if self.augmentor else None)\n",
    "        patient_label = torch.tensor(bool(row['patient_label']), dtype=torch.uint8)\n",
    "\n",
    "        return preprocessed_slices, labels, patient_label\n",
    "\n",
    "    def _prepare_tensor(self, preprocessed_slices, aug_level):\n",
    "        # Convert to numpy array and then to torch tensor\n",
    "        preprocessed_slices = np.array(preprocessed_slices)\n",
    "        preprocessed_slices = torch.tensor(preprocessed_slices, dtype=torch.float32)\n",
    "\n",
    "        # Add an additional dimension for channel if it's missing (no augmentor)\n",
    "        if preprocessed_slices.ndim == 3:\n",
    "            preprocessed_slices = preprocessed_slices.unsqueeze(1)  # shape: [slices, 1, H, W]\n",
    "\n",
    "        # Apply augmentation if augmentor is specified\n",
    "        if self.augmentor and aug_level is not None:\n",
    "            if preprocessed_slices.ndim == 4:  # Ensure it has the [slices, channels, H, W] format\n",
    "                return torch.stack([self.augmentor.apply_transform(img, aug_level) for img in preprocessed_slices])\n",
    "\n",
    "        return preprocessed_slices  # Return without augmentation if augmentor is None\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.022171Z",
     "iopub.execute_input": "2024-11-09T12:04:07.022528Z",
     "iopub.status.idle": "2024-11-09T12:04:07.085840Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.022481Z",
     "shell.execute_reply": "2024-11-09T12:04:07.084942Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.712168Z",
     "start_time": "2024-12-26T16:32:22.688326Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.086910Z",
     "iopub.execute_input": "2024-11-09T12:04:07.087252Z",
     "iopub.status.idle": "2024-11-09T12:04:07.255534Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.087214Z",
     "shell.execute_reply": "2024-11-09T12:04:07.254706Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.812904Z",
     "start_time": "2024-12-26T16:32:22.738686Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "len(original_dataset)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.256674Z",
     "iopub.execute_input": "2024-11-09T12:04:07.256977Z",
     "iopub.status.idle": "2024-11-09T12:04:07.320972Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.256945Z",
     "shell.execute_reply": "2024-11-09T12:04:07.320056Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:22.833994Z",
     "start_time": "2024-12-26T16:32:22.818710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "x,y,z = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:07.322156Z",
     "iopub.execute_input": "2024-11-09T12:04:07.322491Z",
     "iopub.status.idle": "2024-11-09T12:04:08.712840Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.322458Z",
     "shell.execute_reply": "2024-11-09T12:04:08.711911Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.081866Z",
     "start_time": "2024-12-26T16:32:22.863367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 224, 224]) torch.Size([60]) torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "# # Check if the returned data is valid\n",
    "# if x is not None:\n",
    "#     # Convert the tensor to a numpy array\n",
    "#     x_np = x.numpy()\n",
    "#\n",
    "#     # Check the number of dimensions and squeeze if necessary\n",
    "#     if x_np.ndim == 4:  # RGB images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i].transpose(1, 2, 0))  # Convert CHW to HWC\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     elif x_np.ndim == 3:  # Grayscale images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i], cmap='gray')\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unexpected number of dimensions: {x_np.ndim}\")\n",
    "# else:\n",
    "#     print(\"No data available for this patient.\")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:08.713992Z",
     "iopub.execute_input": "2024-11-09T12:04:08.714306Z",
     "iopub.status.idle": "2024-11-09T12:04:10.720210Z",
     "shell.execute_reply.started": "2024-11-09T12:04:08.714273Z",
     "shell.execute_reply": "2024-11-09T12:04:10.719091Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.107713Z",
     "start_time": "2024-12-26T16:32:23.092193Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.074769,
     "end_time": "2024-10-04T19:24:01.438666",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.363897",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:10.721483Z",
     "iopub.execute_input": "2024-11-09T12:04:10.721855Z",
     "iopub.status.idle": "2024-11-09T12:04:10.790322Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.721821Z",
     "shell.execute_reply": "2024-11-09T12:04:10.789279Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.152974Z",
     "start_time": "2024-12-26T16:32:23.134742Z"
    }
   },
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NTXent Loss"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.201345Z",
     "start_time": "2024-12-26T16:32:23.180754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Define NTXentLoss\n",
    "# class NTXentLoss(nn.Module):\n",
    "#     def __init__(self, temperature=0.5):\n",
    "#         super(NTXentLoss, self).__init__()\n",
    "#         self.temperature = temperature\n",
    "#\n",
    "#     def forward(self, z_i, z_j):\n",
    "#         batch_size = z_i.size(0)\n",
    "#         z = torch.cat([z_i, z_j], dim=0)\n",
    "#         z = F.normalize(z, dim=1)\n",
    "#         similarity_matrix = torch.mm(z, z.T) / self.temperature\n",
    "#         mask = torch.eye(2 * batch_size, device=z.device).bool()\n",
    "#         similarity_matrix.masked_fill_(mask, -float('inf'))\n",
    "#         exp_sim = torch.exp(similarity_matrix)\n",
    "#         denominator = exp_sim.sum(dim=1)\n",
    "#         positive_samples = torch.cat(\n",
    "#             [torch.arange(batch_size, 2 * batch_size), torch.arange(batch_size)], dim=0\n",
    "#         ).to(z.device)\n",
    "#         positives = similarity_matrix[torch.arange(2 * batch_size), positive_samples]\n",
    "#         loss = -torch.log(torch.exp(positives) / denominator)\n",
    "#         return loss.mean()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.262085Z",
     "start_time": "2024-12-26T16:32:23.228238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_metric_learning import losses\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class NTXentLoss(losses.NTXentLoss):\n",
    "#     def __init__(self, temperature, **kwargs):\n",
    "#         super().__init__(temperature=temperature, **kwargs)\n",
    "#         self.temperature = temperature\n",
    "#\n",
    "#     def forward(self, embeddings1, embeddings2, labels=None, hard_pairs=None):\n",
    "#         # Concatenate the embeddings\n",
    "#         embeddings = torch.cat([embeddings1, embeddings2], dim=0)\n",
    "#         # Normalize feature vectors\n",
    "#         feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "#\n",
    "#         if labels == None:\n",
    "#             # Self-supervised labels\n",
    "#             labels = torch.arange(feature_vectors_normalized.size(0))\n",
    "#         else:\n",
    "#             # Supervised labels\n",
    "#             labels = torch.cat([labels, labels], dim=0)\n",
    "#\n",
    "#         # Compute logits\n",
    "#         logits = torch.div(\n",
    "#             torch.matmul(\n",
    "#                 feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "#             ),\n",
    "#             self.temperature,\n",
    "#         )\n",
    "#\n",
    "#         if labels == None:\n",
    "#             return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "#         if hard_pairs == None:\n",
    "#             return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "#         return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)\n",
    "\n",
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            # labels = torch.arange(feature_vectors_normalized.size(0))\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Augmentation For Contrastive Learning"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.297880Z",
     "start_time": "2024-12-26T16:32:23.280114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Version 2: Avg time taken: 0.05 seconds for 1 augmentation (w ResizedCrop)\n",
    "def augment_batch(batch_images):\n",
    "    batch_size, num_instances, channels, height, width = batch_images.shape\n",
    "\n",
    "    # Define augmentation transformations using GPU-compatible operations\n",
    "    aug_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.1)),\n",
    "        # transforms.RandomRotation(degrees=(-5, 5)),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4)], p=0.6),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "    ])\n",
    "\n",
    "    # Apply transformations directly on the tensor without converting to PIL\n",
    "    augmented_batch = torch.empty_like(batch_images)  # Preallocate memory for augmented images\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_instances):\n",
    "            # Apply the transformation directly to the tensor\n",
    "            if CHANNELS == 1:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j])\n",
    "            else:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j].permute(2, 0, 1)).permute(1, 2, 0)\n",
    "\n",
    "    return augmented_batch.cuda()  # Move the augmented batch to GPU"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": "## CNN Feature Extractor",
   "metadata": {
    "papermill": {
     "duration": 0.015548,
     "end_time": "2024-10-04T19:24:01.470464",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.454916",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Attention Layer",
   "metadata": {
    "papermill": {
     "duration": 0.015987,
     "end_time": "2024-10-04T19:24:01.719520",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.703533",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.077274,
     "end_time": "2024-10-04T19:24:01.812862",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.735588",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:10.859388Z",
     "iopub.execute_input": "2024-11-09T12:04:10.859734Z",
     "iopub.status.idle": "2024-11-09T12:04:10.926160Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.859698Z",
     "shell.execute_reply": "2024-11-09T12:04:10.925167Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.346151Z",
     "start_time": "2024-12-26T16:32:23.325925Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": "class GatedAttention(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(GatedAttention, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            # nn.Tanh(),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.gate = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            # nn.Tanh(),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        \n    def forward(self, x):\n        # x shape: (batch_size, num_instances, input_dim)\n        attention_weights = self.attention(x)\n        gate_weights = torch.sigmoid(self.gate(x))\n        \n        weights = attention_weights * gate_weights\n        weights = F.softmax(weights, dim=1)\n        \n        return (x * weights).sum(dim=1), weights.squeeze(-1)",
   "metadata": {
    "papermill": {
     "duration": 0.079025,
     "end_time": "2024-10-04T19:24:01.908938",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.829913",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:10.931368Z",
     "iopub.execute_input": "2024-11-09T12:04:10.931690Z",
     "iopub.status.idle": "2024-11-09T12:04:11.000139Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.931654Z",
     "shell.execute_reply": "2024-11-09T12:04:10.999075Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.400261Z",
     "start_time": "2024-12-26T16:32:23.372484Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gaussian Process"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.436808Z",
     "start_time": "2024-12-26T16:32:23.418974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dual-Stream MIL Model (DSMIL)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.504855Z",
     "start_time": "2024-12-26T16:32:23.466336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(input_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class InstanceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(InstanceClassifier, self).__init__()\n",
    "        self.features_extractor = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # self.features_extractor = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.features_extractor.conv1 = nn.Conv2d(CHANNELS, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.features_extractor.fc = nn.Identity()\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if CHANNELS == 1:\n",
    "            batch_size, num_instances, C, H, W = x.shape\n",
    "        else:\n",
    "            batch_size, num_instances, H, W, C = x.shape\n",
    "        x = x.view(batch_size * num_instances, C, H, W)\n",
    "        \n",
    "        instance_features = nn.Dropout(0.25)(self.features_extractor(x)).view(batch_size, num_instances, -1)\n",
    "        classes = self.fc(instance_features)\n",
    "        \n",
    "        return instance_features, classes\n",
    "    \n",
    "class BagClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1, hidden_dim=128, dropout_v=0.2, non_linear=True, passing_v=False):\n",
    "        super(BagClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if non_linear:\n",
    "            self.q = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            self.q = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        if passing_v:\n",
    "            self.v = nn.Sequential(\n",
    "                nn.Dropout(dropout_v),\n",
    "                nn.Linear(input_dim, input_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.v = nn.Identity()\n",
    "            \n",
    "        self.fc = FCLayer(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, features, classes):\n",
    "        batch_size = features.size(0)\n",
    "        num_instances = features.size(1)\n",
    "        features_dim = features.size(2)\n",
    "        \n",
    "        combine_features = features.view(features.shape[0] * features.shape[1], -1)\n",
    "        V = self.v(combine_features)\n",
    "        Q = self.q(combine_features)\n",
    "        assert V.shape[0] == Q.shape[0] == batch_size * num_instances, f'V: {V.shape}, Q: {Q.shape}'\n",
    "        assert V.shape[1] == features_dim, f'V: {V.shape} should be [{batch_size * num_instances}, {features_dim}]'\n",
    "        assert Q.shape[1] == self.hidden_dim, f'Q: {Q.shape} should be [{batch_size * num_instances}, {self.hidden_dim}]'\n",
    "        \n",
    "        # Get critical instance indices by squeezing classes\n",
    "        critical_indices = classes.squeeze(-1).argmax(dim=1)\n",
    "        assert critical_indices.shape[0] == batch_size, f'Critical indices: {critical_indices.shape}'\n",
    "\n",
    "        # Gather features for each batch using critical instance indices\n",
    "        m_features = features[torch.arange(batch_size).unsqueeze(1), critical_indices.unsqueeze(1)].squeeze()\n",
    "        m_features = m_features.view(batch_size, -1)\n",
    "        assert m_features.shape[0] == batch_size, f'M features: {m_features.shape} should be [{batch_size}, {features_dim}]'\n",
    "        q_max = self.q(m_features)\n",
    "        assert q_max.shape[0] == batch_size and q_max.shape[1] == self.hidden_dim, f'Q max: {q_max.shape} should be [{batch_size}, {self.hidden_dim}]'\n",
    "        \n",
    "        A = torch.mm(Q, q_max.mT)\n",
    "        A = F.softmax(A / torch.sqrt(torch.tensor(Q.shape[-1]).float()), dim=0)\n",
    "        assert A.shape[0] == batch_size * num_instances and A.shape[1] == batch_size, f'A: {A.shape} should be [{batch_size * num_instances}, {batch_size}]'\n",
    "        \n",
    "        B = torch.mm(A.T, V)\n",
    "        assert B.shape[0] == batch_size and B.shape[1] == features_dim, f'B: {B.shape} should be [{batch_size}, {features_dim}]'\n",
    "        \n",
    "        B = B.view(1, B.shape[0], B.shape[1]) # Shape of B: [1, batch_size, features_dim]\n",
    "        C = self.fc(B)\n",
    "        C = C.view(1, -1) # Shape of C: [1, batch_size, 1] -> [1, batch_size]\n",
    "        return C, A, B"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": "### ResNet2D Model",
   "metadata": {
    "papermill": {
     "duration": 0.015805,
     "end_time": "2024-10-04T19:24:02.065521",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.049716",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.353086",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.337113",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels=CHANNELS, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "        # self.attention = AttentionLayer(input_dim=512, hidden_dim=512)\n",
    "        self.attention = GatedAttention(input_dim=512, hidden_dim=512)\n",
    "\n",
    "        self.classifier = nn.Linear(512 + 1, 1) \n",
    "        self.attention_classifier = nn.Linear(512, 1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        inducing_points = torch.randn(32, 512)\n",
    "        self.gp_layer = GPModel(inducing_points=inducing_points)\n",
    "        \n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(512 + 1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, bags):\n",
    "        if CHANNELS == 1:\n",
    "            batch_size, num_instances, c, h, w = bags.size()\n",
    "        else:\n",
    "            batch_size, num_instances, h, w, c = bags.size()\n",
    "\n",
    "        bags_flattened = bags.view(batch_size * num_instances, c, h, w)\n",
    "        \n",
    "        # # Version 1: CNN-ResNet + Att + GP\n",
    "        features = self.resnet(bags_flattened)\n",
    "        features = self.dropout(features)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "        \n",
    "        # projection_head = self.projection_head(features.view(batch_size * num_instances, -1))\n",
    "\n",
    "        attended_features, attended_weights = self.attention(features)\n",
    "        # attended_features_reshaped = attended_features.view(-1, 512)\n",
    "        attended_features_reshaped = attended_features.view(batch_size, -1)\n",
    "\n",
    "        # Ver2.1\n",
    "        # projection_head = self.projection_head(attended_features_reshaped)\n",
    "        # Ver2.2\n",
    "        # projection_head = attended_features_reshaped\n",
    "\n",
    "        # CNN_ATT_GP\n",
    "        gp_output = self.gp_layer(attended_features_reshaped)\n",
    "        gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "\n",
    "        combine_features = torch.cat((attended_features, gp_mean), dim=1)\n",
    "\n",
    "        projection_head = self.projection_head(combine_features)\n",
    "\n",
    "        combine_features = self.dropout(combine_features)\n",
    "\n",
    "        # outputs = torch.sigmoid(self.classifier(combine_features))\n",
    "        # att_outputs = torch.sigmoid(self.attention_classifier(attended_features_reshaped))\n",
    "        outputs = self.classifier(combine_features)\n",
    "        att_outputs = self.attention_classifier(attended_features_reshaped)\n",
    "\n",
    "        return outputs, att_outputs, attended_weights, gp_output, projection_head"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.071536Z",
     "iopub.execute_input": "2024-11-09T12:04:11.071923Z",
     "iopub.status.idle": "2024-11-09T12:04:11.143565Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.071889Z",
     "shell.execute_reply": "2024-11-09T12:04:11.142556Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.550050Z",
     "start_time": "2024-12-26T16:32:23.524001Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple CNN Model "
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.595847Z",
     "start_time": "2024-12-26T16:32:23.573784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=CHANNELS, out_channels=16, kernel_size=(5, 5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))  # Max pooling layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3,3))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3))\n",
    "        self.conv_layers = nn.ModuleList([self.conv2 for _ in range(3)])\n",
    "        self.fc = nn.Linear(800, 1)\n",
    "        \n",
    "    def forward(self, bag):\n",
    "        if CHANNELS == 1:\n",
    "            batch_size, num_instances, c, h, w = bag.size()\n",
    "        else:\n",
    "            batch_size, num_instances, h, w, c = bag.size()\n",
    "            \n",
    "        bag = bag.view(batch_size * num_instances, c, h, w)\n",
    "        \n",
    "        # First convolutional layer\n",
    "        x = self.conv0(bag)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(batch_size, num_instances, -1) # From (batch_size * num_instances, 32, 5, 5) to (batch_size, num_instances, 32 * 5 * 5)\n",
    "        \n",
    "        classes = self.fc(x)\n",
    "        return x, classes\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoder Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.648092Z",
     "start_time": "2024-12-26T16:32:23.623449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, projection_dim=128, input_dim=512):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.simple_cnn = SimpleCNN()\n",
    "        \n",
    "        self.instance_classifier = InstanceClassifier(input_dim)\n",
    "        self.bag_classifier = BagClassifier(input_dim)\n",
    "        \n",
    "        self.attention = AttentionLayer(input_dim=input_dim, hidden_dim=input_dim)\n",
    "        # self.fc = nn.Linear(512, 1)\n",
    "        \n",
    "        inducing_points = torch.randn(32, input_dim) \n",
    "        self.gp_model = GPModel(inducing_points=inducing_points)\n",
    "        self.fc = nn.Linear(input_dim + 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_instances, channels, height, width = x.size()\n",
    "        \n",
    "        instances_features, classes = self.instance_classifier(x)\n",
    "  \n",
    "        # instances_features, classes = self.simple_cnn(x)\n",
    "        \n",
    "        features = instances_features.view(batch_size * num_instances, -1)  # Flatten to (batch_size * num_instances, feature_dim)\n",
    "        \n",
    "        projection_features = self.projection(features)\n",
    "        \n",
    "        predicted_bags, A, B = self.bag_classifier(instances_features, classes)\n",
    "        \n",
    "        # gp_output = self.gp_model(B.squeeze())\n",
    "        # gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "        # combined_features = torch.cat((B.squeeze(), gp_mean), dim=1) # [batch_size, 512 + 1]\n",
    "        # combined_features = self.fc(combined_features.squeeze())\n",
    "        # combined_features = torch.sigmoid(combined_features)\n",
    "        combined_features = None\n",
    "\n",
    "        # # Assuming gp_output.mean has shape [batch_size, 1] and B has shape [batch_size, num_instances, 512]\n",
    "        # gp_mean_expanded = gp_output.mean.unsqueeze(dim=1).unsqueeze(dim=0)  # Shape: [batch_size, 1] -> [1, batch_size, 1]\n",
    "        # gp_mean_broadcasted = gp_mean_expanded.expand(-1, -1, B.shape[-1])  # Shape: [1, batch_size, 1] -> [1, batch_size, feature_dim]\n",
    "        # combined_features = B + gp_mean_broadcasted  # Element-wise addition\n",
    "        \n",
    "        predicted_bags = torch.sigmoid(predicted_bags)\n",
    "        return projection_features, classes, predicted_bags, A, B, combined_features\n",
    "        \n",
    "        # attention_features, attention_weights = self.attention(instances_features)\n",
    "        # att_out = self.fc(attention_features)\n",
    "        # att_out = torch.sigmoid(att_out)\n",
    "        # return projection_features, classes, att_out, attention_weights, None, None\n",
    "        \n",
    "        # attention_features, attention_weights = self.attention(instances_features)\n",
    "        # gp_output = self.gp_model(attention_features)\n",
    "        # \n",
    "        # gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "        # combined_features = torch.cat((attention_features, gp_mean), dim=1)\n",
    "        # combined_features = self.fc(combined_features)\n",
    "        # combined_features = torch.sigmoid(combined_features)\n",
    "        # \n",
    "        # return projection_features, classes, attention_features, attention_weights, None, combined_features"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": "## Training and Evaluation",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.496444",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.480471",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loss Function"
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.144744Z",
     "iopub.execute_input": "2024-11-09T12:04:11.145137Z",
     "iopub.status.idle": "2024-11-09T12:04:11.209993Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.145066Z",
     "shell.execute_reply": "2024-11-09T12:04:11.209156Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.694482Z",
     "start_time": "2024-12-26T16:32:23.674747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    # Cross-Entropy Loss for CNN outputs\n",
    "    # bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n",
    "    # bce_loss = nn.BCEWithLogitsLoss(outputs.squeeze(), target.float()) # Safe with autocast\n",
    "    bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "    kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "    total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    \n",
    "    return total_loss"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": "### Training",
   "metadata": {
    "papermill": {
     "duration": 0.016325,
     "end_time": "2024-10-04T19:24:02.529050",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.512725",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.740700Z",
     "start_time": "2024-12-26T16:32:23.719748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_metric_learning.miners import BaseMiner\n",
    "from pytorch_metric_learning.utils import loss_and_miner_utils as lmu\n",
    "\n",
    "class ExamplePairMiner(BaseMiner):\n",
    "    def __init__(self, margin=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "\n",
    "    def mine(self, embeddings, labels, ref_emb, ref_labels):\n",
    "        mat = self.distance(embeddings, ref_emb)\n",
    "        a1, p, a2, n = lmu.get_all_pairs_indices(labels, ref_labels)\n",
    "        pos_pairs = mat[a1, p]\n",
    "        neg_pairs = mat[a2, n]\n",
    "        pos_mask = (\n",
    "            pos_pairs < self.margin\n",
    "            if self.distance.is_inverted\n",
    "            else pos_pairs > self.margin\n",
    "        )\n",
    "        neg_mask = (\n",
    "            neg_pairs > self.margin\n",
    "            if self.distance.is_inverted\n",
    "            else neg_pairs < self.margin\n",
    "        )\n",
    "        return a1[pos_mask], p[pos_mask], a2[neg_mask], n[neg_mask]"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, data_loader, criterion_cl, criterion_bce, optimizer, scheduler, scaler, device):\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # MIL ResNet18\n",
    "        # output, attention_out, _, _ = model(batch_data)\n",
    "        # LOSS VER1: This way is not good\n",
    "        # loss_att = criterion_bce(attention_out.squeeze(), batch_patient_labels)\n",
    "        # loss_gp = criterion_bce(output.squeeze(), batch_patient_labels)\n",
    "        # loss = loss_att * 0.3 + loss_gp * 0.4 + model.gp_layer.variational_strategy.kl_divergence() * 0.3\n",
    "        # LOSS VER2: This way is better\n",
    "        # loss_model = combined_loss(output, model.gp_layer, batch_patient_labels)\n",
    "        \n",
    "        aug1 = augment_batch(batch_data).cuda()\n",
    "        aug2 = augment_batch(batch_data).cuda()\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            # output_1, attention_out_1, _, _, proj_head_1 = model(aug1)\n",
    "            # output_2, attention_out_2, _, _, proj_head_2 = model(aug2)\n",
    "            #\n",
    "            # miner_func = ExamplePairMiner()\n",
    "            # hard_pairs = miner_func(torch.cat([output_1, output_2], dim=0), torch.cat([batch_patient_labels, batch_patient_labels], dim=0))\n",
    "            #\n",
    "            # NTXLoss = criterion_cl(proj_head_1, proj_head_2, batch_patient_labels, hard_pairs)\n",
    "            # # NTXLoss = criterion_cl(proj_head_1, proj_head_2, batch_patient_labels)\n",
    "            # loss_mod_1 = combined_loss(output_1, model.gp_layer, batch_patient_labels)\n",
    "            # loss_mod_2 = combined_loss(output_2, model.gp_layer, batch_patient_labels)\n",
    "            # loss = NTXLoss * 0.7 + loss_mod_1 * 0.15 + loss_mod_2 * 0.15\n",
    "\n",
    "            output, attention_out, _, _, proj_head = model(torch.cat([aug1, aug2], dim=0))\n",
    "            batch_patient_labels = torch.cat([batch_patient_labels, batch_patient_labels], dim=0)\n",
    "            miner_func = ExamplePairMiner()\n",
    "            hard_pairs = miner_func(output, batch_patient_labels)\n",
    "            #\n",
    "            NTXLoss = criterion_cl(proj_head, batch_patient_labels, hard_pairs)\n",
    "            # NTXLoss = criterion_cl(proj_head)\n",
    "            loss_mod = combined_loss(output, model.gp_layer, batch_patient_labels)\n",
    "            loss = NTXLoss * 0.7 + loss_mod * 0.3\n",
    "\n",
    "        # loss = loss_mod_1 * 0.5 + loss_mod_2 * 0.5\n",
    "        loss = loss.mean()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # scheduler.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        # scaler.step(scheduler)\n",
    "        scheduler.step()\n",
    "\n",
    "        scaler.update()\n",
    "\n",
    "        # predictions.extend((predicted_bags_1.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        # predictions.extend((gp_combine_1.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        \n",
    "        # MIL-ResNet18\n",
    "        # predictions.extend((output_1.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def validate(model, data_loader, criterion_cl, criterion_bce, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            \n",
    "            # z_i, output, predicted_bags, _, _, gp_combine = model(batch_data)\n",
    "            # max_agg = torch.max(output, dim=1).values.squeeze()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            # predictions.extend((predicted_bags.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "            # predictions.extend((gp_combine.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "            \n",
    "            output, attention_out, _, _, _ = model(batch_data)\n",
    "            # MIL-ResNet18\n",
    "            predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate, device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "                                              steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "    scaler = GradScaler()\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, train_loader, criterion_cl, criterion_bce, optimizer, scheduler, scaler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\"train/loss\": train_loss / len(train_loader), **train_metrics})\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, val_loader, criterion_cl, criterion_bce, device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\"val/loss\": val_loss / len(val_loader), **val_metrics})\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.089765,
     "end_time": "2024-10-04T19:24:02.636485",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.546720",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.211419Z",
     "iopub.execute_input": "2024-11-09T12:04:11.212295Z",
     "iopub.status.idle": "2024-11-09T12:04:11.290690Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.212259Z",
     "shell.execute_reply": "2024-11-09T12:04:11.289598Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.788427Z",
     "start_time": "2024-12-26T16:32:23.765804Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": "### Evaluation Functions",
   "metadata": {
    "papermill": {
     "duration": 0.016054,
     "end_time": "2024-10-04T19:24:02.668925",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.652871",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode(): \n",
    "        for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            \n",
    "            # z_i, outputs, predicted_bags, _, _, gp_combine = model(batch_data)\n",
    "            # \n",
    "            # predictions.extend((predicted_bags.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "            # # predictions.extend((gp_combine.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "            \n",
    "            output, attention_out, _, _, _ = model(batch_data)\n",
    "            # MIL-ResNet18\n",
    "            predictions.extend((output.squeeze() > 0.25).cpu().detach().numpy())\n",
    "            # predictions.extend((output.squeeze() > 0.1).cpu().detach().numpy())\n",
    "            \n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.079611,
     "end_time": "2024-10-04T19:24:02.764721",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.685110",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.291769Z",
     "iopub.execute_input": "2024-11-09T12:04:11.292052Z",
     "iopub.status.idle": "2024-11-09T12:04:11.359152Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.292017Z",
     "shell.execute_reply": "2024-11-09T12:04:11.358128Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.849410Z",
     "start_time": "2024-12-26T16:32:23.831295Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": "### Visualization Functions",
   "metadata": {
    "papermill": {
     "duration": 0.01814,
     "end_time": "2024-10-04T19:24:02.800208",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.782068",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "\n",
    "            # z_i, outputs, predicted_bags, _, _, gp_combine = model(batch_data)\n",
    "            # \n",
    "            # predictions.extend(predicted_bags.squeeze().cpu().numpy())\n",
    "            # # predictions.extend(gp_combine.squeeze().cpu().numpy())\n",
    "            \n",
    "            output, attention_out, _, _, _ = model(batch_data)\n",
    "            # MIL-ResNet18\n",
    "            predictions.extend(output.squeeze().cpu().numpy())\n",
    "            \n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            \n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.092339,
     "end_time": "2024-10-04T19:24:02.911187",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.818848",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.360394Z",
     "iopub.execute_input": "2024-11-09T12:04:11.360796Z",
     "iopub.status.idle": "2024-11-09T12:04:11.429794Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.360761Z",
     "shell.execute_reply": "2024-11-09T12:04:11.428871Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.901472Z",
     "start_time": "2024-12-26T16:32:23.876500Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": "### Data Processing Functions",
   "metadata": {
    "papermill": {
     "duration": 0.018543,
     "end_time": "2024-10-04T19:24:02.948830",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.930287",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class()\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.079014,
     "end_time": "2024-10-04T19:24:03.046531",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.967517",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.430912Z",
     "iopub.execute_input": "2024-11-09T12:04:11.431237Z",
     "iopub.status.idle": "2024-11-09T12:04:11.497859Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.431205Z",
     "shell.execute_reply": "2024-11-09T12:04:11.496987Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.944424Z",
     "start_time": "2024-12-26T16:32:23.926092Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": "## Visualizing Attention Weights and Images",
   "metadata": {
    "papermill": {
     "duration": 0.01837,
     "end_time": "2024-10-04T19:24:03.082997",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.064627",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_label_attention_weights(model, data_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Plot images with their labels and attention values in a single large plot.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model\n",
    "    - data_loader: DataLoader containing test dataset\n",
    "    - device: Device to run the model on ('cuda' or 'cpu')\n",
    "    - CHANNELS: Number of channels in the image (e.g., 1 for grayscale, 3 for RGB)\n",
    "\n",
    "    Expected shapes:\n",
    "    - 1-channel images: (batch_size, num_images, 224, 224)\n",
    "    - 3-channel images: (batch_size, num_images, 3, 224, 224)\n",
    "    - attention: float value per image indicating attention weight\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    num_images = MAX_SLICES\n",
    "    rows, cols = 10, 6  # Adjust to fit 60 images in a single plot\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patients_label in data_loader:\n",
    "            # Move data to the appropriate device\n",
    "            batch_data = batch_data.to(device)\n",
    "            outputs, _, attention_weight_batch, _, _ = model(batch_data)\n",
    "\n",
    "            # Process each patient in the batch\n",
    "            for patient_idx in range(batch_data.size(0)):\n",
    "                if batch_patients_label[patient_idx].item() == 1:  # Check if patient has positive label\n",
    "                    # Create a new figure for this patient\n",
    "                    fig = plt.figure(figsize=(cols * 4, rows * 4 + 2))  # Increased height for suptitle\n",
    "\n",
    "                    for img_idx in range(num_images):\n",
    "                        # Get the image and its label\n",
    "                        img = batch_data[patient_idx, img_idx].cpu().numpy()\n",
    "                        img_label = batch_labels[patient_idx, img_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Get attention value\n",
    "                        if attention_weight_batch.size(1) == batch_data.size(1):\n",
    "                            attention_value = attention_weight_batch[patient_idx, img_idx].cpu().item()\n",
    "                        else:\n",
    "                            attention_value = attention_weight_batch[patient_idx].cpu().item()\n",
    "                        \n",
    "                        # Plot image\n",
    "                        plt.subplot(rows, cols, img_idx + 1)\n",
    "                        if CHANNELS == 3:  # RGB image\n",
    "                            plt.imshow(img)\n",
    "                        else:  # Grayscale image\n",
    "                            if img.ndim == 3:  # If shape is (1, H, W)\n",
    "                                img = np.squeeze(img)  # Convert to (H, W)\n",
    "                            plt.imshow(img, cmap='gray')\n",
    "                        \n",
    "                        plt.title(f'Label: {img_label}\\nAttention: {attention_value:.4f}', fontsize=12)\n",
    "                        plt.axis('off')\n",
    "\n",
    "                    # Add overall title for the patient\n",
    "                    plt.suptitle(f'Patient Images (Patient Label: {batch_patients_label[patient_idx].cpu().numpy()})', fontsize=16)\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust rect to make space for suptitle\n",
    "                    plt.show()\n",
    "                                      \n",
    "                    # Since we are plotting only for one patient, return after the first plot\n",
    "                    return"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.095451,
     "end_time": "2024-10-04T19:24:03.197071",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.101620",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.499019Z",
     "iopub.execute_input": "2024-11-09T12:04:11.499350Z",
     "iopub.status.idle": "2024-11-09T12:04:11.569343Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.499317Z",
     "shell.execute_reply": "2024-11-09T12:04:11.568408Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:23.996471Z",
     "start_time": "2024-12-26T16:32:23.972681Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualization Augmented Bags"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:24.042106Z",
     "start_time": "2024-12-26T16:32:24.021334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_augmented_bags(original_bags, augmented_bags, num_bags=12):\n",
    "    \"\"\"\n",
    "    Visualizes all instances of the first bag of original and augmented images.\n",
    "\n",
    "    Parameters:\n",
    "    - original_bags: A tensor of shape (batch_size, num_instances, channels, height, width)\n",
    "    - augmented_bags: A tensor of shape (batch_size, num_instances, channels, height, width)\n",
    "    - num_bags: Number of bags to visualize (only the first bag will be shown).\n",
    "    \"\"\"\n",
    "    # Only visualize the first bag\n",
    "    first_bag_index = 0\n",
    "    \n",
    "    # Get number of instances\n",
    "    num_instances = original_bags.size(1)\n",
    "    \n",
    "    print(f'Num instances: {num_instances}')\n",
    "    \n",
    "    # Limit the number of bags to visualize (but we only show the first one)\n",
    "    num_bags = min(num_bags, 1)  # We only want to visualize the first bag\n",
    "\n",
    "    fig, axes = plt.subplots(num_instances, 2, figsize=(10, 2 * num_instances))\n",
    "    \n",
    "    # Original images\n",
    "    for j in range(num_instances):  # Iterate over instances in the first bag\n",
    "        img = original_bags[first_bag_index][j].cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        axes[j, 0].imshow(img, cmap='gray')  # Use gray colormap for single channel images\n",
    "        axes[j, 0].axis('off')  # Hide axes for better visualization\n",
    "        axes[j, 0].set_title(f'Original Instance {j + 1}')\n",
    "        \n",
    "    # Augmented images\n",
    "    for j in range(num_instances):\n",
    "        img = augmented_bags[first_bag_index][j].cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        axes[j, 1].imshow(img.squeeze(), cmap='gray')  # Use gray colormap for single channel images\n",
    "        axes[j, 1].axis('off')  # Hide axes for better visualization\n",
    "        axes[j, 1].set_title(f'Augmented Instance {j + 1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": "## Main",
   "metadata": {
    "papermill": {
     "duration": 0.016248,
     "end_time": "2024-10-04T19:24:03.231225",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.214977",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "def set_seed(seed=42):\n    \"\"\"Set seed for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    np.random.seed(seed)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.570677Z",
     "iopub.execute_input": "2024-11-09T12:04:11.571424Z",
     "iopub.status.idle": "2024-11-09T12:04:11.634204Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.571366Z",
     "shell.execute_reply": "2024-11-09T12:04:11.633349Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:24.089195Z",
     "start_time": "2024-12-26T16:32:24.068473Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [
    "def main(mode='train'):\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\")\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    set_seed()\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=0.0)\n",
    "    test_labels = pd.read_csv('./data_analyze/testing_dataset_150.csv')\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    set_seed()\n",
    "    \n",
    "    # Initialize model, criterion, and optimizer\n",
    "    model = MILResNet18()\n",
    "    # model = Encoder()\n",
    "    \n",
    "    criterion_cl = NTXentLoss(0.5)\n",
    "    criterion_bce = torch.nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    if mode == 'train':\n",
    "        # Watch the model to log gradients and parameters\n",
    "        wandb.watch(model)\n",
    "\n",
    "        # Train model\n",
    "        trained_model = train_model(model, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    # Load best model\n",
    "    # trained_model = load_model(Encoder, MODEL_PATH)\n",
    "    trained_model = load_model(MILResNet18, MODEL_PATH)\n",
    "\n",
    "    # Evaluate model\n",
    "    predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log(metrics)\n",
    "\n",
    "    print_metrics(metrics)\n",
    "    \n",
    "    # if mode == 'train':\n",
    "        # Visualizations\n",
    "    plot_roc_curve(trained_model, test_loader, DEVICE)\n",
    "    plot_confusion_matrix(trained_model, test_loader, DEVICE)\n",
    "\n",
    "    if mode == 'train':\n",
    "        required_columns = ['patient_id', 'study_instance_uid', 'patient_label']\n",
    "        temp_test_labels = test_labels[required_columns]\n",
    "        \n",
    "        # Save results\n",
    "        # results_df = get_test_results(trained_model, test_loader, temp_test_labels, device)\n",
    "        # results_df.to_csv('results/results.csv', index=False)\n",
    "        # print(results_df.head())\n",
    "\n",
    "        # # Log results DataFrame as a table in W&B (optional)\n",
    "        # wandb.log({\"results\": wandb.Table(dataframe=results_df)})\n",
    "\n",
    "    # # Call the function with the test_loader\n",
    "    # if isinstance(model, MILResNet18):\n",
    "    #     test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "    #     plot_label_attention_weights(trained_model, test_loader, device)\n",
    "    \n",
    "    # Get the first batch of images from the evaluation loader\n",
    "    images, _, _ = next(iter(train_loader))\n",
    "    print(f'Original batch shape: {images.shape}')\n",
    "    \n",
    "    # Augment the batch of images\n",
    "    # Start timer \n",
    "    start = time.time()\n",
    "    augmented_images = augment_batch(images)\n",
    "    end = time.time()\n",
    "    taken_time = end - start\n",
    "    print(f'Augmented batch shape: {augmented_images.shape} | Time: {taken_time:.4f}')\n",
    "    \n",
    "    # Visualize the original and augmented bags\n",
    "    visualize_augmented_bags(images, augmented_images)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ],
   "metadata": {
    "papermill": {
     "duration": 24983.553969,
     "end_time": "2024-10-05T02:20:26.801619",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.247650",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-11-09T12:04:11.635549Z",
     "iopub.execute_input": "2024-11-09T12:04:11.635991Z",
     "iopub.status.idle": "2024-11-09T12:18:52.781211Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.635946Z",
     "shell.execute_reply": "2024-11-09T12:18:52.780156Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-12-26T16:32:34.604376Z",
     "start_time": "2024-12-26T16:32:24.125143Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_246533/3011830513.py:128: FutureWarning: `torch.cpu.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cpu', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTX Loss: 0.0\n",
      "NTX Loss: 1.4814082384109497\n",
      "NTX Loss: 1.4813593626022339\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 1.4815059900283813\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 1.386096477508545\n",
      "NTX Loss: 1.4811642169952393\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 1.183528184890747\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 1.1831483840942383\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 1.9443966150283813\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 1.4813594818115234\n",
      "NTX Loss: 1.4815547466278076\n",
      "NTX Loss: 0.0\n",
      "NTX Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 91\u001B[0m\n\u001B[1;32m     88\u001B[0m     visualize_augmented_bags(images, augmented_images)\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 91\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[45], line 35\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(mode)\u001B[0m\n\u001B[1;32m     32\u001B[0m wandb\u001B[38;5;241m.\u001B[39mwatch(model)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_cl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_bce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Save model\u001B[39;00m\n\u001B[1;32m     38\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(trained_model\u001B[38;5;241m.\u001B[39mstate_dict(), MODEL_PATH)\n",
      "Cell \u001B[0;32mIn[38], line 134\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate, device)\u001B[0m\n\u001B[1;32m    130\u001B[0m best_model_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;66;03m# Training phase\u001B[39;00m\n\u001B[0;32m--> 134\u001B[0m     train_loss, train_predictions, train_labels \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_cl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion_bce\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m calculate_metrics(train_predictions, train_labels)\n\u001B[1;32m    136\u001B[0m     print_epoch_stats(epoch, num_epochs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_loss, train_metrics)\n",
      "Cell \u001B[0;32mIn[38], line 26\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, data_loader, criterion_cl, criterion_bce, optimizer, scheduler, scaler, device)\u001B[0m\n\u001B[1;32m     15\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# MIL ResNet18\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# output, attention_out, _, _ = model(batch_data)\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# LOSS VER1: This way is not good\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# LOSS VER2: This way is better\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# loss_model = combined_loss(output, model.gp_layer, batch_patient_labels)\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m aug1 \u001B[38;5;241m=\u001B[39m \u001B[43maugment_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     27\u001B[0m aug2 \u001B[38;5;241m=\u001B[39m augment_batch(batch_data)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautocast(device_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16):\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# output_1, attention_out_1, _, _, proj_head_1 = model(aug1)\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;66;03m# output_2, attention_out_2, _, _, proj_head_2 = model(aug2)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;66;03m# loss_mod_2 = combined_loss(output_2, model.gp_layer, batch_patient_labels)\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;66;03m# loss = NTXLoss * 0.7 + loss_mod_1 * 0.15 + loss_mod_2 * 0.15\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[28], line 20\u001B[0m, in \u001B[0;36maugment_batch\u001B[0;34m(batch_images)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_instances):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Apply the transformation directly to the tensor\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m CHANNELS \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 20\u001B[0m         augmented_batch[i, j] \u001B[38;5;241m=\u001B[39m \u001B[43maug_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_images\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     22\u001B[0m         augmented_batch[i, j] \u001B[38;5;241m=\u001B[39m aug_transform(batch_images[i, j]\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_container.py:51\u001B[0m, in \u001B[0;36mCompose.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m     49\u001B[0m needs_unpacking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 51\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m outputs \u001B[38;5;28;01mif\u001B[39;00m needs_unpacking \u001B[38;5;28;01melse\u001B[39;00m (outputs,)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_container.py:105\u001B[0m, in \u001B[0;36mRandomApply.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs \u001B[38;5;28;01mif\u001B[39;00m needs_unpacking \u001B[38;5;28;01melse\u001B[39;00m inputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m--> 105\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m outputs \u001B[38;5;28;01mif\u001B[39;00m needs_unpacking \u001B[38;5;28;01melse\u001B[39;00m (outputs,)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_transform.py:50\u001B[0m, in \u001B[0;36mTransform.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m     45\u001B[0m needs_transform_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_needs_transform_list(flat_inputs)\n\u001B[1;32m     46\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_params(\n\u001B[1;32m     47\u001B[0m     [inpt \u001B[38;5;28;01mfor\u001B[39;00m (inpt, needs_transform) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_inputs, needs_transform_list) \u001B[38;5;28;01mif\u001B[39;00m needs_transform]\n\u001B[1;32m     48\u001B[0m )\n\u001B[0;32m---> 50\u001B[0m flat_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transform(inpt, params) \u001B[38;5;28;01mif\u001B[39;00m needs_transform \u001B[38;5;28;01melse\u001B[39;00m inpt\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (inpt, needs_transform) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_inputs, needs_transform_list)\n\u001B[1;32m     53\u001B[0m ]\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_transform.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     45\u001B[0m needs_transform_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_needs_transform_list(flat_inputs)\n\u001B[1;32m     46\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_params(\n\u001B[1;32m     47\u001B[0m     [inpt \u001B[38;5;28;01mfor\u001B[39;00m (inpt, needs_transform) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_inputs, needs_transform_list) \u001B[38;5;28;01mif\u001B[39;00m needs_transform]\n\u001B[1;32m     48\u001B[0m )\n\u001B[1;32m     50\u001B[0m flat_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43minpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m needs_transform \u001B[38;5;28;01melse\u001B[39;00m inpt\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (inpt, needs_transform) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_inputs, needs_transform_list)\n\u001B[1;32m     53\u001B[0m ]\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_color.py:165\u001B[0m, in \u001B[0;36mColorJitter._transform\u001B[0;34m(self, inpt, params)\u001B[0m\n\u001B[1;32m    163\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_kernel(F\u001B[38;5;241m.\u001B[39madjust_brightness, output, brightness_factor\u001B[38;5;241m=\u001B[39mbrightness_factor)\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m contrast_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 165\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_kernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_contrast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontrast_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontrast_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m saturation_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    167\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_kernel(F\u001B[38;5;241m.\u001B[39madjust_saturation, output, saturation_factor\u001B[38;5;241m=\u001B[39msaturation_factor)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_transform.py:35\u001B[0m, in \u001B[0;36mTransform._call_kernel\u001B[0;34m(self, functional, inpt, *args, **kwargs)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call_kernel\u001B[39m(\u001B[38;5;28mself\u001B[39m, functional: Callable, inpt: Any, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     34\u001B[0m     kernel \u001B[38;5;241m=\u001B[39m _get_kernel(functional, \u001B[38;5;28mtype\u001B[39m(inpt), allow_passthrough\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 35\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mkernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/functional/_color.py:199\u001B[0m, in \u001B[0;36madjust_contrast_image\u001B[0;34m(image, contrast_factor)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m]:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput image tensor permitted channel values are 1 or 3, but found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 199\u001B[0m fp \u001B[38;5;241m=\u001B[39m \u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n\u001B[1;32m    201\u001B[0m     grayscale_image \u001B[38;5;241m=\u001B[39m _rgb_to_grayscale_image(image, num_output_channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, preserve_dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": "## Results",
   "metadata": {}
  }
 ]
}
