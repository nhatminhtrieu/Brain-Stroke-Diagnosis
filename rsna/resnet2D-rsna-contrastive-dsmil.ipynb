{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015487,
     "end_time": "2024-10-04T19:23:30.290610",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.275123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:03:29.962697Z",
     "iopub.status.busy": "2024-11-09T12:03:29.962323Z",
     "iopub.status.idle": "2024-11-09T12:03:54.971291Z",
     "shell.execute_reply": "2024-11-09T12:03:54.970161Z",
     "shell.execute_reply.started": "2024-11-09T12:03:29.962659Z"
    },
    "papermill": {
     "duration": 13.842163,
     "end_time": "2024-10-04T19:23:44.147775",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.305612",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:37.898838Z",
     "start_time": "2025-01-01T17:20:36.632990Z"
    }
   },
   "source": [
    "!pip install gpytorch\n",
    "!pip install wandb"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n",
      "Requirement already satisfied: wandb in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (0.18.5)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (5.28.3)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.17.0)\r\n",
      "Requirement already satisfied: setproctitle in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (72.1.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:03:54.973660Z",
     "iopub.status.busy": "2024-11-09T12:03:54.973321Z",
     "iopub.status.idle": "2024-11-09T12:04:04.345572Z",
     "shell.execute_reply": "2024-11-09T12:04:04.344540Z",
     "shell.execute_reply.started": "2024-11-09T12:03:54.973624Z"
    },
    "papermill": {
     "duration": 11.793387,
     "end_time": "2024-10-04T19:23:55.956283",
     "exception": false,
     "start_time": "2024-10-04T19:23:44.162896",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.154170Z",
     "start_time": "2025-01-01T17:20:37.910819Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_metric_learning import losses\n",
    "from torch.cpu.amp import GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from models.mil_resnet import CNN_ATT_GP, CNN_GP_ATT, SupConResnet, LinearClassifier, DKLModel\n",
    "from utils import hard_negative_mining as hnm\n",
    "import gpytorch\n",
    "from layers.gaussian_process import GPModel, PGLikelihood\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.347247Z",
     "iopub.status.busy": "2024-11-09T12:04:04.346735Z",
     "iopub.status.idle": "2024-11-09T12:04:04.352304Z",
     "shell.execute_reply": "2024-11-09T12:04:04.351162Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.347210Z"
    },
    "papermill": {
     "duration": 0.02447,
     "end_time": "2024-10-04T19:23:55.997833",
     "exception": false,
     "start_time": "2024-10-04T19:23:55.973363",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.182197Z",
     "start_time": "2025-01-01T17:20:40.180637Z"
    }
   },
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024749,
     "end_time": "2024-10-04T19:23:56.038412",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.013663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations\n",
    "\n",
    "### GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.354684Z",
     "iopub.status.busy": "2024-11-09T12:04:04.354377Z",
     "iopub.status.idle": "2024-11-09T12:04:04.410802Z",
     "shell.execute_reply": "2024-11-09T12:04:04.409843Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.354653Z"
    },
    "papermill": {
     "duration": 0.105269,
     "end_time": "2024-10-04T19:23:56.159555",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.054286",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.248413Z",
     "start_time": "2025-01-01T17:20:40.225784Z"
    }
   },
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.412279Z",
     "iopub.status.busy": "2024-11-09T12:04:04.411954Z",
     "iopub.status.idle": "2024-11-09T12:04:04.479489Z",
     "shell.execute_reply": "2024-11-09T12:04:04.478787Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.412244Z"
    },
    "papermill": {
     "duration": 0.081958,
     "end_time": "2024-10-04T19:23:56.258405",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.176447",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.309647Z",
     "start_time": "2025-01-01T17:20:40.290106Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed Everything"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.356507Z",
     "start_time": "2025-01-01T17:20:40.338560Z"
    }
   },
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016234,
     "end_time": "2024-10-04T19:23:56.291671",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.275437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Constant Info"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.480805Z",
     "iopub.status.busy": "2024-11-09T12:04:04.480520Z",
     "iopub.status.idle": "2024-11-09T12:04:04.541310Z",
     "shell.execute_reply": "2024-11-09T12:04:04.540405Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.480773Z"
    },
    "papermill": {
     "duration": 0.076674,
     "end_time": "2024-10-04T19:23:56.384583",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.307909",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.403348Z",
     "start_time": "2025-01-01T17:20:40.383576Z"
    }
   },
   "source": [
    "import yaml\n",
    "with open(\"../config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.542646Z",
     "iopub.status.busy": "2024-11-09T12:04:04.542362Z",
     "iopub.status.idle": "2024-11-09T12:04:04.649755Z",
     "shell.execute_reply": "2024-11-09T12:04:04.648985Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.542615Z"
    },
    "papermill": {
     "duration": 4.085647,
     "end_time": "2024-10-04T19:24:00.487263",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.401616",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.471139Z",
     "start_time": "2025-01-01T17:20:40.429798Z"
    }
   },
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "ROOT_DIR = '/kaggle/input/rsna-mil-training/' if KAGGLE else None\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "DICOM_DIR = DATA_DIR\n",
    "CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_1150.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "\n",
    "# Load patient scan labels\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.651199Z",
     "iopub.status.busy": "2024-11-09T12:04:04.650858Z",
     "iopub.status.idle": "2024-11-09T12:04:06.250597Z",
     "shell.execute_reply": "2024-11-09T12:04:06.249864Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.651166Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.496809Z",
     "start_time": "2025-01-01T17:20:40.477840Z"
    }
   },
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret(\"Wandb key\")\n",
    "# \n",
    "# wandb.login(key=key, relogin=True)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.252251Z",
     "iopub.status.busy": "2024-11-09T12:04:06.251742Z",
     "iopub.status.idle": "2024-11-09T12:04:06.331892Z",
     "shell.execute_reply": "2024-11-09T12:04:06.331086Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.252216Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.552620Z",
     "start_time": "2025-01-01T17:20:40.529633Z"
    }
   },
   "source": [
    "patient_scan_labels.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_e29b63250.dcm', 'ID_766be7451.dcm', 'ID_b...   \n",
       "1  ['ID_b2e23d464.dcm', 'ID_0d5c28287.dcm', 'ID_3...   \n",
       "2  ['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...   \n",
       "3  ['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...   \n",
       "4  ['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_005f241d   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0075b28c   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...  ID_00760731   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00859e11   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00a3b735   \n",
       "\n",
       "  study_instance_uid                                     image_position  \\\n",
       "0      ID_07e2cf7b4b  ['[-125.000, -131.252, 63.879]', '[-125.000, -...   \n",
       "1      ID_0373dbdd02  ['[-125.000000, -130.318451, 38.952644]', '[-1...   \n",
       "2      ID_006a2c59e4  ['[-125.000, -146.200, 5.250]', '[-125.000, -1...   \n",
       "3      ID_01f49be39f  ['[-125.000, -121.018, -1.452]', '[-125.000, -...   \n",
       "4      ID_065682422f  ['[-125, -42, 69.9000244]', '[-125, -42, 74.90...   \n",
       "\n",
       "                                   samples_per_pixel  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                       pixel_spacing  \\\n",
       "0  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "1  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "2  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "3  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "4  ['[0.48828125, 0.48828125]', '[0.48828125, 0.4...   \n",
       "\n",
       "                                pixel_representation  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       window_center  \\\n",
       "0  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "1  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "2  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "3  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "4  ['[00036, 00036]', '[00036, 00036]', '[00036, ...   \n",
       "\n",
       "                                        window_width  \\\n",
       "0  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "1  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "2  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "3  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "4  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "\n",
       "                                   rescale_intercept  \\\n",
       "0  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "1  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "2  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "3  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "4  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "\n",
       "                                       rescale_slope  patient_label  \n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              0  \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              0  \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              1  \n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              1  \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...              0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>image_position</th>\n",
       "      <th>samples_per_pixel</th>\n",
       "      <th>pixel_spacing</th>\n",
       "      <th>pixel_representation</th>\n",
       "      <th>window_center</th>\n",
       "      <th>window_width</th>\n",
       "      <th>rescale_intercept</th>\n",
       "      <th>rescale_slope</th>\n",
       "      <th>patient_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_e29b63250.dcm', 'ID_766be7451.dcm', 'ID_b...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_005f241d</td>\n",
       "      <td>ID_07e2cf7b4b</td>\n",
       "      <td>['[-125.000, -131.252, 63.879]', '[-125.000, -...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_b2e23d464.dcm', 'ID_0d5c28287.dcm', 'ID_3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0075b28c</td>\n",
       "      <td>ID_0373dbdd02</td>\n",
       "      <td>['[-125.000000, -130.318451, 38.952644]', '[-1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>ID_00760731</td>\n",
       "      <td>ID_006a2c59e4</td>\n",
       "      <td>['[-125.000, -146.200, 5.250]', '[-125.000, -1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00859e11</td>\n",
       "      <td>ID_01f49be39f</td>\n",
       "      <td>['[-125.000, -121.018, -1.452]', '[-125.000, -...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00a3b735</td>\n",
       "      <td>ID_065682422f</td>\n",
       "      <td>['[-125, -42, 69.9000244]', '[-125, -42, 74.90...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['[0.48828125, 0.48828125]', '[0.48828125, 0.4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['[00036, 00036]', '[00036, 00036]', '[00036, ...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015844,
     "end_time": "2024-10-04T19:24:00.868235",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.852391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.538200Z",
     "iopub.status.busy": "2024-11-09T12:04:06.537885Z",
     "iopub.status.idle": "2024-11-09T12:04:06.603427Z",
     "shell.execute_reply": "2024-11-09T12:04:06.602545Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.538168Z"
    },
    "papermill": {
     "duration": 0.075616,
     "end_time": "2024-10-04T19:24:00.959889",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.884273",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.609788Z",
     "start_time": "2025-01-01T17:20:40.592772Z"
    }
   },
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016194,
     "end_time": "2024-10-04T19:24:01.118549",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.102355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.742670Z",
     "iopub.status.busy": "2024-11-09T12:04:06.742336Z",
     "iopub.status.idle": "2024-11-09T12:04:06.817331Z",
     "shell.execute_reply": "2024-11-09T12:04:06.816387Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.742638Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.663209Z",
     "start_time": "2025-01-01T17:20:40.637319Z"
    }
   },
   "source": [
    "class DatasetAugmentor:\n",
    "    def __init__(self, height, width, levels=2, seed=None):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.levels = levels  # Dynamic number of levels\n",
    "        self.seed = seed\n",
    "        self.params = []\n",
    "\n",
    "        # Create different levels of transforms based on the number of levels specified\n",
    "        for i in range(levels):\n",
    "            factor = (i + 1) / levels\n",
    "            self.params.append(\n",
    "                self._create_transform(\n",
    "                    degrees=int(15 * factor), \n",
    "                    translate_range=(0.2 * factor, 0.2 * factor),\n",
    "                    scale_range=(1 - 0.2 * factor, 1 + 0.2 * factor),\n",
    "                    brightness_range=0.2 * factor,\n",
    "                    contrast_range=0.2 * factor,\n",
    "                    blur_sigma_range=(0.5 * factor, 1.0 * factor),\n",
    "                    apply_elastic=(i >= levels // 2),\n",
    "                    level_name=f'level_{i + 1}'\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _sample_value(self, value_range):\n",
    "        if isinstance(value_range, tuple):\n",
    "            random.seed(self.seed)\n",
    "            return random.uniform(value_range[0], value_range[1])\n",
    "        return value_range\n",
    "\n",
    "    def _create_transform(self, degrees, translate_range, scale_range, brightness_range, contrast_range, blur_sigma_range, apply_elastic, level_name):\n",
    "        print(f\"Creating '{level_name}' transform with parameters:\")\n",
    "        sampled_values = {\n",
    "            \"degrees\": abs(self._sample_value((-degrees, degrees))),\n",
    "            \"translate\": (abs(self._sample_value(translate_range[0])), abs(self._sample_value(translate_range[1]))),\n",
    "            \"scale\": self._sample_value(scale_range),\n",
    "            \"brightness\": self._sample_value(brightness_range),\n",
    "            \"contrast\": self._sample_value(contrast_range),\n",
    "            \"blur_sigma\": self._sample_value(blur_sigma_range),\n",
    "            \"apply_elastic\": apply_elastic\n",
    "        }\n",
    "        \n",
    "        print(sampled_values)\n",
    "        return sampled_values\n",
    "\n",
    "    def apply_transform(self, image, level):\n",
    "        params = self.params[level]\n",
    "        transform = self._get_transform(params, channels=image.shape[0])\n",
    "        return transform(image)\n",
    "\n",
    "    def _get_transform(self, params, channels=3):\n",
    "        transform_list = [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomAffine(degrees=params[\"degrees\"], translate=params[\"translate\"], scale=(params[\"scale\"], params[\"scale\"])),\n",
    "            transforms.ColorJitter(brightness=params[\"brightness\"], contrast=params[\"contrast\"]),\n",
    "            transforms.GaussianBlur(kernel_size=(3, 3), sigma=params[\"blur_sigma\"]),\n",
    "            transforms.RandomApply([transforms.ElasticTransform()] if params[\"apply_elastic\"] else [], p=0.3),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(self.height),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "\n",
    "        if channels == 3:\n",
    "            transform_list.extend([\n",
    "                transforms.Normalize(mean=[0.16774411, 0.1360026, 0.19076315], std=[0.3101935, 0.27605791, 0.30469988]),\n",
    "                transforms.RandomApply([self._channel_shuffle], p=0.3)\n",
    "            ])\n",
    "        elif channels == 1:\n",
    "            transform_list.append(transforms.Normalize(mean=[0.16774411], std=[0.3101935]))\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "\n",
    "    def _channel_shuffle(self, tensor):\n",
    "        torch.manual_seed(self.seed)\n",
    "        channels = tensor.shape[0]\n",
    "        indices = torch.randperm(channels)\n",
    "        return tensor[indices]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.818878Z",
     "iopub.status.busy": "2024-11-09T12:04:06.818519Z",
     "iopub.status.idle": "2024-11-09T12:04:06.881845Z",
     "shell.execute_reply": "2024-11-09T12:04:06.881035Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.818836Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.704478Z",
     "start_time": "2025-01-01T17:20:40.687880Z"
    }
   },
   "source": [
    "augmentor = DatasetAugmentor(224, 224, seed=42)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'level_1' transform with parameters:\n",
      "{'degrees': 1.9519751784103718, 'translate': (0.1, 0.1), 'scale': 1.027885359691577, 'brightness': 0.1, 'contrast': 0.1, 'blur_sigma': 0.4098566996144709, 'apply_elastic': False}\n",
      "Creating 'level_2' transform with parameters:\n",
      "{'degrees': 4.182803953736514, 'translate': (0.2, 0.2), 'scale': 1.0557707193831534, 'brightness': 0.2, 'contrast': 0.2, 'blur_sigma': 0.8197133992289418, 'apply_elastic': True}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015494,
     "end_time": "2024-10-04T19:24:01.252123",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.236629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.859695Z",
     "start_time": "2025-01-01T17:20:40.739715Z"
    }
   },
   "source": [
    "from dataset_generators.RSNA_Dataset import MedicalScanDataset"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.022528Z",
     "iopub.status.busy": "2024-11-09T12:04:07.022171Z",
     "iopub.status.idle": "2024-11-09T12:04:07.085840Z",
     "shell.execute_reply": "2024-11-09T12:04:07.084942Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.022481Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:40.882082Z",
     "start_time": "2025-01-01T17:20:40.865537Z"
    }
   },
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.087252Z",
     "iopub.status.busy": "2024-11-09T12:04:07.086910Z",
     "iopub.status.idle": "2024-11-09T12:04:07.255534Z",
     "shell.execute_reply": "2024-11-09T12:04:07.254706Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.087214Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.073677Z",
     "start_time": "2025-01-01T17:20:40.927517Z"
    }
   },
   "source": [
    "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.256977Z",
     "iopub.status.busy": "2024-11-09T12:04:07.256674Z",
     "iopub.status.idle": "2024-11-09T12:04:07.320972Z",
     "shell.execute_reply": "2024-11-09T12:04:07.320056Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.256945Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.094297Z",
     "start_time": "2025-01-01T17:20:41.077674Z"
    }
   },
   "source": [
    "len(original_dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.322491Z",
     "iopub.status.busy": "2024-11-09T12:04:07.322156Z",
     "iopub.status.idle": "2024-11-09T12:04:08.712840Z",
     "shell.execute_reply": "2024-11-09T12:04:08.711911Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.322458Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.294607Z",
     "start_time": "2025-01-01T17:20:41.124418Z"
    }
   },
   "source": [
    "x,y,z, _ = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 224, 224, 3]) torch.Size([57]) torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:08.714306Z",
     "iopub.status.busy": "2024-11-09T12:04:08.713992Z",
     "iopub.status.idle": "2024-11-09T12:04:10.720210Z",
     "shell.execute_reply": "2024-11-09T12:04:10.719091Z",
     "shell.execute_reply.started": "2024-11-09T12:04:08.714273Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.321941Z",
     "start_time": "2025-01-01T17:20:41.305316Z"
    }
   },
   "source": [
    "# # Check if the returned data is valid\n",
    "# if x is not None:\n",
    "#     # Convert the tensor to a numpy array\n",
    "#     x_np = x.numpy()\n",
    "#\n",
    "#     # Check the number of dimensions and squeeze if necessary\n",
    "#     if x_np.ndim == 4:  # RGB images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i].transpose(1, 2, 0))  # Convert CHW to HWC\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     elif x_np.ndim == 3:  # Grayscale images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i], cmap='gray')\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unexpected number of dimensions: {x_np.ndim}\")\n",
    "# else:\n",
    "#     print(\"No data available for this patient.\")\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:10.721855Z",
     "iopub.status.busy": "2024-11-09T12:04:10.721483Z",
     "iopub.status.idle": "2024-11-09T12:04:10.790322Z",
     "shell.execute_reply": "2024-11-09T12:04:10.789279Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.721821Z"
    },
    "papermill": {
     "duration": 0.074769,
     "end_time": "2024-10-04T19:24:01.438666",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.363897",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.365196Z",
     "start_time": "2025-01-01T17:20:41.348029Z"
    }
   },
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTXent Loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.411454Z",
     "start_time": "2025-01-01T17:20:41.393231Z"
    }
   },
   "source": [
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation For Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.464341Z",
     "start_time": "2025-01-01T17:20:41.438006Z"
    }
   },
   "source": [
    "# Version 2: Avg time taken: 0.05 seconds for 1 augmentation (w ResizedCrop)\n",
    "def augment_batch(batch_images):\n",
    "    if CHANNELS == 1:\n",
    "        batch_size, num_instances, channels, height, width = batch_images.shape\n",
    "    else:\n",
    "        batch_size, num_instances, height, width, channels = batch_images.shape\n",
    "\n",
    "    # Define augmentation transformations using GPU-compatible operations\n",
    "    aug_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((height, width), scale=(0.8, 1.1)),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4)], p=0.6),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]),\n",
    "    ])\n",
    "\n",
    "    # Apply transformations directly on the tensor without converting to PIL\n",
    "    augmented_batch = torch.empty_like(batch_images)  # Preallocate memory for augmented images\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_instances):\n",
    "            if CHANNELS == 1:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j])\n",
    "            else:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j].permute(2, 0, 1)).permute(1, 2, 0)\n",
    "\n",
    "    return augmented_batch.cuda()  # Move the augmented batch to GPU"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.496444",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.480471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.503298Z",
     "start_time": "2025-01-01T17:20:41.485409Z"
    }
   },
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.145137Z",
     "iopub.status.busy": "2024-11-09T12:04:11.144744Z",
     "iopub.status.idle": "2024-11-09T12:04:11.209993Z",
     "shell.execute_reply": "2024-11-09T12:04:11.209156Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.145066Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.554641Z",
     "start_time": "2025-01-01T17:20:41.533040Z"
    }
   },
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    if CHANNELS == 1:\n",
    "        bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    else:\n",
    "        ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        ce_loss = ce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * kl_divergence\n",
    "\n",
    "    return total_loss"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.612654Z",
     "start_time": "2025-01-01T17:20:41.594709Z"
    }
   },
   "source": [
    "def calculate_distribution_extremes_batch(batch_array):\n",
    "    # Calculate percentiles directly using NumPy\n",
    "    lower_percentile = np.percentile(batch_array, 10, axis=1, keepdims=True)\n",
    "    upper_percentile = np.percentile(batch_array, 90, axis=1, keepdims=True)\n",
    "\n",
    "    # Get boolean masks for smallest and largest values\n",
    "    smallest_mask = batch_array <= lower_percentile\n",
    "    largest_mask = batch_array >= upper_percentile\n",
    "\n",
    "    # Get indices for smallest and largest values\n",
    "    smallest_indices_batch = [np.where(smallest_mask[i])[0] for i in range(batch_array.shape[0])]\n",
    "    largest_indices_batch = [np.where(largest_mask[i])[0] for i in range(batch_array.shape[0])]\n",
    "\n",
    "    return smallest_indices_batch, largest_indices_batch\n",
    "\n",
    "# Assign values based on smallest and largest weights\n",
    "def assign_value_for_labels(cl_labels, batch_size, num_features, smallest_weights, largest_weights, batch_patient_labels):\n",
    "    for i in range(batch_size):\n",
    "        # Calculate the base index for this batch item\n",
    "        base_index = i * num_features\n",
    "\n",
    "        # Get indices for this specific batch item\n",
    "        smallest_indices = smallest_weights[i] + base_index\n",
    "        largest_indices = largest_weights[i] + base_index\n",
    "\n",
    "        # If the batch_patient_label for this bag is 0, assign all instances to 0\n",
    "        if batch_patient_labels[i] == 0:\n",
    "            cl_labels[base_index:base_index + num_features] = 0\n",
    "            cl_labels[base_index + batch_size * num_features:base_index + batch_size * num_features + num_features] = 0\n",
    "        else:\n",
    "            # Update final_cl_labels for smallest weights (set to 0)\n",
    "            cl_labels[smallest_indices] = 0\n",
    "            cl_labels[smallest_indices + batch_size * num_features] = 0\n",
    "\n",
    "            # Update final_cl_labels for largest weights (set to 1)\n",
    "            cl_labels[largest_indices] = 1\n",
    "            cl_labels[largest_indices + batch_size * num_features] = 1\n",
    "    return cl_labels"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Epoch End-to-end"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.212295Z",
     "iopub.status.busy": "2024-11-09T12:04:11.211419Z",
     "iopub.status.idle": "2024-11-09T12:04:11.290690Z",
     "shell.execute_reply": "2024-11-09T12:04:11.289598Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.212259Z"
    },
    "papermill": {
     "duration": 0.089765,
     "end_time": "2024-10-04T19:24:02.636485",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.546720",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.672724Z",
     "start_time": "2025-01-01T17:20:41.651198Z"
    }
   },
   "source": [
    "def train_epoch(model, likelihood, data_loader, criterion_cl, criterion_bce, mll, optimizer, variational_ngd_optimizer, scheduler, scaler, device):\n",
    "    total_loss = 0.0\n",
    "    total_nlls = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    with gpytorch.settings.num_likelihood_samples(4):\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "\n",
    "            # aug1 = augment_batch(batch_data).cuda()\n",
    "            # aug2 = augment_batch(batch_data).cuda()\n",
    "            # with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                # output, attention_weight, _, proj_head = model(torch.cat([aug1, aug2], dim=0))\n",
    "                # batch_multi_labels = torch.cat([batch_multi_labels, batch_multi_labels], dim=0)\n",
    "                #\n",
    "                # # miner_func = ExamplePairMiner()\n",
    "                # # hard_pairs = miner_func(proj_head, batch_multi_labels)\n",
    "                # # NTXLoss = criterion_cl(proj_head, batch_multi_labels, hard_pairs=None)\n",
    "                #\n",
    "                # # Scan-level Miner\n",
    "                # batch_patient_labels = torch.cat([batch_patient_labels, batch_patient_labels], dim=0)\n",
    "\n",
    "                # miner_func = ExamplePairMiner()\n",
    "                # hard_pairs = miner_func(proj_head, batch_patient_labels)\n",
    "                # NTXLoss = criterion_cl(proj_head, batch_patient_labels, hard_pairs)\n",
    "\n",
    "                # # Pseudo-Labeling / Slice-level Miner\n",
    "                # smallest_indices, largest_indices = calculate_distribution_extremes_batch(attention_weight.cpu().detach().numpy())\n",
    "                # batch_size = attention_weight.size(0) // 2\n",
    "                # num_instances = attention_weight.size(1)\n",
    "                # cl_labels = torch.cat([torch.arange(batch_size * num_instances), torch.arange(batch_size * num_instances)], dim=0).to(device)\n",
    "                # pseudo_labels = assign_value_for_labels(cl_labels.clone().cpu().numpy(), batch_size, num_instances, smallest_indices, largest_indices, batch_patient_labels)\n",
    "                # pseudo_labels = torch.tensor(pseudo_labels).to(device)\n",
    "                #\n",
    "                # miner_func = hnm.ExamplePairMiner()\n",
    "                # hard_pairs = miner_func(proj_head, pseudo_labels)\n",
    "                # NTXLoss = criterion_cl(proj_head, pseudo_labels, hard_pairs)\n",
    "\n",
    "                # # Loss in BCE\n",
    "                # loss_mod = combined_loss(output, model.gp_layer, batch_patient_labels)\n",
    "                # Loss in CE\n",
    "                # loss_mod = combined_loss(output, model.gp_layer, batch_multi_labels)\n",
    "\n",
    "            # batch_patient_labels = torch.cat([batch_patient_labels, batch_patient_labels], dim=0)\n",
    "            # output, _, _, proj_head = model(torch.cat([aug1, aug2], dim=0))\n",
    "            # NTXLoss = criterion_cl(proj_head)\n",
    "            # loss_gp = -mll(output, batch_patient_labels)\n",
    "            # loss = NTXLoss * 0.5 + loss_gp * 0.5\n",
    "\n",
    "            # output, _, _, proj_head1 = model(aug1)\n",
    "            # output2, _, _, proj_head2 = model(aug2)\n",
    "            # NTXLoss = criterion_cl(torch.cat([proj_head1, proj_head2], dim=0))\n",
    "            # loss_gp1 = -mll(output, batch_patient_labels)\n",
    "            # loss_gp2 = -mll(output2, batch_patient_labels)\n",
    "            # loss = NTXLoss * 0.4 + 0.3 * loss_gp1 + 0.3 * loss_gp2\n",
    "\n",
    "            # predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, proj_head = model(batch_data)\n",
    "                loss = -mll(output, batch_patient_labels)\n",
    "\n",
    "                loss = loss.mean()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                variational_ngd_optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "                preds = likelihood(output).probs.gt(0.5).float()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "                nlls = -likelihood.log_marginal(batch_patient_labels, output)\n",
    "                total_nlls += nlls.sum().item()\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "                loss = -mll(output, batch_patient_labels)\n",
    "\n",
    "                loss = loss.mean()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                preds = likelihood(output).probs.mean(0).argmax(-1)\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    # print(f'NLL: {total_nlls / len(labels):.4f}')\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def validate(model, likelihood, data_loader, criterion_cl, criterion_bce, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, _ = model(batch_data)\n",
    "\n",
    "                preds = likelihood(output).probs.gt(0.5).float()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "\n",
    "                preds = likelihood(output).probs.mean(0).argmax(-1)\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "            # predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate, device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(train_loader.dataset))\n",
    "    print(f'Length of train_loader: {len(train_loader.dataset)}')\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.gp_layer.variational_parameters(), num_data=len(train_loader.dataset), lr=learning_rate)\n",
    "\n",
    "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * num_epochs, 0.75 * num_epochs], gamma=0.1)\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    with gpytorch.settings.use_toeplitz(False):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            train_loss, train_predictions, train_labels = train_epoch(model, likelihood, train_loader, criterion_cl, criterion_bce, mll,optimizer, variational_ngd_optimizer,scheduler, scaler, device)\n",
    "            train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "            print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "            # Log training metrics to W&B\n",
    "            wandb.log({\n",
    "                \"train/loss\": train_loss,\n",
    "                \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "                \"train/precision\": train_metrics[\"precision\"],\n",
    "                \"train/recall\": train_metrics[\"recall\"],\n",
    "                \"train/f1\": train_metrics[\"f1\"],\n",
    "            })\n",
    "\n",
    "            # # Validation phase\n",
    "            # val_loss, val_predictions, val_labels = validate(model, likelihood, val_loader, criterion_cl, criterion_bce, device)\n",
    "            # val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "            # print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "            # # Log validation metrics to W&B\n",
    "            # wandb.log({\n",
    "            #     \"val/loss\": val_loss,\n",
    "            #     \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            #     \"val/precision\": val_metrics[\"precision\"],\n",
    "            #     \"val/recall\": val_metrics[\"recall\"],\n",
    "            #     \"val/f1\": val_metrics[\"f1\"],\n",
    "            # })\n",
    "            #\n",
    "            # # Save best model\n",
    "            # if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            #     best_val_accuracy = val_metrics['accuracy']\n",
    "            #     best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    # model.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Epoch with 2 Phases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.737227Z",
     "start_time": "2025-01-01T17:20:41.712589Z"
    }
   },
   "source": [
    "def train_epoch_phase1(model, data_loader, criterion_cl, optimizer, scheduler, scaler, device):\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        aug1 = augment_batch(batch_data).cuda()\n",
    "        aug2 = augment_batch(batch_data).cuda()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output, _ = model(torch.cat([aug1, aug2], dim=0))\n",
    "            NTXLoss = criterion_cl(output)\n",
    "\n",
    "        loss = NTXLoss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "        predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def train_epoch_phase2(model_supcon, model_classifier, data_loader, criterion_bce, optimizer, scheduler, scaler, device):\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model_supcon.eval()\n",
    "    model_classifier.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            aug_data = augment_batch(batch_data).cuda()\n",
    "            _, features = model_supcon(aug_data)\n",
    "            output = model_classifier(features)\n",
    "            loss = combined_loss(output, model_classifier.gp_layer, batch_patient_labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "        predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def valid_epoch_phase2(model_supcon, model_classifier, data_loader, criterion_bce, device):\n",
    "    model_supcon.eval()\n",
    "    model_classifier.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                _, features = model_supcon(batch_data)\n",
    "                output = model_classifier(features)\n",
    "                loss = combined_loss(output, model_classifier.gp_layer, batch_patient_labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def train_model_2_phases(model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                optimizer_supcon, optimizer_att_gp, num_epochs_phase1, num_epochs_phase2,\n",
    "                learning_rate, device='cuda', training_type='classification'):\n",
    "\n",
    "    model_supcon.to(device)\n",
    "    model_att_gp.to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    if training_type != 'classification':\n",
    "        # Phase 1: Train SupConResnet\n",
    "        scheduler_supcon = optim.lr_scheduler.OneCycleLR(optimizer_supcon,\n",
    "                                                          max_lr=learning_rate,\n",
    "                                                          steps_per_epoch=len(train_loader),\n",
    "                                                          epochs=num_epochs_phase1)\n",
    "\n",
    "        for epoch in range(num_epochs_phase1):\n",
    "            train_loss, train_predictions, train_labels = train_epoch_phase1(model_supcon,\n",
    "                                                                            train_loader,\n",
    "                                                                            criterion_cl,\n",
    "                                                                            optimizer_supcon,\n",
    "                                                                            scheduler_supcon,\n",
    "                                                                            scaler,\n",
    "                                                                            device)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs_phase1}: Loss = {train_loss}')\n",
    "\n",
    "    # Save the state of SupConResnet after training phase 1\n",
    "    supcon_model_state = model_supcon.state_dict()\n",
    "\n",
    "    # Freeze SupConResnet parameters\n",
    "    for param in model_supcon.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Phase 2: Train ATT_GP\n",
    "    scheduler_att_gp = optim.lr_scheduler.OneCycleLR(optimizer_att_gp,\n",
    "                                                      max_lr=learning_rate,\n",
    "                                                      steps_per_epoch=len(train_loader),\n",
    "                                                      epochs=num_epochs_phase2)\n",
    "\n",
    "    for epoch in range(num_epochs_phase2):\n",
    "        train_loss_att_gp, train_predictions_att_gp, train_labels_att_gp = train_epoch_phase2( model_supcon,\n",
    "                                                                                               model_att_gp,\n",
    "                                                                                               train_loader,\n",
    "                                                                                               criterion_bce,\n",
    "                                                                                               optimizer_att_gp,\n",
    "                                                                                               scheduler_att_gp,\n",
    "                                                                                               scaler,\n",
    "                                                                                               device)\n",
    "        train_metrics = calculate_metrics(train_predictions_att_gp, train_labels_att_gp)\n",
    "        print_epoch_stats(epoch, num_epochs_phase2, \"train\", train_loss_att_gp, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss_att_gp,\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = valid_epoch_phase2(model_supcon, model_att_gp, val_loader, criterion_bce, device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs_phase2, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model_att_gp.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model_supcon.load_state_dict(supcon_model_state)\n",
    "    model_att_gp.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model_supcon, model_att_gp\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016054,
     "end_time": "2024-10-04T19:24:02.668925",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.652871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.292052Z",
     "iopub.status.busy": "2024-11-09T12:04:11.291769Z",
     "iopub.status.idle": "2024-11-09T12:04:11.359152Z",
     "shell.execute_reply": "2024-11-09T12:04:11.358128Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.292017Z"
    },
    "papermill": {
     "duration": 0.079611,
     "end_time": "2024-10-04T19:24:02.764721",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.685110",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.785223Z",
     "start_time": "2025-01-01T17:20:41.766575Z"
    }
   },
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, likelihood, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, _ = model(batch_data)\n",
    "                preds = likelihood(output).probs.gt(0.5).float()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "                preds = likelihood(output).probs.mean(0).argmax(-1)\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            # predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "def evaluate_model_2(model_supcon, model_classifier,data_loader, device='cuda'):\n",
    "    model_supcon = model_supcon.to(device)\n",
    "    model_classifier = model_classifier.to(device)\n",
    "    model_supcon.eval(), model_classifier.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            projection_head, features = model_supcon(batch_data)\n",
    "            output = model_classifier(features)\n",
    "            predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01814,
     "end_time": "2024-10-04T19:24:02.800208",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.782068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.360796Z",
     "iopub.status.busy": "2024-11-09T12:04:11.360394Z",
     "iopub.status.idle": "2024-11-09T12:04:11.429794Z",
     "shell.execute_reply": "2024-11-09T12:04:11.428871Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.360761Z"
    },
    "papermill": {
     "duration": 0.092339,
     "end_time": "2024-10-04T19:24:02.911187",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.818848",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.843666Z",
     "start_time": "2025-01-01T17:20:41.825085Z"
    }
   },
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, likelihood, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, _ = model(batch_data)\n",
    "                preds = likelihood(output).probs.float()\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "                preds = likelihood(output).probs.mean(0).float()\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            # predictions.extend(output.squeeze().cpu().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1: # Binary classification\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, likelihood, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, likelihood, data_loader, device)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018543,
     "end_time": "2024-10-04T19:24:02.948830",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.930287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.431237Z",
     "iopub.status.busy": "2024-11-09T12:04:11.430912Z",
     "iopub.status.idle": "2024-11-09T12:04:11.497859Z",
     "shell.execute_reply": "2024-11-09T12:04:11.496987Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.431205Z"
    },
    "papermill": {
     "duration": 0.079014,
     "end_time": "2024-10-04T19:24:03.046531",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.967517",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.889083Z",
     "start_time": "2025-01-01T17:20:41.871411Z"
    }
   },
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path, params):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class(params)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01837,
     "end_time": "2024-10-04T19:24:03.082997",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.064627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualization Functions\n",
    "\n",
    "### Visualizing Attention Weights and Images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.499350Z",
     "iopub.status.busy": "2024-11-09T12:04:11.499019Z",
     "iopub.status.idle": "2024-11-09T12:04:11.569343Z",
     "shell.execute_reply": "2024-11-09T12:04:11.568408Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.499317Z"
    },
    "papermill": {
     "duration": 0.095451,
     "end_time": "2024-10-04T19:24:03.197071",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.101620",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.935346Z",
     "start_time": "2025-01-01T17:20:41.917282Z"
    }
   },
   "source": [
    "def plot_label_attention_weights(model, data_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Plot images with their labels and attention values in a single large plot.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model\n",
    "    - data_loader: DataLoader containing test dataset\n",
    "    - device: Device to run the model on ('cuda' or 'cpu')\n",
    "    - CHANNELS: Number of channels in the image (e.g., 1 for grayscale, 3 for RGB)\n",
    "\n",
    "    Expected shapes:\n",
    "    - 1-channel images: (batch_size, num_images, 224, 224)\n",
    "    - 3-channel images: (batch_size, num_images, 3, 224, 224)\n",
    "    - attention: float value per image indicating attention weight\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    num_images = MAX_SLICES\n",
    "    rows, cols = 10, 6  # Adjust to fit 60 images in a single plot\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patients_label, batch_multi_labels in data_loader:\n",
    "            # Move data to the appropriate device\n",
    "            batch_data = batch_data.to(device)\n",
    "            outputs, _, attention_weight_batch, _, _ = model(batch_data)\n",
    "\n",
    "            # Process each patient in the batch\n",
    "            for patient_idx in range(batch_data.size(0)):\n",
    "                if batch_patients_label[patient_idx].item() == 1:  # Check if patient has positive label\n",
    "                    # Create a new figure for this patient\n",
    "                    fig = plt.figure(figsize=(cols * 4, rows * 4 + 2))  # Increased height for suptitle\n",
    "\n",
    "                    for img_idx in range(num_images):\n",
    "                        # Get the image and its label\n",
    "                        img = batch_data[patient_idx, img_idx].cpu().numpy()\n",
    "                        img_label = batch_labels[patient_idx, img_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Get attention value\n",
    "                        if attention_weight_batch.size(1) == batch_data.size(1):\n",
    "                            attention_value = attention_weight_batch[patient_idx, img_idx].cpu().item()\n",
    "                        else:\n",
    "                            attention_value = attention_weight_batch[patient_idx].cpu().item()\n",
    "                        \n",
    "                        # Plot image\n",
    "                        plt.subplot(rows, cols, img_idx + 1)\n",
    "                        if CHANNELS == 3:  # RGB image\n",
    "                            plt.imshow(img)\n",
    "                        else:  # Grayscale image\n",
    "                            if img.ndim == 3:  # If shape is (1, H, W)\n",
    "                                img = np.squeeze(img)  # Convert to (H, W)\n",
    "                            plt.imshow(img, cmap='gray')\n",
    "                        \n",
    "                        plt.title(f'Label: {img_label}\\nAttention: {attention_value:.4f}', fontsize=12)\n",
    "                        plt.axis('off')\n",
    "\n",
    "                    # Add overall title for the patient\n",
    "                    plt.suptitle(f'Patient Images (Patient Label: {batch_patients_label[patient_idx].cpu().numpy()})', fontsize=16)\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust rect to make space for suptitle\n",
    "                    plt.show()\n",
    "                                      \n",
    "                    # Since we are plotting only for one patient, return after the first plot\n",
    "                    return"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Augmented Bags"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:41.980883Z",
     "start_time": "2025-01-01T17:20:41.963170Z"
    }
   },
   "source": [
    "def visualize_augmented_bags(original_bags, augmented_bags, num_bags=12):\n",
    "    \"\"\"\n",
    "    Visualizes all instances of the first bag of original and augmented images.\n",
    "\n",
    "    Parameters:\n",
    "    - original_bags: A tensor of shape (batch_size, num_instances, channels, height, width)\n",
    "    - augmented_bags: A tensor of shape (batch_size, num_instances, channels, height, width)\n",
    "    - num_bags: Number of bags to visualize (only the first bag will be shown).\n",
    "    \"\"\"\n",
    "    # Only visualize the first bag\n",
    "    first_bag_index = 0\n",
    "    \n",
    "    # Get number of instances\n",
    "    num_instances = original_bags.size(1)\n",
    "    \n",
    "    print(f'Num instances: {num_instances}')\n",
    "    \n",
    "    # Limit the number of bags to visualize (but we only show the first one)\n",
    "    num_bags = min(num_bags, 1)  # We only want to visualize the first bag\n",
    "\n",
    "    fig, axes = plt.subplots(num_instances, 2, figsize=(12, 2 * num_instances))\n",
    "    \n",
    "    # Original images\n",
    "    for j in range(num_instances):  # Iterate over instances in the first bag\n",
    "        img = original_bags[first_bag_index][j].cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        axes[j, 0].imshow(img, cmap='gray')  # Use gray colormap for single channel images\n",
    "        axes[j, 0].axis('off')  # Hide axes for better visualization\n",
    "        axes[j, 0].set_title(f'Original Instance {j + 1}')\n",
    "        \n",
    "    # Augmented images\n",
    "    for j in range(num_instances):\n",
    "        img = augmented_bags[first_bag_index][j].cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        axes[j, 1].imshow(img.squeeze(), cmap='gray')  # Use gray colormap for single channel images\n",
    "        axes[j, 1].axis('off')  # Hide axes for better visualization\n",
    "        axes[j, 1].set_title(f'Augmented Instance {j + 1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016248,
     "end_time": "2024-10-04T19:24:03.231225",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.214977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.635991Z",
     "iopub.status.busy": "2024-11-09T12:04:11.635549Z",
     "iopub.status.idle": "2024-11-09T12:18:52.781211Z",
     "shell.execute_reply": "2024-11-09T12:18:52.780156Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.635946Z"
    },
    "papermill": {
     "duration": 24983.553969,
     "end_time": "2024-10-05T02:20:26.801619",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.247650",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-01T17:20:42.039580Z",
     "start_time": "2025-01-01T17:20:42.019421Z"
    }
   },
   "source": [
    "def main(mode='train'):\n",
    "    # os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\")\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=0.0)\n",
    "    test_labels = pd.read_csv('./data_analyze/testing_dataset_150.csv')\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    params = {\n",
    "        'channels': CHANNELS,  # Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
    "        'num_classes': NUM_CLASSES,  # Number of output classes for classification\n",
    "        'drop_prob': 0.25,  # Dropout probability\n",
    "        'inducing_points': INDUCING_POINTS,  # Number of inducing points for the Gaussian Process layer\n",
    "        'projection_location': PROJECTION_LOCATION,  # Choose from 'after_resnet', 'after_attention', or 'after_gp'\n",
    "        'projection_hidden_dim': PROJECTION_HIDDEN_DIM,  # Hidden dimension size for the projection head\n",
    "        'projection_output_dim': PROJECTION_OUTPUT_DIM,  # Output dimension size for the projection head\n",
    "        'attention_hidden_dim': ATTENTION_HIDDEN_DIM,  # Hidden dimension size for the attention head\n",
    "    }\n",
    "\n",
    "    if TRAINING_TYPE == 'end_to_end':\n",
    "        # Instantiate the CNN_GP_ATT model with the specified parameters\n",
    "        # model = CNN_GP_ATT(params=params)\n",
    "        model = CNN_ATT_GP(params=params)\n",
    "        likelihood = PGLikelihood().cuda()\n",
    "        # optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "            {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "        ])\n",
    "    elif TRAINING_TYPE == 'dkl':\n",
    "        model = DKLModel(params=params)\n",
    "        config.learning_rate = 0.1\n",
    "        likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, num_classes=NUM_CLASSES + 1).cuda()\n",
    "\n",
    "        optimizer = torch.optim.SGD([\n",
    "            {'params': model.feature_extractor.parameters()},\n",
    "            {'params': model.gp_layer.hyperparameters(), 'lr': config.learning_rate * 0.01},\n",
    "            {'params': model.gp_layer.variational_parameters()},\n",
    "            {'params': likelihood.parameters()},\n",
    "        ], lr=config.learning_rate, momentum=0.9, nesterov=True, weight_decay=0)\n",
    "    else:\n",
    "        model_supcon = SupConResnet(params=params)\n",
    "        model_att_gp = LinearClassifier(params=params)\n",
    "\n",
    "        optimizer_phase1 = optim.Adam(model_supcon.parameters(), lr=config.learning_rate)\n",
    "        optimizer_phase2 = optim.RMSprop(model_att_gp.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    criterion_cl = NTXentLoss(0.5)\n",
    "    criterion_bce = torch.nn.BCELoss()\n",
    "    criterion_bce_wll = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if mode == 'train':\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            wandb.watch(model) # Watch the model to log gradients and parameters\n",
    "            trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "            predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "            metrics = calculate_metrics(predictions, labels)\n",
    "            wandb.log(metrics)\n",
    "            print_metrics(metrics)\n",
    "\n",
    "            plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "            plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "            torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "        elif TRAINING_TYPE == 'dkl':\n",
    "            wandb.watch(model)\n",
    "            trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "            predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "            metrics = calculate_metrics(predictions, labels)\n",
    "            wandb.log(metrics)\n",
    "            print_metrics(metrics)\n",
    "\n",
    "            plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "            plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "            torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "        elif TRAINING_TYPE == 'only_phase2':\n",
    "            trained_model_supcon = load_model(SupConResnet, 'results/trained_supcon.pth', params)\n",
    "            _, trained_model_att_gp = train_model_2_phases(trained_model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce_wll, optimizer_phase1, optimizer_phase2, 20, 20, config.learning_rate, DEVICE, 'classification')\n",
    "            torch.save(trained_model_att_gp.state_dict(), 'results/trained_att_gp.pth')\n",
    "        else:\n",
    "            wandb.watch(model_supcon)\n",
    "            trained_model_supcon, trained_model_att_gp = train_model_2_phases(model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce_wll, optimizer_phase1, optimizer_phase2, 20, 20, config.learning_rate, DEVICE)\n",
    "            torch.save(trained_model_supcon.state_dict(), 'results/trained_supcon.pth')\n",
    "            torch.save(trained_model_att_gp.state_dict(), 'results/trained_att_gp.pth')\n",
    "\n",
    "    if TRAINING_TYPE == 'end_to_end':\n",
    "        trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "        predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "        # plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "        # plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "    else:\n",
    "        trained_model_supcon = load_model(SupConResnet, 'results/trained_supcon.pth', params)\n",
    "        trained_model_att_gp = load_model(LinearClassifier, 'results/trained_att_gp.pth', params)\n",
    "        predictions, labels = evaluate_model_2(trained_model_supcon, trained_model_att_gp, test_loader, DEVICE)\n",
    "\n",
    "\n",
    "    metrics = calculate_metrics(predictions, labels)\n",
    "    wandb.log(metrics)\n",
    "    print_metrics(metrics)\n",
    "\n",
    "    # if mode == 'train':\n",
    "    #     required_columns = ['patient_id', 'study_instance_uid', 'patient_label']\n",
    "    #     temp_test_labels = test_labels[required_columns]\n",
    "    #\n",
    "    #     # Save results\n",
    "    #     results_df = get_test_results(trained_model, test_loader, temp_test_labels, device)\n",
    "    #     results_df.to_csv('results/results.csv', index=False)\n",
    "    #     print(results_df.head())\n",
    "    #\n",
    "    #     # Log results DataFrame as a table in W&B (optional)\n",
    "    #     wandb.log({\"results\": wandb.Table(dataframe=results_df)})\n",
    "    \n",
    "    # Get the first batch of images from the evaluation loader\n",
    "    images, _, _, _ = next(iter(train_loader))\n",
    "    print(f'Original batch shape: {images.shape}')\n",
    "    \n",
    "    # Augment the batch of images\n",
    "    start = time.time()\n",
    "    augmented_images = augment_batch(images)\n",
    "    end = time.time()\n",
    "    taken_time = end - start\n",
    "    print(f'Augmented batch shape: {augmented_images.shape} | Time: {taken_time:.4f}')\n",
    "    \n",
    "    # Visualize the original and augmented bags\n",
    "    visualize_augmented_bags(images, augmented_images)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-01T17:20:42.065190Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mhuynhsikha2003\u001B[0m (\u001B[33mhuynhsikha2003-i-h-c-qu-c-gia-tp-hcm\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/media/hskha23/Kha/Brain-Stroke-Diagnosis/rsna/wandb/run-20250102_002043-32117z46</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/32117z46' target=\"_blank\">feasible-universe-195</a></strong> to <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/32117z46' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/32117z46</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_loader: 750\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5705276,
     "sourceId": 9652074,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25022.389189,
   "end_time": "2024-10-05T02:20:29.848670",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-04T19:23:27.459481",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
