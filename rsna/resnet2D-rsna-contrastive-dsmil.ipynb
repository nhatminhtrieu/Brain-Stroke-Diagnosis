{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015487,
     "end_time": "2024-10-04T19:23:30.290610",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.275123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:03:29.962697Z",
     "iopub.status.busy": "2024-11-09T12:03:29.962323Z",
     "iopub.status.idle": "2024-11-09T12:03:54.971291Z",
     "shell.execute_reply": "2024-11-09T12:03:54.970161Z",
     "shell.execute_reply.started": "2024-11-09T12:03:29.962659Z"
    },
    "papermill": {
     "duration": 13.842163,
     "end_time": "2024-10-04T19:23:44.147775",
     "exception": false,
     "start_time": "2024-10-04T19:23:30.305612",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:55.600318Z",
     "start_time": "2025-01-15T04:52:54.170006Z"
    }
   },
   "source": [
    "!pip install gpytorch\n",
    "!pip install wandb"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n",
      "Requirement already satisfied: wandb in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (0.18.5)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.21.12)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.17.0)\r\n",
      "Requirement already satisfied: setproctitle in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (72.1.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:03:54.973660Z",
     "iopub.status.busy": "2024-11-09T12:03:54.973321Z",
     "iopub.status.idle": "2024-11-09T12:04:04.345572Z",
     "shell.execute_reply": "2024-11-09T12:04:04.344540Z",
     "shell.execute_reply.started": "2024-11-09T12:03:54.973624Z"
    },
    "papermill": {
     "duration": 11.793387,
     "end_time": "2024-10-04T19:23:55.956283",
     "exception": false,
     "start_time": "2024-10-04T19:23:44.162896",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.240272Z",
     "start_time": "2025-01-15T04:52:55.603343Z"
    }
   },
   "source": [
    "import optuna\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_metric_learning import losses\n",
    "from torch.cpu.amp import GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from models.mil_resnet import CNN_ATT_GP, CNN_GP_ATT, SupConResnet, LinearClassifier, DKLModel\n",
    "from utils import hard_negative_mining as hnm\n",
    "import gpytorch\n",
    "from layers.gaussian_process import SingletaskGPModel, PGLikelihood\n",
    "from utils.early_stopping import EarlyStoppingForOptimization, EarlyStopping\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.347247Z",
     "iopub.status.busy": "2024-11-09T12:04:04.346735Z",
     "iopub.status.idle": "2024-11-09T12:04:04.352304Z",
     "shell.execute_reply": "2024-11-09T12:04:04.351162Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.347210Z"
    },
    "papermill": {
     "duration": 0.02447,
     "end_time": "2024-10-04T19:23:55.997833",
     "exception": false,
     "start_time": "2024-10-04T19:23:55.973363",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.265456Z",
     "start_time": "2025-01-15T04:52:57.263471Z"
    }
   },
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024749,
     "end_time": "2024-10-04T19:23:56.038412",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.013663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations\n",
    "\n",
    "### GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.354684Z",
     "iopub.status.busy": "2024-11-09T12:04:04.354377Z",
     "iopub.status.idle": "2024-11-09T12:04:04.410802Z",
     "shell.execute_reply": "2024-11-09T12:04:04.409843Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.354653Z"
    },
    "papermill": {
     "duration": 0.105269,
     "end_time": "2024-10-04T19:23:56.159555",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.054286",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.335640Z",
     "start_time": "2025-01-15T04:52:57.313933Z"
    }
   },
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.412279Z",
     "iopub.status.busy": "2024-11-09T12:04:04.411954Z",
     "iopub.status.idle": "2024-11-09T12:04:04.479489Z",
     "shell.execute_reply": "2024-11-09T12:04:04.478787Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.412244Z"
    },
    "papermill": {
     "duration": 0.081958,
     "end_time": "2024-10-04T19:23:56.258405",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.176447",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.392796Z",
     "start_time": "2025-01-15T04:52:57.365567Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed Everything"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.433580Z",
     "start_time": "2025-01-15T04:52:57.414457Z"
    }
   },
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016234,
     "end_time": "2024-10-04T19:23:56.291671",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.275437",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Constant Info"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.480805Z",
     "iopub.status.busy": "2024-11-09T12:04:04.480520Z",
     "iopub.status.idle": "2024-11-09T12:04:04.541310Z",
     "shell.execute_reply": "2024-11-09T12:04:04.540405Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.480773Z"
    },
    "papermill": {
     "duration": 0.076674,
     "end_time": "2024-10-04T19:23:56.384583",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.307909",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.488267Z",
     "start_time": "2025-01-15T04:52:57.462767Z"
    }
   },
   "source": [
    "import yaml\n",
    "with open(\"../config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "GP_MODEL = config['gp_model']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']\n",
    "\n",
    "FEATURE_EXTRACTOR = config['feature_extractor']"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.542646Z",
     "iopub.status.busy": "2024-11-09T12:04:04.542362Z",
     "iopub.status.idle": "2024-11-09T12:04:04.649755Z",
     "shell.execute_reply": "2024-11-09T12:04:04.648985Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.542615Z"
    },
    "papermill": {
     "duration": 4.085647,
     "end_time": "2024-10-04T19:24:00.487263",
     "exception": false,
     "start_time": "2024-10-04T19:23:56.401616",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.550783Z",
     "start_time": "2025-01-15T04:52:57.518108Z"
    }
   },
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "ROOT_DIR = '/kaggle/input/rsna-mil-training/' if KAGGLE else None\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "DICOM_DIR = DATA_DIR\n",
    "# CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_1150_redundancy.csv'\n",
    "CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_1_redundancy.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "\n",
    "# Load patient scan labels\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:04.651199Z",
     "iopub.status.busy": "2024-11-09T12:04:04.650858Z",
     "iopub.status.idle": "2024-11-09T12:04:06.250597Z",
     "shell.execute_reply": "2024-11-09T12:04:06.249864Z",
     "shell.execute_reply.started": "2024-11-09T12:04:04.651166Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.578200Z",
     "start_time": "2025-01-15T04:52:57.562156Z"
    }
   },
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret(\"Wandb key\")\n",
    "# \n",
    "# wandb.login(key=key, relogin=True)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.252251Z",
     "iopub.status.busy": "2024-11-09T12:04:06.251742Z",
     "iopub.status.idle": "2024-11-09T12:04:06.331892Z",
     "shell.execute_reply": "2024-11-09T12:04:06.331086Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.252216Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.627887Z",
     "start_time": "2025-01-15T04:52:57.606044Z"
    }
   },
   "source": [
    "patient_scan_labels.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...   \n",
       "1  ['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...   \n",
       "2  ['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...   \n",
       "3  ['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...   \n",
       "4  ['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0002cd41   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00054f3f   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0006d192   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00086119   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_000e5623   \n",
       "\n",
       "  study_instance_uid  ...                                      pixel_spacing  \\\n",
       "0      ID_66929e09d4  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "1      ID_8a449ae31b  ...  ['[0.435546875, 0.435546875]', '[0.435546875, ...   \n",
       "2      ID_25690b4725  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "3      ID_fdde2979b0  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "4      ID_9a4be35b9a  ...  ['[0.44921875, 0.44921875]', '[0.44921875, 0.4...   \n",
       "\n",
       "                                pixel_representation  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       window_center  \\\n",
       "0  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "1  ['[00036, 00036]', '[00036, 00036]', '[00036, ...   \n",
       "2  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "3  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "4  ['[00036, 00036]', '[00036, 00036]', '[00036, ...   \n",
       "\n",
       "                                        window_width  \\\n",
       "0  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "1  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "2  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "3  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "4  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "\n",
       "                                   rescale_intercept  \\\n",
       "0  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "1  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "2  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "3  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "4  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "\n",
       "                                       rescale_slope patient_label  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "\n",
       "                                              z_axis  \\\n",
       "0  [38.484, 43.517, 48.549, 53.582, 58.614, 63.64...   \n",
       "1  [71.9000244, 76.9000244, 81.9000244, 86.900024...   \n",
       "2  [41.921, 49.421, 56.921, 64.421, 71.921, 79.42...   \n",
       "3  [35.556, 40.757, 45.959, 51.16, 56.362, 61.563...   \n",
       "4  [272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...   \n",
       "\n",
       "                                     slice_thickness  \\\n",
       "0  [2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...   \n",
       "1  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "2  [3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...   \n",
       "3  [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                    selected_indices  \n",
       "0  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...  \n",
       "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "2  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...  \n",
       "3  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...  \n",
       "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_spacing</th>\n",
       "      <th>pixel_representation</th>\n",
       "      <th>window_center</th>\n",
       "      <th>window_width</th>\n",
       "      <th>rescale_intercept</th>\n",
       "      <th>rescale_slope</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>selected_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0002cd41</td>\n",
       "      <td>ID_66929e09d4</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[38.484, 43.517, 48.549, 53.582, 58.614, 63.64...</td>\n",
       "      <td>[2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00054f3f</td>\n",
       "      <td>ID_8a449ae31b</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.435546875, 0.435546875]', '[0.435546875, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['[00036, 00036]', '[00036, 00036]', '[00036, ...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[71.9000244, 76.9000244, 81.9000244, 86.900024...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0006d192</td>\n",
       "      <td>ID_25690b4725</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[41.921, 49.421, 56.921, 64.421, 71.921, 79.42...</td>\n",
       "      <td>[3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00086119</td>\n",
       "      <td>ID_fdde2979b0</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.556, 40.757, 45.959, 51.16, 56.362, 61.563...</td>\n",
       "      <td>[2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_000e5623</td>\n",
       "      <td>ID_9a4be35b9a</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.44921875, 0.44921875]', '[0.44921875, 0.4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['[00036, 00036]', '[00036, 00036]', '[00036, ...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015844,
     "end_time": "2024-10-04T19:24:00.868235",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.852391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.538200Z",
     "iopub.status.busy": "2024-11-09T12:04:06.537885Z",
     "iopub.status.idle": "2024-11-09T12:04:06.603427Z",
     "shell.execute_reply": "2024-11-09T12:04:06.602545Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.538168Z"
    },
    "papermill": {
     "duration": 0.075616,
     "end_time": "2024-10-04T19:24:00.959889",
     "exception": false,
     "start_time": "2024-10-04T19:24:00.884273",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.670908Z",
     "start_time": "2025-01-15T04:52:57.654028Z"
    }
   },
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016194,
     "end_time": "2024-10-04T19:24:01.118549",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.102355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.742670Z",
     "iopub.status.busy": "2024-11-09T12:04:06.742336Z",
     "iopub.status.idle": "2024-11-09T12:04:06.817331Z",
     "shell.execute_reply": "2024-11-09T12:04:06.816387Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.742638Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.718987Z",
     "start_time": "2025-01-15T04:52:57.699342Z"
    }
   },
   "source": [
    "class DatasetAugmentor:\n",
    "    def __init__(self, height, width, levels=2, seed=None):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.levels = levels  # Dynamic number of levels\n",
    "        self.seed = seed\n",
    "        self.params = []\n",
    "\n",
    "        # Create different levels of transforms based on the number of levels specified\n",
    "        for i in range(levels):\n",
    "            factor = (i + 1) / levels\n",
    "            self.params.append(\n",
    "                self._create_transform(\n",
    "                    degrees=int(15 * factor), \n",
    "                    translate_range=(0.2 * factor, 0.2 * factor),\n",
    "                    scale_range=(1 - 0.2 * factor, 1 + 0.2 * factor),\n",
    "                    brightness_range=0.2 * factor,\n",
    "                    contrast_range=0.2 * factor,\n",
    "                    blur_sigma_range=(0.5 * factor, 1.0 * factor),\n",
    "                    apply_elastic=(i >= levels // 2),\n",
    "                    level_name=f'level_{i + 1}'\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _sample_value(self, value_range):\n",
    "        if isinstance(value_range, tuple):\n",
    "            random.seed(self.seed)\n",
    "            return random.uniform(value_range[0], value_range[1])\n",
    "        return value_range\n",
    "\n",
    "    def _create_transform(self, degrees, translate_range, scale_range, brightness_range, contrast_range, blur_sigma_range, apply_elastic, level_name):\n",
    "        print(f\"Creating '{level_name}' transform with parameters:\")\n",
    "        sampled_values = {\n",
    "            \"degrees\": abs(self._sample_value((-degrees, degrees))),\n",
    "            \"translate\": (abs(self._sample_value(translate_range[0])), abs(self._sample_value(translate_range[1]))),\n",
    "            \"scale\": self._sample_value(scale_range),\n",
    "            \"brightness\": self._sample_value(brightness_range),\n",
    "            \"contrast\": self._sample_value(contrast_range),\n",
    "            \"blur_sigma\": self._sample_value(blur_sigma_range),\n",
    "            \"apply_elastic\": apply_elastic\n",
    "        }\n",
    "        \n",
    "        print(sampled_values)\n",
    "        return sampled_values\n",
    "\n",
    "    def apply_transform(self, image, level):\n",
    "        params = self.params[level]\n",
    "        transform = self._get_transform(params, channels=image.shape[0])\n",
    "        return transform(image)\n",
    "\n",
    "    def _get_transform(self, params, channels=3):\n",
    "        transform_list = [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomAffine(degrees=params[\"degrees\"], translate=params[\"translate\"], scale=(params[\"scale\"], params[\"scale\"])),\n",
    "            transforms.ColorJitter(brightness=params[\"brightness\"], contrast=params[\"contrast\"]),\n",
    "            transforms.GaussianBlur(kernel_size=(3, 3), sigma=params[\"blur_sigma\"]),\n",
    "            transforms.RandomApply([transforms.ElasticTransform()] if params[\"apply_elastic\"] else [], p=0.3),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(self.height),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "\n",
    "        if channels == 3:\n",
    "            transform_list.extend([\n",
    "                transforms.Normalize(mean=[0.16774411, 0.1360026, 0.19076315], std=[0.3101935, 0.27605791, 0.30469988]),\n",
    "                transforms.RandomApply([self._channel_shuffle], p=0.3)\n",
    "            ])\n",
    "        elif channels == 1:\n",
    "            transform_list.append(transforms.Normalize(mean=[0.16774411], std=[0.3101935]))\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "\n",
    "    def _channel_shuffle(self, tensor):\n",
    "        torch.manual_seed(self.seed)\n",
    "        channels = tensor.shape[0]\n",
    "        indices = torch.randperm(channels)\n",
    "        return tensor[indices]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:06.818878Z",
     "iopub.status.busy": "2024-11-09T12:04:06.818519Z",
     "iopub.status.idle": "2024-11-09T12:04:06.881845Z",
     "shell.execute_reply": "2024-11-09T12:04:06.881035Z",
     "shell.execute_reply.started": "2024-11-09T12:04:06.818836Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.770194Z",
     "start_time": "2025-01-15T04:52:57.752989Z"
    }
   },
   "source": [
    "augmentor = DatasetAugmentor(224, 224, seed=42)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'level_1' transform with parameters:\n",
      "{'degrees': 1.9519751784103718, 'translate': (0.1, 0.1), 'scale': 1.027885359691577, 'brightness': 0.1, 'contrast': 0.1, 'blur_sigma': 0.4098566996144709, 'apply_elastic': False}\n",
      "Creating 'level_2' transform with parameters:\n",
      "{'degrees': 4.182803953736514, 'translate': (0.2, 0.2), 'scale': 1.0557707193831534, 'brightness': 0.2, 'contrast': 0.2, 'blur_sigma': 0.8197133992289418, 'apply_elastic': True}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015494,
     "end_time": "2024-10-04T19:24:01.252123",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.236629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.880283Z",
     "start_time": "2025-01-15T04:52:57.802426Z"
    }
   },
   "source": [
    "from dataset_generators.RSNA_Dataset import MedicalScanDataset"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.022528Z",
     "iopub.status.busy": "2024-11-09T12:04:07.022171Z",
     "iopub.status.idle": "2024-11-09T12:04:07.085840Z",
     "shell.execute_reply": "2024-11-09T12:04:07.084942Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.022481Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:57.900029Z",
     "start_time": "2025-01-15T04:52:57.883057Z"
    }
   },
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.087252Z",
     "iopub.status.busy": "2024-11-09T12:04:07.086910Z",
     "iopub.status.idle": "2024-11-09T12:04:07.255534Z",
     "shell.execute_reply": "2024-11-09T12:04:07.254706Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.087214Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.071583Z",
     "start_time": "2025-01-15T04:52:57.928808Z"
    }
   },
   "source": [
    "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.256977Z",
     "iopub.status.busy": "2024-11-09T12:04:07.256674Z",
     "iopub.status.idle": "2024-11-09T12:04:07.320972Z",
     "shell.execute_reply": "2024-11-09T12:04:07.320056Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.256945Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.091107Z",
     "start_time": "2025-01-15T04:52:58.074474Z"
    }
   },
   "source": [
    "len(original_dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:07.322491Z",
     "iopub.status.busy": "2024-11-09T12:04:07.322156Z",
     "iopub.status.idle": "2024-11-09T12:04:08.712840Z",
     "shell.execute_reply": "2024-11-09T12:04:08.711911Z",
     "shell.execute_reply.started": "2024-11-09T12:04:07.322458Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.213560Z",
     "start_time": "2025-01-15T04:52:58.124395Z"
    }
   },
   "source": [
    "x,y,z, _ = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 1, 224, 224]) torch.Size([28]) torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:08.714306Z",
     "iopub.status.busy": "2024-11-09T12:04:08.713992Z",
     "iopub.status.idle": "2024-11-09T12:04:10.720210Z",
     "shell.execute_reply": "2024-11-09T12:04:10.719091Z",
     "shell.execute_reply.started": "2024-11-09T12:04:08.714273Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.236753Z",
     "start_time": "2025-01-15T04:52:58.217215Z"
    }
   },
   "source": [
    "# # Check if the returned data is valid\n",
    "# if x is not None:\n",
    "#     # Convert the tensor to a numpy array\n",
    "#     x_np = x.numpy()\n",
    "#\n",
    "#     # Check the number of dimensions and squeeze if necessary\n",
    "#     if x_np.ndim == 4:  # RGB images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i].transpose(1, 2, 0))  # Convert CHW to HWC\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     elif x_np.ndim == 3:  # Grayscale images\n",
    "#         # Plot each slice\n",
    "#         fig, axes = plt.subplots(1, x_np.shape[0], figsize=(15, 5))\n",
    "#         for i, ax in enumerate(axes):\n",
    "#             ax.imshow(x_np[i], cmap='gray')\n",
    "#             ax.axis('off')\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unexpected number of dimensions: {x_np.ndim}\")\n",
    "# else:\n",
    "#     print(\"No data available for this patient.\")\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:10.721855Z",
     "iopub.status.busy": "2024-11-09T12:04:10.721483Z",
     "iopub.status.idle": "2024-11-09T12:04:10.790322Z",
     "shell.execute_reply": "2024-11-09T12:04:10.789279Z",
     "shell.execute_reply.started": "2024-11-09T12:04:10.721821Z"
    },
    "papermill": {
     "duration": 0.074769,
     "end_time": "2024-10-04T19:24:01.438666",
     "exception": false,
     "start_time": "2024-10-04T19:24:01.363897",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.293197Z",
     "start_time": "2025-01-15T04:52:58.262380Z"
    }
   },
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTXent Loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.330418Z",
     "start_time": "2025-01-15T04:52:58.310231Z"
    }
   },
   "source": [
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation For Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.384104Z",
     "start_time": "2025-01-15T04:52:58.358433Z"
    }
   },
   "source": [
    "# Version 2: Avg time taken: 0.05 seconds for 1 augmentation (w ResizedCrop)\n",
    "def augment_batch(batch_images):\n",
    "    if CHANNELS == 1:\n",
    "        batch_size, num_instances, channels, height, width = batch_images.shape\n",
    "    else:\n",
    "        batch_size, num_instances, height, width, channels = batch_images.shape\n",
    "\n",
    "    # Define augmentation transformations using GPU-compatible operations\n",
    "    aug_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((height, width), scale=(0.8, 1.1)),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4)], p=0.6),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]),\n",
    "    ])\n",
    "\n",
    "    # Apply transformations directly on the tensor without converting to PIL\n",
    "    augmented_batch = torch.empty_like(batch_images)  # Preallocate memory for augmented images\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_instances):\n",
    "            if CHANNELS == 1:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j])\n",
    "            else:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j].permute(2, 0, 1)).permute(1, 2, 0)\n",
    "\n",
    "    return augmented_batch.cuda()  # Move the augmented batch to GPU"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015973,
     "end_time": "2024-10-04T19:24:02.496444",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.480471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.424497Z",
     "start_time": "2025-01-15T04:52:58.403775Z"
    }
   },
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.145137Z",
     "iopub.status.busy": "2024-11-09T12:04:11.144744Z",
     "iopub.status.idle": "2024-11-09T12:04:11.209993Z",
     "shell.execute_reply": "2024-11-09T12:04:11.209156Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.145066Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.474270Z",
     "start_time": "2025-01-15T04:52:58.452688Z"
    }
   },
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    if CHANNELS == 1:\n",
    "        bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    else:\n",
    "        ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        ce_loss = ce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * kl_divergence\n",
    "\n",
    "    return total_loss"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.525033Z",
     "start_time": "2025-01-15T04:52:58.500493Z"
    }
   },
   "source": [
    "def calculate_distribution_extremes_batch(batch_array):\n",
    "    # Calculate percentiles directly using NumPy\n",
    "    lower_percentile = np.percentile(batch_array, 10, axis=1, keepdims=True)\n",
    "    upper_percentile = np.percentile(batch_array, 90, axis=1, keepdims=True)\n",
    "\n",
    "    # Get boolean masks for smallest and largest values\n",
    "    smallest_mask = batch_array <= lower_percentile\n",
    "    largest_mask = batch_array >= upper_percentile\n",
    "\n",
    "    # Get indices for smallest and largest values\n",
    "    smallest_indices_batch = [np.where(smallest_mask[i])[0] for i in range(batch_array.shape[0])]\n",
    "    largest_indices_batch = [np.where(largest_mask[i])[0] for i in range(batch_array.shape[0])]\n",
    "\n",
    "    return smallest_indices_batch, largest_indices_batch\n",
    "\n",
    "# Assign values based on smallest and largest weights\n",
    "def assign_value_for_labels(cl_labels, batch_size, num_features, smallest_weights, largest_weights, batch_patient_labels):\n",
    "    for i in range(batch_size):\n",
    "        # Calculate the base index for this batch item\n",
    "        base_index = i * num_features\n",
    "\n",
    "        # Get indices for this specific batch item\n",
    "        smallest_indices = smallest_weights[i] + base_index\n",
    "        largest_indices = largest_weights[i] + base_index\n",
    "\n",
    "        # If the batch_patient_label for this bag is 0, assign all instances to 0\n",
    "        if batch_patient_labels[i] == 0:\n",
    "            cl_labels[base_index:base_index + num_features] = 0\n",
    "            cl_labels[base_index + batch_size * num_features:base_index + batch_size * num_features + num_features] = 0\n",
    "        else:\n",
    "            # Update final_cl_labels for smallest weights (set to 0)\n",
    "            cl_labels[smallest_indices] = 0\n",
    "            cl_labels[smallest_indices + batch_size * num_features] = 0\n",
    "\n",
    "            # Update final_cl_labels for largest weights (set to 1)\n",
    "            cl_labels[largest_indices] = 1\n",
    "            cl_labels[largest_indices + batch_size * num_features] = 1\n",
    "    return cl_labels"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Epoch End-to-end"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.212295Z",
     "iopub.status.busy": "2024-11-09T12:04:11.211419Z",
     "iopub.status.idle": "2024-11-09T12:04:11.290690Z",
     "shell.execute_reply": "2024-11-09T12:04:11.289598Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.212259Z"
    },
    "papermill": {
     "duration": 0.089765,
     "end_time": "2024-10-04T19:24:02.636485",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.546720",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.580572Z",
     "start_time": "2025-01-15T04:52:58.550540Z"
    }
   },
   "source": [
    "def train_epoch(model, likelihood, data_loader, criterion_cl, criterion_bce, mll, optimizer, variational_ngd_optimizer, scheduler, scaler, device):\n",
    "    total_loss = 0.0\n",
    "    total_nlls = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    with gpytorch.settings.num_likelihood_samples(4):\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, proj_head = model(batch_data)\n",
    "                loss = -mll(output, batch_patient_labels)\n",
    "\n",
    "                loss = loss.mean()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                variational_ngd_optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                preds = likelihood(output).probs.gt(0.5).float()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "                nlls = -likelihood.log_marginal(batch_patient_labels, output)\n",
    "                total_nlls += nlls.sum().item()\n",
    "\n",
    "                # aug1 = augment_batch(batch_data).cuda()\n",
    "                # aug2 = augment_batch(batch_data).cuda()\n",
    "                #\n",
    "                # output, _, _, proj_head = model(torch.cat([aug1, aug2], dim=0))\n",
    "                # batch_patient_labels = torch.cat([batch_patient_labels, batch_patient_labels], dim=0)\n",
    "                #\n",
    "                # NTXLoss = criterion_cl(proj_head, batch_patient_labels)\n",
    "                # loss_gp = -mll(output, batch_patient_labels)\n",
    "                # loss = NTXLoss * 0.5 + loss_gp * 0.5\n",
    "                #\n",
    "                # loss = loss.mean()\n",
    "                # total_loss += loss.item()\n",
    "                #\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "                # variational_ngd_optimizer.step()\n",
    "                # scheduler.step()\n",
    "                #\n",
    "                # preds = likelihood(output).probs.gt(0.5).float()\n",
    "                # predictions.extend(preds.cpu().detach().numpy())\n",
    "                #\n",
    "                # nlls = -likelihood.log_marginal(batch_patient_labels, output)\n",
    "                # total_nlls += nlls.sum().item()\n",
    "\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "                loss = -mll(output, batch_patient_labels)\n",
    "\n",
    "                loss = loss.mean()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                preds = likelihood(output).probs.mean(0).argmax(-1)\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    # print(f'NLL: {total_nlls / len(labels):.4f}')\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def validate(model, likelihood, data_loader, criterion_cl, criterion_bce, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, _ = model(batch_data)\n",
    "\n",
    "                preds = likelihood(output).probs.gt(0.5).float()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "\n",
    "                preds = likelihood(output).probs.mean(0).argmax(-1)\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "            # predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate, device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(train_loader.dataset))\n",
    "    print(f'Length of train_loader: {len(train_loader.dataset)}')\n",
    "\n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.gp_layer.variational_parameters(), num_data=len(train_loader.dataset), lr=learning_rate)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    best_likelihood_state = None\n",
    "\n",
    "    # Initialize Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "    with gpytorch.settings.use_toeplitz(False):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            train_loss, train_predictions, train_labels = train_epoch(model, likelihood, train_loader, criterion_cl, criterion_bce, mll, optimizer, variational_ngd_optimizer, scheduler, scaler, device)\n",
    "            train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "            print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "            wandb.log({\n",
    "                \"train/loss\": train_loss,\n",
    "                \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "                \"train/precision\": train_metrics[\"precision\"],\n",
    "                \"train/recall\": train_metrics[\"recall\"],\n",
    "                \"train/f1\": train_metrics[\"f1\"],\n",
    "            })\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss, val_predictions, val_labels = validate(model, likelihood, val_loader, criterion_cl, criterion_bce, device)\n",
    "            val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "            print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "            wandb.log({\n",
    "                \"val/loss\": val_loss,\n",
    "                \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "                \"val/precision\": val_metrics[\"precision\"],\n",
    "                \"val/recall\": val_metrics[\"recall\"],\n",
    "                \"val/f1\": val_metrics[\"f1\"],\n",
    "            })\n",
    "\n",
    "            # Early Stopping Check\n",
    "            early_stopping(val_metrics[\"accuracy\"], model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "            # Save best model\n",
    "            if val_metrics['accuracy'] > best_val_accuracy:\n",
    "                best_val_accuracy = val_metrics['accuracy']\n",
    "                best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(early_stopping.best_model_state or best_model_state)\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Epoch with 2 Phases"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.626437Z",
     "start_time": "2025-01-15T04:52:58.601044Z"
    }
   },
   "source": [
    "def train_epoch_phase1(model, data_loader, criterion_cl, optimizer, scheduler, scaler, device):\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        aug1 = augment_batch(batch_data).cuda()\n",
    "        aug2 = augment_batch(batch_data).cuda()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output, _ = model(torch.cat([aug1, aug2], dim=0))\n",
    "            NTXLoss = criterion_cl(output)\n",
    "\n",
    "        loss = NTXLoss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "        predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss, predictions, labels\n",
    "\n",
    "def train_epoch_phase2(model_supcon, model_classifier, data_loader, criterion_bce, optimizer, scheduler, scaler, device):\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model_supcon.eval()\n",
    "    model_classifier.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            aug_data = augment_batch(batch_data).cuda()\n",
    "            _, features = model_supcon(aug_data)\n",
    "            output = model_classifier(features)\n",
    "            loss = combined_loss(output, model_classifier.gp_layer, batch_patient_labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "        scaler.update()\n",
    "\n",
    "        predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "        labels.extend(batch_patient_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def valid_epoch_phase2(model_supcon, model_classifier, data_loader, criterion_bce, device):\n",
    "    model_supcon.eval()\n",
    "    model_classifier.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                _, features = model_supcon(batch_data)\n",
    "                output = model_classifier(features)\n",
    "                loss = combined_loss(output, model_classifier.gp_layer, batch_patient_labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "def train_model_2_phases(model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                optimizer_supcon, optimizer_att_gp, num_epochs_phase1, num_epochs_phase2,\n",
    "                learning_rate, device='cuda', training_type='classification'):\n",
    "\n",
    "    model_supcon.to(device)\n",
    "    model_att_gp.to(device)\n",
    "    best_val_accuracy = 0.0\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    if training_type != 'classification':\n",
    "        # Phase 1: Train SupConResnet\n",
    "        scheduler_supcon = optim.lr_scheduler.OneCycleLR(optimizer_supcon,\n",
    "                                                          max_lr=learning_rate,\n",
    "                                                          steps_per_epoch=len(train_loader),\n",
    "                                                          epochs=num_epochs_phase1)\n",
    "\n",
    "        for epoch in range(num_epochs_phase1):\n",
    "            train_loss, train_predictions, train_labels = train_epoch_phase1(model_supcon,\n",
    "                                                                            train_loader,\n",
    "                                                                            criterion_cl,\n",
    "                                                                            optimizer_supcon,\n",
    "                                                                            scheduler_supcon,\n",
    "                                                                            scaler,\n",
    "                                                                            device)\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs_phase1}: Loss = {train_loss}')\n",
    "\n",
    "    # Save the state of SupConResnet after training phase 1\n",
    "    supcon_model_state = model_supcon.state_dict()\n",
    "\n",
    "    # Freeze SupConResnet parameters\n",
    "    for param in model_supcon.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Phase 2: Train ATT_GP\n",
    "    scheduler_att_gp = optim.lr_scheduler.OneCycleLR(optimizer_att_gp,\n",
    "                                                      max_lr=learning_rate,\n",
    "                                                      steps_per_epoch=len(train_loader),\n",
    "                                                      epochs=num_epochs_phase2)\n",
    "\n",
    "    for epoch in range(num_epochs_phase2):\n",
    "        train_loss_att_gp, train_predictions_att_gp, train_labels_att_gp = train_epoch_phase2( model_supcon,\n",
    "                                                                                               model_att_gp,\n",
    "                                                                                               train_loader,\n",
    "                                                                                               criterion_bce,\n",
    "                                                                                               optimizer_att_gp,\n",
    "                                                                                               scheduler_att_gp,\n",
    "                                                                                               scaler,\n",
    "                                                                                               device)\n",
    "        train_metrics = calculate_metrics(train_predictions_att_gp, train_labels_att_gp)\n",
    "        print_epoch_stats(epoch, num_epochs_phase2, \"train\", train_loss_att_gp, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss_att_gp,\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = valid_epoch_phase2(model_supcon, model_att_gp, val_loader, criterion_bce, device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs_phase2, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model_att_gp.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model_supcon.load_state_dict(supcon_model_state)\n",
    "    model_att_gp.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model_supcon, model_att_gp\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016054,
     "end_time": "2024-10-04T19:24:02.668925",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.652871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.292052Z",
     "iopub.status.busy": "2024-11-09T12:04:11.291769Z",
     "iopub.status.idle": "2024-11-09T12:04:11.359152Z",
     "shell.execute_reply": "2024-11-09T12:04:11.358128Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.292017Z"
    },
    "papermill": {
     "duration": 0.079611,
     "end_time": "2024-10-04T19:24:02.764721",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.685110",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.674024Z",
     "start_time": "2025-01-15T04:52:58.651748Z"
    }
   },
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, likelihood, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode(), gpytorch.settings.num_likelihood_samples(16):\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, _ = model(batch_data)\n",
    "                preds = likelihood(output).probs.gt(0.5).float()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "                preds = likelihood(output).probs.mean(0).argmax(-1)\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            # predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "def evaluate_model_2(model_supcon, model_classifier,data_loader, device='cuda'):\n",
    "    model_supcon = model_supcon.to(device)\n",
    "    model_classifier = model_classifier.to(device)\n",
    "    model_supcon.eval(), model_classifier.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            projection_head, features = model_supcon(batch_data)\n",
    "            output = model_classifier(features)\n",
    "            predictions.extend((output.squeeze() > THRESHOLD).cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01814,
     "end_time": "2024-10-04T19:24:02.800208",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.782068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.360796Z",
     "iopub.status.busy": "2024-11-09T12:04:11.360394Z",
     "iopub.status.idle": "2024-11-09T12:04:11.429794Z",
     "shell.execute_reply": "2024-11-09T12:04:11.428871Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.360761Z"
    },
    "papermill": {
     "duration": 0.092339,
     "end_time": "2024-10-04T19:24:02.911187",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.818848",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.729362Z",
     "start_time": "2025-01-15T04:52:58.702679Z"
    }
   },
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, likelihood, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                output, _, _, _ = model(batch_data)\n",
    "                preds = likelihood(output).probs.float()\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                output = model(batch_data)\n",
    "                preds = likelihood(output).probs.mean(0).float()\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "            # predictions.extend(output.squeeze().cpu().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1: # Binary classification\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, likelihood, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, likelihood, data_loader, device)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018543,
     "end_time": "2024-10-04T19:24:02.948830",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.930287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.431237Z",
     "iopub.status.busy": "2024-11-09T12:04:11.430912Z",
     "iopub.status.idle": "2024-11-09T12:04:11.497859Z",
     "shell.execute_reply": "2024-11-09T12:04:11.496987Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.431205Z"
    },
    "papermill": {
     "duration": 0.079014,
     "end_time": "2024-10-04T19:24:03.046531",
     "exception": false,
     "start_time": "2024-10-04T19:24:02.967517",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.773967Z",
     "start_time": "2025-01-15T04:52:58.752885Z"
    }
   },
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path, params):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class(params)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01837,
     "end_time": "2024-10-04T19:24:03.082997",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.064627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualization Functions\n",
    "\n",
    "### Visualizing Attention Weights and Images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.499350Z",
     "iopub.status.busy": "2024-11-09T12:04:11.499019Z",
     "iopub.status.idle": "2024-11-09T12:04:11.569343Z",
     "shell.execute_reply": "2024-11-09T12:04:11.568408Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.499317Z"
    },
    "papermill": {
     "duration": 0.095451,
     "end_time": "2024-10-04T19:24:03.197071",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.101620",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.828438Z",
     "start_time": "2025-01-15T04:52:58.801395Z"
    }
   },
   "source": [
    "def plot_label_attention_weights(model, data_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Plot images with their labels and attention values in a single large plot.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model\n",
    "    - data_loader: DataLoader containing test dataset\n",
    "    - device: Device to run the model on ('cuda' or 'cpu')\n",
    "    - CHANNELS: Number of channels in the image (e.g., 1 for grayscale, 3 for RGB)\n",
    "\n",
    "    Expected shapes:\n",
    "    - 1-channel images: (batch_size, num_images, 224, 224)\n",
    "    - 3-channel images: (batch_size, num_images, 3, 224, 224)\n",
    "    - attention: float value per image indicating attention weight\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    num_images = MAX_SLICES\n",
    "    rows, cols = 10, 6  # Adjust to fit 60 images in a single plot\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patients_label, batch_multi_labels in data_loader:\n",
    "            # Move data to the appropriate device\n",
    "            batch_data = batch_data.to(device)\n",
    "            outputs, _, attention_weight_batch, _, _ = model(batch_data)\n",
    "\n",
    "            # Process each patient in the batch\n",
    "            for patient_idx in range(batch_data.size(0)):\n",
    "                if batch_patients_label[patient_idx].item() == 1:  # Check if patient has positive label\n",
    "                    # Create a new figure for this patient\n",
    "                    fig = plt.figure(figsize=(cols * 4, rows * 4 + 2))  # Increased height for suptitle\n",
    "\n",
    "                    for img_idx in range(num_images):\n",
    "                        # Get the image and its label\n",
    "                        img = batch_data[patient_idx, img_idx].cpu().numpy()\n",
    "                        img_label = batch_labels[patient_idx, img_idx].cpu().numpy()\n",
    "                        \n",
    "                        # Get attention value\n",
    "                        if attention_weight_batch.size(1) == batch_data.size(1):\n",
    "                            attention_value = attention_weight_batch[patient_idx, img_idx].cpu().item()\n",
    "                        else:\n",
    "                            attention_value = attention_weight_batch[patient_idx].cpu().item()\n",
    "                        \n",
    "                        # Plot image\n",
    "                        plt.subplot(rows, cols, img_idx + 1)\n",
    "                        if CHANNELS == 3:  # RGB image\n",
    "                            plt.imshow(img)\n",
    "                        else:  # Grayscale image\n",
    "                            if img.ndim == 3:  # If shape is (1, H, W)\n",
    "                                img = np.squeeze(img)  # Convert to (H, W)\n",
    "                            plt.imshow(img, cmap='gray')\n",
    "                        \n",
    "                        plt.title(f'Label: {img_label}\\nAttention: {attention_value:.4f}', fontsize=12)\n",
    "                        plt.axis('off')\n",
    "\n",
    "                    # Add overall title for the patient\n",
    "                    plt.suptitle(f'Patient Images (Patient Label: {batch_patients_label[patient_idx].cpu().numpy()})', fontsize=16)\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust rect to make space for suptitle\n",
    "                    plt.show()\n",
    "                                      \n",
    "                    # Since we are plotting only for one patient, return after the first plot\n",
    "                    return"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Augmented Bags"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.869210Z",
     "start_time": "2025-01-15T04:52:58.847704Z"
    }
   },
   "source": [
    "def visualize_augmented_bags(original_bags, augmented_bags, num_bags=12):\n",
    "    \"\"\"\n",
    "    Visualizes all instances of the first bag of original and augmented images.\n",
    "\n",
    "    Parameters:\n",
    "    - original_bags: A tensor of shape (batch_size, num_instances, channels, height, width)\n",
    "    - augmented_bags: A tensor of shape (batch_size, num_instances, channels, height, width)\n",
    "    - num_bags: Number of bags to visualize (only the first bag will be shown).\n",
    "    \"\"\"\n",
    "    # Only visualize the first bag\n",
    "    first_bag_index = 0\n",
    "    \n",
    "    # Get number of instances\n",
    "    num_instances = original_bags.size(1)\n",
    "    \n",
    "    print(f'Num instances: {num_instances}')\n",
    "    \n",
    "    # Limit the number of bags to visualize (but we only show the first one)\n",
    "    num_bags = min(num_bags, 1)  # We only want to visualize the first bag\n",
    "\n",
    "    fig, axes = plt.subplots(num_instances, 2, figsize=(12, 2 * num_instances))\n",
    "    \n",
    "    # Original images\n",
    "    for j in range(num_instances):  # Iterate over instances in the first bag\n",
    "        img = original_bags[first_bag_index][j].cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        axes[j, 0].imshow(img, cmap='gray')  # Use gray colormap for single channel images\n",
    "        axes[j, 0].axis('off')  # Hide axes for better visualization\n",
    "        axes[j, 0].set_title(f'Original Instance {j + 1}')\n",
    "        \n",
    "    # Augmented images\n",
    "    for j in range(num_instances):\n",
    "        img = augmented_bags[first_bag_index][j].cpu().numpy().squeeze()  # Remove channel dimension\n",
    "        axes[j, 1].imshow(img.squeeze(), cmap='gray')  # Use gray colormap for single channel images\n",
    "        axes[j, 1].axis('off')  # Hide axes for better visualization\n",
    "        axes[j, 1].set_title(f'Augmented Instance {j + 1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016248,
     "end_time": "2024-10-04T19:24:03.231225",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.214977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T12:04:11.635991Z",
     "iopub.status.busy": "2024-11-09T12:04:11.635549Z",
     "iopub.status.idle": "2024-11-09T12:18:52.781211Z",
     "shell.execute_reply": "2024-11-09T12:18:52.780156Z",
     "shell.execute_reply.started": "2024-11-09T12:04:11.635946Z"
    },
    "papermill": {
     "duration": 24983.553969,
     "end_time": "2024-10-05T02:20:26.801619",
     "exception": false,
     "start_time": "2024-10-04T19:24:03.247650",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.919110Z",
     "start_time": "2025-01-15T04:52:58.897170Z"
    }
   },
   "source": [
    "# def main(mode='train'):\n",
    "#     # os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "#\n",
    "#     # Initialize W&B\n",
    "#     wandb.init(project=\"MIL_Resnet_ICH\")\n",
    "#\n",
    "#     # Log hyperparameters\n",
    "#     config = wandb.config\n",
    "#     config.learning_rate = LEARNING_RATE\n",
    "#     config.batch_size = TRAIN_BATCH_SIZE\n",
    "#     config.num_epochs = NUM_EPOCHS\n",
    "#\n",
    "#     train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "#     test_labels = pd.read_csv('./data_analyze/testing_dataset_150_redundancy.csv')\n",
    "#     train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "#     val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "#     test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "#\n",
    "#     params = {\n",
    "#         'channels': CHANNELS,  # Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
    "#         'num_classes': NUM_CLASSES,  # Number of output classes for classification\n",
    "#         'drop_prob': 0.25,  # Dropout probability\n",
    "#         'inducing_points': INDUCING_POINTS,  # Number of inducing points for the Gaussian Process layer\n",
    "#         'projection_location': PROJECTION_LOCATION,  # Choose from 'after_resnet', 'after_attention', or 'after_gp'\n",
    "#         'projection_hidden_dim': PROJECTION_HIDDEN_DIM,  # Hidden dimension size for the projection head\n",
    "#         'projection_output_dim': PROJECTION_OUTPUT_DIM,  # Output dimension size for the projection head\n",
    "#         'attention_hidden_dim': ATTENTION_HIDDEN_DIM,  # Hidden dimension size for the attention head\n",
    "#         'gp_model': GP_MODEL,\n",
    "#         'feature_extractor': FEATURE_EXTRACTOR\n",
    "#     }\n",
    "#\n",
    "#     if TRAINING_TYPE == 'end_to_end':\n",
    "#         # Instantiate the CNN_GP_ATT model with the specified parameters\n",
    "#         # model = CNN_GP_ATT(params=params)\n",
    "#         model = CNN_ATT_GP(params=params)\n",
    "#         if GP_MODEL == 'multi_task':\n",
    "#             likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=8).cuda()\n",
    "#         likelihood = PGLikelihood().cuda()\n",
    "#         optimizer = optim.Adam([\n",
    "#             {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "#             {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "#         ])\n",
    "#     elif TRAINING_TYPE == 'dkl':\n",
    "#         model = DKLModel(params=params)\n",
    "#         config.learning_rate = 0.1\n",
    "#         likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, num_classes=NUM_CLASSES + 1).cuda()\n",
    "#\n",
    "#         optimizer = torch.optim.SGD([\n",
    "#             {'params': model.feature_extractor.parameters()},\n",
    "#             {'params': model.gp_layer.hyperparameters(), 'lr': config.learning_rate * 0.01},\n",
    "#             {'params': model.gp_layer.variational_parameters()},\n",
    "#             {'params': likelihood.parameters()},\n",
    "#         ], lr=config.learning_rate, momentum=0.9, nesterov=True, weight_decay=0)\n",
    "#     else:\n",
    "#         model_supcon = SupConResnet(params=params)\n",
    "#         model_att_gp = LinearClassifier(params=params)\n",
    "#\n",
    "#         optimizer_phase1 = optim.Adam(model_supcon.parameters(), lr=config.learning_rate)\n",
    "#         optimizer_phase2 = optim.RMSprop(model_att_gp.parameters(), lr=config.learning_rate)\n",
    "#\n",
    "#     criterion_cl = NTXentLoss(0.5)\n",
    "#     criterion_bce = torch.nn.BCELoss()\n",
    "#     criterion_bce_wll = torch.nn.BCEWithLogitsLoss()\n",
    "#\n",
    "#     if mode == 'train':\n",
    "#         if TRAINING_TYPE == 'end_to_end':\n",
    "#             wandb.watch(model) # Watch the model to log gradients and parameters\n",
    "#             trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "#\n",
    "#             predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "#             metrics = calculate_metrics(predictions, labels)\n",
    "#             wandb.log(metrics)\n",
    "#             print_metrics(metrics)\n",
    "#\n",
    "#             plot_roc_curve(trained_model, likelihood_, test_loader, DEVICE)\n",
    "#             plot_confusion_matrix(trained_model, likelihood_, test_loader, DEVICE)\n",
    "#             torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "#         elif TRAINING_TYPE == 'dkl':\n",
    "#             wandb.watch(model)\n",
    "#             trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "#\n",
    "#             predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "#             metrics = calculate_metrics(predictions, labels)\n",
    "#             wandb.log(metrics)\n",
    "#             print_metrics(metrics)\n",
    "#\n",
    "#             plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "#             plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "#             torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "#\n",
    "#         elif TRAINING_TYPE == 'only_phase2':\n",
    "#             trained_model_supcon = load_model(SupConResnet, 'results/trained_supcon.pth', params)\n",
    "#             _, trained_model_att_gp = train_model_2_phases(trained_model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce_wll, optimizer_phase1, optimizer_phase2, 20, 20, config.learning_rate, DEVICE, 'classification')\n",
    "#             torch.save(trained_model_att_gp.state_dict(), 'results/trained_att_gp.pth')\n",
    "#         else:\n",
    "#             wandb.watch(model_supcon)\n",
    "#             trained_model_supcon, trained_model_att_gp = train_model_2_phases(model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce_wll, optimizer_phase1, optimizer_phase2, 20, 20, config.learning_rate, DEVICE)\n",
    "#             torch.save(trained_model_supcon.state_dict(), 'results/trained_supcon.pth')\n",
    "#             torch.save(trained_model_att_gp.state_dict(), 'results/trained_att_gp.pth')\n",
    "#\n",
    "#     if TRAINING_TYPE == 'end_to_end':\n",
    "#         trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "#         predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "#         # plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "#         # plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "#     else:\n",
    "#         trained_model_supcon = load_model(SupConResnet, 'results/trained_supcon.pth', params)\n",
    "#         trained_model_att_gp = load_model(LinearClassifier, 'results/trained_att_gp.pth', params)\n",
    "#         predictions, labels = evaluate_model_2(trained_model_supcon, trained_model_att_gp, test_loader, DEVICE)\n",
    "#\n",
    "#\n",
    "#     metrics = calculate_metrics(predictions, labels)\n",
    "#     wandb.log(metrics)\n",
    "#     print_metrics(metrics)\n",
    "#\n",
    "#     # if mode == 'train':\n",
    "#     #     required_columns = ['patient_id', 'study_instance_uid', 'patient_label']\n",
    "#     #     temp_test_labels = test_labels[required_columns]\n",
    "#     #\n",
    "#     #     # Save results\n",
    "#     #     results_df = get_test_results(trained_model, test_loader, temp_test_labels, device)\n",
    "#     #     results_df.to_csv('results/results.csv', index=False)\n",
    "#     #     print(results_df.head())\n",
    "#     #\n",
    "#     #     # Log results DataFrame as a table in W&B (optional)\n",
    "#     #     wandb.log({\"results\": wandb.Table(dataframe=results_df)})\n",
    "#\n",
    "#     # Get the first batch of images from the evaluation loader\n",
    "#     images, _, _, _ = next(iter(train_loader))\n",
    "#     print(f'Original batch shape: {images.shape}')\n",
    "#\n",
    "#     # Augment the batch of images\n",
    "#     start = time.time()\n",
    "#     augmented_images = augment_batch(images)\n",
    "#     end = time.time()\n",
    "#     taken_time = end - start\n",
    "#     print(f'Augmented batch shape: {augmented_images.shape} | Time: {taken_time:.4f}')\n",
    "#\n",
    "#     # Visualize the original and augmented bags\n",
    "#     visualize_augmented_bags(images, augmented_images)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross - Validation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:58.975111Z",
     "start_time": "2025-01-15T04:52:58.947009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def main(mode='train'):\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    # Load the dataset\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    test_labels = pd.read_csv('./data_analyze/testing_dataset_150_redundancy.csv')\n",
    "\n",
    "    # Combine train and validation labels for cross-validation\n",
    "    all_labels = pd.concat([train_labels, val_labels])\n",
    "    # Initialize k-fold cross-validation\n",
    "    k_folds = 5  # Number of folds\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Store metrics for each fold\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_labels)):\n",
    "        print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "        # Split data into training and validation sets for this fold\n",
    "        train_fold_labels = all_labels.iloc[train_idx]\n",
    "        val_fold_labels = all_labels.iloc[val_idx]\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        train_loader = get_train_loader(dicom_dir, train_fold_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "        val_loader = get_train_loader(dicom_dir, val_fold_labels, batch_size=VALID_BATCH_SIZE)\n",
    "        test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "        # Initialize model and optimizer\n",
    "        params = {\n",
    "            'channels': CHANNELS,\n",
    "            'num_classes': NUM_CLASSES,\n",
    "            'drop_prob': 0.25,\n",
    "            'inducing_points': INDUCING_POINTS,\n",
    "            'projection_location': PROJECTION_LOCATION,\n",
    "            'projection_hidden_dim': PROJECTION_HIDDEN_DIM,\n",
    "            'projection_output_dim': PROJECTION_OUTPUT_DIM,\n",
    "            'attention_hidden_dim': ATTENTION_HIDDEN_DIM,\n",
    "            'gp_model': GP_MODEL,\n",
    "            'feature_extractor': FEATURE_EXTRACTOR\n",
    "        }\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            model = CNN_ATT_GP(params=params)\n",
    "            if GP_MODEL == 'multi_task':\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=8).cuda()\n",
    "            likelihood = PGLikelihood().cuda()\n",
    "            optimizer = optim.Adam([\n",
    "                {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "            ])\n",
    "        elif TRAINING_TYPE == 'dkl':\n",
    "            model = DKLModel(params=params)\n",
    "            config.learning_rate = 0.1\n",
    "            likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_features=model.num_dim, num_classes=NUM_CLASSES + 1).cuda()\n",
    "            optimizer = torch.optim.SGD([\n",
    "                {'params': model.feature_extractor.parameters()},\n",
    "                {'params': model.gp_layer.hyperparameters(), 'lr': config.learning_rate * 0.01},\n",
    "                {'params': model.gp_layer.variational_parameters()},\n",
    "                {'params': likelihood.parameters()},\n",
    "            ], lr=config.learning_rate, momentum=0.9, nesterov=True, weight_decay=0)\n",
    "        else:\n",
    "            model_supcon = SupConResnet(params=params)\n",
    "            model_att_gp = LinearClassifier(params=params)\n",
    "            optimizer_phase1 = optim.Adam(model_supcon.parameters(), lr=config.learning_rate)\n",
    "            optimizer_phase2 = optim.RMSprop(model_att_gp.parameters(), lr=config.learning_rate)\n",
    "\n",
    "        criterion_cl = NTXentLoss(0.5)\n",
    "        criterion_bce = torch.nn.BCELoss()\n",
    "        criterion_bce_wll = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        if mode == 'train':\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                wandb.watch(model)\n",
    "                trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "                # Evaluate the model on the validation set for this fold\n",
    "                predictions, labels = evaluate_model(trained_model, likelihood, val_loader, DEVICE)\n",
    "                metrics = calculate_metrics(predictions, labels)\n",
    "                fold_metrics.append(metrics)\n",
    "                wandb.log({f\"fold_{fold + 1}_{k}\": v for k, v in metrics.items()})\n",
    "                print_metrics(metrics)\n",
    "\n",
    "            elif TRAINING_TYPE == 'dkl':\n",
    "                wandb.watch(model)\n",
    "                trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "                # Evaluate the model on the validation set for this fold\n",
    "                predictions, labels = evaluate_model(trained_model, likelihood, val_loader, DEVICE)\n",
    "                metrics = calculate_metrics(predictions, labels)\n",
    "                fold_metrics.append(metrics)\n",
    "                wandb.log({f\"fold_{fold + 1}_{k}\": v for k, v in metrics.items()})\n",
    "                print_metrics(metrics)\n",
    "\n",
    "            elif TRAINING_TYPE == 'only_phase2':\n",
    "                trained_model_supcon = load_model(SupConResnet, 'results/trained_supcon.pth', params)\n",
    "                _, trained_model_att_gp = train_model_2_phases(trained_model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce_wll, optimizer_phase1, optimizer_phase2, 20, 20, config.learning_rate, DEVICE, 'classification')\n",
    "                torch.save(trained_model_att_gp.state_dict(), 'results/trained_att_gp.pth')\n",
    "            else:\n",
    "                wandb.watch(model_supcon)\n",
    "                trained_model_supcon, trained_model_att_gp = train_model_2_phases(model_supcon, model_att_gp, train_loader, val_loader, criterion_cl, criterion_bce_wll, optimizer_phase1, optimizer_phase2, 20, 20, config.learning_rate, DEVICE)\n",
    "                torch.save(trained_model_supcon.state_dict(), 'results/trained_supcon.pth')\n",
    "                torch.save(trained_model_att_gp.state_dict(), 'results/trained_att_gp.pth')\n",
    "\n",
    "    # Aggregate metrics across all folds\n",
    "    if fold_metrics:\n",
    "        avg_metrics = {k: np.mean([m[k] for m in fold_metrics]) for k in fold_metrics[0].keys()}\n",
    "        print(\"Average metrics across all folds:\")\n",
    "        print_metrics(avg_metrics)\n",
    "        wandb.log({\"average_metrics\": avg_metrics})\n",
    "\n",
    "    # # Final evaluation on the test set\n",
    "    # if TRAINING_TYPE == 'end_to_end':\n",
    "    #     trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "    #     predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "    # else:\n",
    "    #     trained_model_supcon = load_model(SupConResnet, 'results/trained_supcon.pth', params)\n",
    "    #     trained_model_att_gp = load_model(LinearClassifier, 'results/trained_att_gp.pth', params)\n",
    "    #     predictions, labels = evaluate_model_2(trained_model_supcon, trained_model_att_gp, test_loader, DEVICE)\n",
    "    #\n",
    "    # metrics = calculate_metrics(predictions, labels)\n",
    "    # wandb.log(metrics)\n",
    "    # print_metrics(metrics)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Optimization"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:59.032423Z",
     "start_time": "2025-01-15T04:52:59.002112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "# test_labels = pd.read_csv('./data_analyze/testing_dataset_150_redundancy.csv')\n",
    "train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "criterion_cl = NTXentLoss(0.5)\n",
    "criterion_bce = torch.nn.BCELoss()\n",
    "\n",
    "# Define the objective function for Bayesian Optimization\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters using Optuna\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'drop_prob': trial.suggest_float('drop_prob', 0.1, 0.5),\n",
    "        'inducing_points': trial.suggest_categorical('inducing_points', [16, 32, 64, 128]),\n",
    "        'attention_hidden_dim': trial.suggest_categorical('attention_hidden_dim', [8, 16, 32, 64]),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [4, 8]),\n",
    "        'num_epochs': trial.suggest_int('num_epochs', 10, 20),\n",
    "    }\n",
    "\n",
    "    # Initialize W&B run\n",
    "    wandb.init(config=params, project=\"MIL_Resnet_ICH_Optim\", group=\"bayesian_optimization\")\n",
    "\n",
    "    # Update the model parameters\n",
    "    model_params = {\n",
    "        'channels': CHANNELS,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'drop_prob': params['drop_prob'],\n",
    "        'inducing_points': params['inducing_points'],\n",
    "        'attention_hidden_dim': params['attention_hidden_dim'],\n",
    "        'projection_location': 'after_resnet',\n",
    "        'projection_hidden_dim': PROJECTION_HIDDEN_DIM,\n",
    "        'projection_output_dim': PROJECTION_OUTPUT_DIM,\n",
    "        'gp_model': GP_MODEL,\n",
    "        'feature_extractor': FEATURE_EXTRACTOR\n",
    "    }\n",
    "\n",
    "    # Set up the model\n",
    "    model = CNN_ATT_GP(params=model_params)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Set up the likelihood\n",
    "    likelihood = PGLikelihood().cuda()\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.parameters(), 'lr': params['learning_rate']},\n",
    "        {'params': likelihood.parameters(), 'lr': params['learning_rate']}\n",
    "    ])\n",
    "\n",
    "    # Set up early stopping\n",
    "    # early_stopping = EarlyStoppingForOptimization(patience=5, verbose=True, delta=0.001)\n",
    "\n",
    "    # Train the model using your train_model function\n",
    "    trained_model = train_model(\n",
    "        model=model,\n",
    "        likelihood=likelihood,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion_cl=criterion_cl,\n",
    "        criterion_bce=criterion_bce,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=params['num_epochs'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    # Evaluate the trained model on the validation set\n",
    "    val_loss, val_predictions, val_labels = validate(trained_model, likelihood, val_loader, criterion_cl, criterion_bce, DEVICE)\n",
    "    val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "\n",
    "    # Log the best validation accuracy to W&B\n",
    "    best_val_accuracy = val_metrics['accuracy']\n",
    "    wandb.log({'best_val_acc': best_val_accuracy})\n",
    "    wandb.finish()\n",
    "\n",
    "    return best_val_accuracy"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train with Hyperparameter Optimization"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T04:52:59.071473Z",
     "start_time": "2025-01-15T04:52:59.052667Z"
    }
   },
   "source": [
    "# if __name__ == '__main__':\n",
    "#     # Set up Optuna study\n",
    "#     study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "#\n",
    "#     # Start optimization\n",
    "#     study.optimize(objective, n_trials=30, timeout=None)\n",
    "#\n",
    "#     # Get the best hyperparameters\n",
    "#     best_params = study.best_trial.params\n",
    "#     print(\"Best hyperparameters:\", best_params)\n",
    "#\n",
    "#     # Retrain the model with the best hyperparameters\n",
    "#     model_params = {\n",
    "#         'channels': CHANNELS,\n",
    "#         'num_classes': NUM_CLASSES,\n",
    "#         'drop_prob': best_params['drop_prob'],\n",
    "#         'inducing_points': best_params['inducing_points'],\n",
    "#         'attention_hidden_dim': best_params['attention_hidden_dim'],\n",
    "#         'gp_model': 'single_task',\n",
    "#         'projection_location': 'after_resnet',\n",
    "#         'projection_hidden_dim': PROJECTION_HIDDEN_DIM,\n",
    "#         'projection_output_dim': PROJECTION_OUTPUT_DIM,\n",
    "#     }\n",
    "#\n",
    "#     model = CNN_ATT_GP(params=model_params)\n",
    "#     model.to(DEVICE)\n",
    "#     likelihood = PGLikelihood().cuda()\n",
    "#     optimizer = optim.Adam([\n",
    "#         {'params': model.parameters(), 'lr': best_params['learning_rate']},\n",
    "#         {'params': likelihood.parameters(), 'lr': best_params['learning_rate']}\n",
    "#     ])\n",
    "#\n",
    "#     # Train the model with the best hyperparameters\n",
    "#     trained_model = train_model(\n",
    "#         model=model,\n",
    "#         likelihood=likelihood,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         criterion_cl=criterion_cl,\n",
    "#         criterion_bce=criterion_bce,\n",
    "#         optimizer=optimizer,\n",
    "#         num_epochs=best_params['num_epochs'],\n",
    "#         learning_rate=best_params['learning_rate'],\n",
    "#         device=DEVICE\n",
    "#     )\n",
    "#\n",
    "#     # Evaluate on the test set\n",
    "#     predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "#     metrics = calculate_metrics(predictions, labels)\n",
    "#     print_metrics(metrics)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Result"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T05:44:33.717441Z",
     "start_time": "2025-01-15T04:52:59.098305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mhuynhsikha2003\u001B[0m (\u001B[33mhuynhsikha2003-i-h-c-qu-c-gia-tp-hcm\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/media/hskha23/Kha/Brain-Stroke-Diagnosis/rsna/wandb/run-20250115_115300-gw7bpav7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/gw7bpav7' target=\"_blank\">azure-shadow-438</a></strong> to <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/gw7bpav7' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/gw7bpav7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Length of train_loader: 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 0.0311, Accuracy: 0.5872, Precision: 0.2951, Recall: 0.4957, F1: 0.3700\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6020, Precision: 0.3010, Recall: 0.5000, F1: 0.3758\n",
      "Epoch 2/50 - Train:\n",
      "Loss: 0.0014, Accuracy: 0.5923, Precision: 0.2962, Recall: 0.5000, F1: 0.3720\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6071, Precision: 0.8026, Recall: 0.5064, F1: 0.3897\n",
      "Epoch 3/50 - Train:\n",
      "Loss: -0.0497, Accuracy: 0.7128, Precision: 0.7494, Recall: 0.6620, F1: 0.6589\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7347, Precision: 0.8037, Recall: 0.6732, F1: 0.6714\n",
      "Epoch 4/50 - Train:\n",
      "Loss: -0.1269, Accuracy: 0.8000, Precision: 0.8035, Recall: 0.7782, F1: 0.7853\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7500, Precision: 0.7700, Recall: 0.7055, F1: 0.7122\n",
      "Epoch 5/50 - Train:\n",
      "Loss: -0.2071, Accuracy: 0.8462, Precision: 0.8490, Recall: 0.8304, F1: 0.8369\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8163, Precision: 0.8649, Recall: 0.7736, F1: 0.7877\n",
      "Epoch 6/50 - Train:\n",
      "Loss: -0.2332, Accuracy: 0.8333, Precision: 0.8339, Recall: 0.8181, F1: 0.8237\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7755, Precision: 0.8149, Recall: 0.7288, F1: 0.7382\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 7/50 - Train:\n",
      "Loss: -0.2502, Accuracy: 0.8385, Precision: 0.8347, Recall: 0.8288, F1: 0.8314\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8214, Precision: 0.8147, Recall: 0.8256, F1: 0.8174\n",
      "Epoch 8/50 - Train:\n",
      "Loss: -0.3095, Accuracy: 0.8782, Precision: 0.8768, Recall: 0.8697, F1: 0.8728\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7398, Precision: 0.7579, Recall: 0.7643, F1: 0.7395\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 9/50 - Train:\n",
      "Loss: -0.3220, Accuracy: 0.8744, Precision: 0.8700, Recall: 0.8693, F1: 0.8697\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7449, Precision: 0.7521, Recall: 0.7621, F1: 0.7436\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 10/50 - Train:\n",
      "Loss: -0.3531, Accuracy: 0.8897, Precision: 0.8874, Recall: 0.8833, F1: 0.8851\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7653, Precision: 0.8347, Recall: 0.7095, F1: 0.7155\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 11/50 - Train:\n",
      "Loss: -0.4252, Accuracy: 0.9308, Precision: 0.9291, Recall: 0.9273, F1: 0.9282\n",
      "Epoch 11/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7602, Precision: 0.8437, Recall: 0.7009, F1: 0.7047\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 12/50 - Train:\n",
      "Loss: -0.4291, Accuracy: 0.9295, Precision: 0.9284, Recall: 0.9253, F1: 0.9267\n",
      "Epoch 12/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7857, Precision: 0.7960, Recall: 0.8068, F1: 0.7849\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 13/50 - Train:\n",
      "Loss: -0.4786, Accuracy: 0.9577, Precision: 0.9578, Recall: 0.9544, F1: 0.9560\n",
      "Epoch 13/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7857, Precision: 0.7782, Recall: 0.7873, F1: 0.7806\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 14/50 - Train:\n",
      "Loss: -0.4644, Accuracy: 0.9462, Precision: 0.9476, Recall: 0.9408, F1: 0.9438\n",
      "Epoch 14/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8214, Precision: 0.8174, Recall: 0.8061, F1: 0.8105\n",
      "Epoch 15/50 - Train:\n",
      "Loss: -0.5267, Accuracy: 0.9782, Precision: 0.9781, Recall: 0.9767, F1: 0.9774\n",
      "Epoch 15/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7908, Precision: 0.7816, Recall: 0.7850, F1: 0.7831\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 16/50 - Train:\n",
      "Loss: -0.5553, Accuracy: 0.9808, Precision: 0.9808, Recall: 0.9794, F1: 0.9801\n",
      "Epoch 16/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8265, Precision: 0.8190, Recall: 0.8190, F1: 0.8190\n",
      "Epoch 17/50 - Train:\n",
      "Loss: -0.6123, Accuracy: 0.9987, Precision: 0.9989, Recall: 0.9984, F1: 0.9987\n",
      "Epoch 17/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8214, Precision: 0.8605, Recall: 0.7822, F1: 0.7962\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 18/50 - Train:\n",
      "Loss: -0.6320, Accuracy: 0.9974, Precision: 0.9973, Recall: 0.9973, F1: 0.9973\n",
      "Epoch 18/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8367, Precision: 0.8349, Recall: 0.8209, F1: 0.8263\n",
      "Epoch 19/50 - Train:\n",
      "Loss: -0.6000, Accuracy: 0.9872, Precision: 0.9872, Recall: 0.9862, F1: 0.9867\n",
      "Epoch 19/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7398, Precision: 0.8491, Recall: 0.6731, F1: 0.6683\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 20/50 - Train:\n",
      "Loss: -0.5821, Accuracy: 0.9692, Precision: 0.9677, Recall: 0.9686, F1: 0.9682\n",
      "Epoch 20/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8163, Precision: 0.8223, Recall: 0.7910, F1: 0.8000\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 21/50 - Train:\n",
      "Loss: -0.6039, Accuracy: 0.9756, Precision: 0.9765, Recall: 0.9730, F1: 0.9747\n",
      "Epoch 21/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8163, Precision: 0.8735, Recall: 0.7714, F1: 0.7858\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 22/50 - Train:\n",
      "Loss: -0.6475, Accuracy: 0.9910, Precision: 0.9909, Recall: 0.9905, F1: 0.9907\n",
      "Epoch 22/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7653, Precision: 0.7653, Recall: 0.7768, F1: 0.7628\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 23/50 - Train:\n",
      "Loss: -0.6462, Accuracy: 0.9872, Precision: 0.9867, Recall: 0.9867, F1: 0.9867\n",
      "Epoch 23/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8367, Precision: 0.8589, Recall: 0.8057, F1: 0.8186\n",
      "Epoch 24/50 - Train:\n",
      "Loss: -0.6871, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 24/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8367, Precision: 0.8589, Recall: 0.8057, F1: 0.8186\n",
      "Epoch 25/50 - Train:\n",
      "Loss: -0.7047, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 25/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8469, Precision: 0.8663, Recall: 0.8186, F1: 0.8311\n",
      "Epoch 26/50 - Train:\n",
      "Loss: -0.7145, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 26/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8418, Precision: 0.8537, Recall: 0.8165, F1: 0.8272\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 27/50 - Train:\n",
      "Loss: -0.7229, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 27/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8571, Precision: 0.8738, Recall: 0.8314, F1: 0.8434\n",
      "Epoch 28/50 - Train:\n",
      "Loss: -0.7313, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 28/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8571, Precision: 0.8738, Recall: 0.8314, F1: 0.8434\n",
      "Epoch 29/50 - Train:\n",
      "Loss: -0.7381, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 29/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8520, Precision: 0.8656, Recall: 0.8271, F1: 0.8384\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 30/50 - Train:\n",
      "Loss: -0.7441, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 30/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8571, Precision: 0.8695, Recall: 0.8336, F1: 0.8444\n",
      "Epoch 31/50 - Train:\n",
      "Loss: -0.7504, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 31/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8469, Precision: 0.8542, Recall: 0.8251, F1: 0.8344\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 32/50 - Train:\n",
      "Loss: -0.7555, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 32/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8520, Precision: 0.8656, Recall: 0.8271, F1: 0.8384\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 33/50 - Train:\n",
      "Loss: -0.7616, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 33/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8520, Precision: 0.8617, Recall: 0.8293, F1: 0.8394\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 34/50 - Train:\n",
      "Loss: -0.7671, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 34/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8469, Precision: 0.8618, Recall: 0.8207, F1: 0.8322\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 35/50 - Train:\n",
      "Loss: -0.7728, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 35/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8469, Precision: 0.8618, Recall: 0.8207, F1: 0.8322\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 36/50 - Train:\n",
      "Loss: -0.7783, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 36/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8469, Precision: 0.8618, Recall: 0.8207, F1: 0.8322\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 37/50 - Train:\n",
      "Loss: -0.7827, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 37/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8520, Precision: 0.8656, Recall: 0.8271, F1: 0.8384\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 38/50 - Train:\n",
      "Loss: -0.7866, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 38/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8469, Precision: 0.8577, Recall: 0.8229, F1: 0.8333\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 39/50 - Train:\n",
      "Loss: -0.7903, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 39/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8418, Precision: 0.8537, Recall: 0.8165, F1: 0.8272\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 40/50 - Train:\n",
      "Loss: -0.7939, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 40/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8469, Precision: 0.8618, Recall: 0.8207, F1: 0.8322\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 0.8571428571428571\n",
      "Test Accuracy: 0.8469, Precision: 0.8618, Recall: 0.8207, F1: 0.8322\n",
      "Fold 2/5\n",
      "Length of train_loader: 781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 0.0296, Accuracy: 0.5923, Precision: 0.5551, Recall: 0.5098, F1: 0.4190\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6122, Precision: 0.3061, Recall: 0.5000, F1: 0.3797\n",
      "Epoch 2/50 - Train:\n",
      "Loss: -0.0373, Accuracy: 0.6859, Precision: 0.7799, Recall: 0.6215, F1: 0.5966\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8163, Precision: 0.8331, Recall: 0.7800, F1: 0.7928\n",
      "Epoch 3/50 - Train:\n",
      "Loss: -0.1187, Accuracy: 0.7962, Precision: 0.8084, Recall: 0.7696, F1: 0.7780\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8316, Precision: 0.8241, Recall: 0.8191, F1: 0.8214\n",
      "Epoch 4/50 - Train:\n",
      "Loss: -0.1696, Accuracy: 0.7923, Precision: 0.7952, Recall: 0.7711, F1: 0.7777\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8520, Precision: 0.8611, Recall: 0.8261, F1: 0.8373\n",
      "Epoch 5/50 - Train:\n",
      "Loss: -0.2444, Accuracy: 0.8577, Precision: 0.8620, Recall: 0.8424, F1: 0.8492\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8316, Precision: 0.8608, Recall: 0.7925, F1: 0.8078\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 6/50 - Train:\n",
      "Loss: -0.2889, Accuracy: 0.8782, Precision: 0.8794, Recall: 0.8677, F1: 0.8724\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8673, Precision: 0.8603, Recall: 0.8603, F1: 0.8603\n",
      "Epoch 7/50 - Train:\n",
      "Loss: -0.3448, Accuracy: 0.9128, Precision: 0.9147, Recall: 0.9045, F1: 0.9088\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8061, Precision: 0.7958, Recall: 0.7958, F1: 0.7958\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 8/50 - Train:\n",
      "Loss: -0.3850, Accuracy: 0.9295, Precision: 0.9333, Recall: 0.9212, F1: 0.9262\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8061, Precision: 0.8304, Recall: 0.7645, F1: 0.7778\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 9/50 - Train:\n",
      "Loss: -0.3985, Accuracy: 0.9256, Precision: 0.9289, Recall: 0.9173, F1: 0.9221\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8316, Precision: 0.8608, Recall: 0.7925, F1: 0.8078\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 10/50 - Train:\n",
      "Loss: -0.4385, Accuracy: 0.9436, Precision: 0.9438, Recall: 0.9392, F1: 0.9414\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7959, Precision: 0.7849, Recall: 0.7899, F1: 0.7870\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 11/50 - Train:\n",
      "Loss: -0.4517, Accuracy: 0.9423, Precision: 0.9450, Recall: 0.9359, F1: 0.9398\n",
      "Epoch 11/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8316, Precision: 0.8220, Recall: 0.8263, F1: 0.8239\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 12/50 - Train:\n",
      "Loss: -0.4896, Accuracy: 0.9615, Precision: 0.9626, Recall: 0.9578, F1: 0.9600\n",
      "Epoch 12/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7551, Precision: 0.7476, Recall: 0.7590, F1: 0.7492\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 13/50 - Train:\n",
      "Loss: -0.5115, Accuracy: 0.9705, Precision: 0.9717, Recall: 0.9674, F1: 0.9694\n",
      "Epoch 13/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8520, Precision: 0.8611, Recall: 0.8261, F1: 0.8373\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 14/50 - Train:\n",
      "Loss: -0.5274, Accuracy: 0.9679, Precision: 0.9690, Recall: 0.9647, F1: 0.9667\n",
      "Epoch 14/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8316, Precision: 0.8220, Recall: 0.8263, F1: 0.8239\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 15/50 - Train:\n",
      "Loss: -0.5549, Accuracy: 0.9744, Precision: 0.9739, Recall: 0.9730, F1: 0.9735\n",
      "Epoch 15/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8163, Precision: 0.8092, Recall: 0.8235, F1: 0.8119\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 16/50 - Train:\n",
      "Loss: -0.6090, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 16/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8418, Precision: 0.8459, Recall: 0.8178, F1: 0.8272\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 0.8673469387755102\n",
      "Test Accuracy: 0.8418, Precision: 0.8459, Recall: 0.8178, F1: 0.8272\n",
      "Fold 3/5\n",
      "Length of train_loader: 782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 0.0297, Accuracy: 0.5923, Precision: 0.4220, Recall: 0.4983, F1: 0.3749\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.5885, Precision: 0.2943, Recall: 0.5000, F1: 0.3705\n",
      "Epoch 2/50 - Train:\n",
      "Loss: -0.0070, Accuracy: 0.5987, Precision: 0.6987, Recall: 0.5053, F1: 0.3861\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6875, Precision: 0.7828, Recall: 0.6147, F1: 0.5890\n",
      "Epoch 3/50 - Train:\n",
      "Loss: -0.0638, Accuracy: 0.7179, Precision: 0.7484, Recall: 0.6680, F1: 0.6674\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7500, Precision: 0.7610, Recall: 0.7119, F1: 0.7188\n",
      "Epoch 4/50 - Train:\n",
      "Loss: -0.1315, Accuracy: 0.7962, Precision: 0.8005, Recall: 0.7721, F1: 0.7797\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7865, Precision: 0.7841, Recall: 0.7675, F1: 0.7728\n",
      "Epoch 5/50 - Train:\n",
      "Loss: -0.2013, Accuracy: 0.8333, Precision: 0.8332, Recall: 0.8183, F1: 0.8237\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7969, Precision: 0.8011, Recall: 0.8103, F1: 0.7959\n",
      "Epoch 6/50 - Train:\n",
      "Loss: -0.2411, Accuracy: 0.8500, Precision: 0.8488, Recall: 0.8376, F1: 0.8420\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8125, Precision: 0.8269, Recall: 0.7874, F1: 0.7964\n",
      "Epoch 7/50 - Train:\n",
      "Loss: -0.2995, Accuracy: 0.8769, Precision: 0.8760, Recall: 0.8675, F1: 0.8711\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8854, Precision: 0.8985, Recall: 0.8657, F1: 0.8763\n",
      "Epoch 8/50 - Train:\n",
      "Loss: -0.3395, Accuracy: 0.8923, Precision: 0.8908, Recall: 0.8848, F1: 0.8875\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7812, Precision: 0.7812, Recall: 0.7940, F1: 0.7789\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 9/50 - Train:\n",
      "Loss: -0.3262, Accuracy: 0.8731, Precision: 0.8691, Recall: 0.8672, F1: 0.8681\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7760, Precision: 0.8310, Recall: 0.7304, F1: 0.7382\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 10/50 - Train:\n",
      "Loss: -0.4098, Accuracy: 0.9103, Precision: 0.9080, Recall: 0.9054, F1: 0.9066\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7188, Precision: 0.8393, Recall: 0.6538, F1: 0.6395\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 11/50 - Train:\n",
      "Loss: -0.4377, Accuracy: 0.9359, Precision: 0.9368, Recall: 0.9301, F1: 0.9331\n",
      "Epoch 11/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6198, Precision: 0.8048, Recall: 0.5321, F1: 0.4390\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 12/50 - Train:\n",
      "Loss: -0.4366, Accuracy: 0.9141, Precision: 0.9140, Recall: 0.9071, F1: 0.9102\n",
      "Epoch 12/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7552, Precision: 0.7755, Recall: 0.7806, F1: 0.7550\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 13/50 - Train:\n",
      "Loss: -0.5103, Accuracy: 0.9615, Precision: 0.9601, Recall: 0.9601, F1: 0.9601\n",
      "Epoch 13/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8385, Precision: 0.8598, Recall: 0.8114, F1: 0.8229\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 14/50 - Train:\n",
      "Loss: -0.5172, Accuracy: 0.9577, Precision: 0.9568, Recall: 0.9554, F1: 0.9561\n",
      "Epoch 14/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8125, Precision: 0.8068, Recall: 0.8158, F1: 0.8090\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 15/50 - Train:\n",
      "Loss: -0.5517, Accuracy: 0.9641, Precision: 0.9632, Recall: 0.9623, F1: 0.9627\n",
      "Epoch 15/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8579, Recall: 0.7988, F1: 0.8102\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 16/50 - Train:\n",
      "Loss: -0.5550, Accuracy: 0.9705, Precision: 0.9711, Recall: 0.9677, F1: 0.9693\n",
      "Epoch 16/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8346, Recall: 0.8033, F1: 0.8128\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 17/50 - Train:\n",
      "Loss: -0.5912, Accuracy: 0.9756, Precision: 0.9745, Recall: 0.9750, F1: 0.9747\n",
      "Epoch 17/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7656, Precision: 0.7884, Recall: 0.7895, F1: 0.7656\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 0.8854166666666666\n",
      "Test Accuracy: 0.7604, Precision: 0.7828, Recall: 0.7871, F1: 0.7603\n",
      "Fold 4/5\n",
      "Length of train_loader: 782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 0.0272, Accuracy: 0.6000, Precision: 0.3000, Recall: 0.5000, F1: 0.3750\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.5781, Precision: 0.2891, Recall: 0.5000, F1: 0.3663\n",
      "Epoch 2/50 - Train:\n",
      "Loss: -0.0307, Accuracy: 0.6590, Precision: 0.7566, Recall: 0.5793, F1: 0.5339\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7083, Precision: 0.7401, Recall: 0.6756, F1: 0.6719\n",
      "Epoch 3/50 - Train:\n",
      "Loss: -0.1099, Accuracy: 0.7974, Precision: 0.8125, Recall: 0.7655, F1: 0.7755\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7344, Precision: 0.7313, Recall: 0.7169, F1: 0.7203\n",
      "Epoch 4/50 - Train:\n",
      "Loss: -0.1797, Accuracy: 0.8346, Precision: 0.8358, Recall: 0.8159, F1: 0.8229\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7552, Precision: 0.7644, Recall: 0.7355, F1: 0.7396\n",
      "Epoch 5/50 - Train:\n",
      "Loss: -0.2296, Accuracy: 0.8308, Precision: 0.8287, Recall: 0.8152, F1: 0.8203\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8125, Precision: 0.8097, Recall: 0.8053, F1: 0.8071\n",
      "Epoch 6/50 - Train:\n",
      "Loss: -0.2791, Accuracy: 0.8654, Precision: 0.8644, Recall: 0.8531, F1: 0.8577\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7708, Precision: 0.7815, Recall: 0.7845, F1: 0.7707\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 7/50 - Train:\n",
      "Loss: -0.2791, Accuracy: 0.8487, Precision: 0.8495, Recall: 0.8331, F1: 0.8392\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7031, Precision: 0.7375, Recall: 0.7282, F1: 0.7025\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 8/50 - Train:\n",
      "Loss: -0.3237, Accuracy: 0.8718, Precision: 0.8685, Recall: 0.8634, F1: 0.8657\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7240, Precision: 0.7879, Recall: 0.6830, F1: 0.6773\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 9/50 - Train:\n",
      "Loss: -0.3384, Accuracy: 0.8872, Precision: 0.8859, Recall: 0.8777, F1: 0.8813\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7500, Precision: 0.7472, Recall: 0.7368, F1: 0.7398\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 10/50 - Train:\n",
      "Loss: -0.3781, Accuracy: 0.9064, Precision: 0.9030, Recall: 0.9017, F1: 0.9024\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7083, Precision: 0.7202, Recall: 0.7227, F1: 0.7082\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 11/50 - Train:\n",
      "Loss: -0.4219, Accuracy: 0.9269, Precision: 0.9274, Recall: 0.9200, F1: 0.9233\n",
      "Epoch 11/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7708, Precision: 0.7899, Recall: 0.7411, F1: 0.7478\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 12/50 - Train:\n",
      "Loss: -0.4804, Accuracy: 0.9513, Precision: 0.9523, Recall: 0.9461, F1: 0.9490\n",
      "Epoch 12/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7708, Precision: 0.7827, Recall: 0.7467, F1: 0.7527\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 13/50 - Train:\n",
      "Loss: -0.4888, Accuracy: 0.9577, Precision: 0.9567, Recall: 0.9552, F1: 0.9559\n",
      "Epoch 13/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6823, Precision: 0.7047, Recall: 0.7010, F1: 0.6821\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 14/50 - Train:\n",
      "Loss: -0.5367, Accuracy: 0.9756, Precision: 0.9765, Recall: 0.9728, F1: 0.9745\n",
      "Epoch 14/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8177, Precision: 0.8174, Recall: 0.8093, F1: 0.8122\n",
      "Epoch 15/50 - Train:\n",
      "Loss: -0.5377, Accuracy: 0.9718, Precision: 0.9706, Recall: 0.9706, F1: 0.9706\n",
      "Epoch 15/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7865, Precision: 0.8030, Recall: 0.7619, F1: 0.7688\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 16/50 - Train:\n",
      "Loss: -0.5834, Accuracy: 0.9859, Precision: 0.9851, Recall: 0.9856, F1: 0.9853\n",
      "Epoch 16/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7812, Precision: 0.7864, Recall: 0.7641, F1: 0.7691\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 17/50 - Train:\n",
      "Loss: -0.6243, Accuracy: 0.9949, Precision: 0.9947, Recall: 0.9947, F1: 0.9947\n",
      "Epoch 17/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7865, Precision: 0.8120, Recall: 0.7586, F1: 0.7658\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 18/50 - Train:\n",
      "Loss: -0.6599, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 18/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8073, Precision: 0.8139, Recall: 0.7915, F1: 0.7971\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 19/50 - Train:\n",
      "Loss: -0.6748, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 19/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8340, Recall: 0.8143, F1: 0.8199\n",
      "Epoch 20/50 - Train:\n",
      "Loss: -0.6871, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 20/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8229, Precision: 0.8275, Recall: 0.8110, F1: 0.8157\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 21/50 - Train:\n",
      "Loss: -0.6994, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 21/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8073, Precision: 0.8139, Recall: 0.7915, F1: 0.7971\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 22/50 - Train:\n",
      "Loss: -0.7119, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 22/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8333, Precision: 0.8358, Recall: 0.8208, F1: 0.8258\n",
      "Epoch 23/50 - Train:\n",
      "Loss: -0.7207, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 23/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8229, Precision: 0.8268, Recall: 0.8085, F1: 0.8140\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 24/50 - Train:\n",
      "Loss: -0.7289, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 24/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8177, Precision: 0.8223, Recall: 0.8023, F1: 0.8081\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 25/50 - Train:\n",
      "Loss: -0.7363, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 25/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8385, Precision: 0.8406, Recall: 0.8280, F1: 0.8323\n",
      "Epoch 26/50 - Train:\n",
      "Loss: -0.7438, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 26/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8333, Precision: 0.8361, Recall: 0.8220, F1: 0.8265\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 27/50 - Train:\n",
      "Loss: -0.7512, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 27/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8125, Precision: 0.8203, Recall: 0.7929, F1: 0.8000\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 28/50 - Train:\n",
      "Loss: -0.7560, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 28/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8279, Recall: 0.8190, F1: 0.8222\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 29/50 - Train:\n",
      "Loss: -0.7618, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 29/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8333, Precision: 0.8384, Recall: 0.8204, F1: 0.8258\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 30/50 - Train:\n",
      "Loss: -0.7671, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 30/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8366, Recall: 0.8113, F1: 0.8181\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 31/50 - Train:\n",
      "Loss: -0.7724, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 31/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8177, Precision: 0.8151, Recall: 0.8090, F1: 0.8114\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 32/50 - Train:\n",
      "Loss: -0.7772, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 32/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8385, Precision: 0.8406, Recall: 0.8280, F1: 0.8323\n",
      "Epoch 33/50 - Train:\n",
      "Loss: -0.7810, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 33/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8490, Precision: 0.8517, Recall: 0.8387, F1: 0.8431\n",
      "Epoch 34/50 - Train:\n",
      "Loss: -0.7869, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 34/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8385, Precision: 0.8384, Recall: 0.8287, F1: 0.8323\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 35/50 - Train:\n",
      "Loss: -0.7919, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 35/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8265, Recall: 0.8205, F1: 0.8229\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 36/50 - Train:\n",
      "Loss: -0.7955, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 36/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8125, Precision: 0.8083, Recall: 0.8061, F1: 0.8071\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 37/50 - Train:\n",
      "Loss: -0.8004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 37/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8337, Recall: 0.8130, F1: 0.8190\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 38/50 - Train:\n",
      "Loss: -0.8040, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 38/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8299, Recall: 0.8184, F1: 0.8222\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 39/50 - Train:\n",
      "Loss: -0.8077, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 39/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8229, Precision: 0.8205, Recall: 0.8160, F1: 0.8179\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 40/50 - Train:\n",
      "Loss: -0.8105, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 40/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8333, Precision: 0.8308, Recall: 0.8258, F1: 0.8279\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 41/50 - Train:\n",
      "Loss: -0.8138, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 41/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8281, Precision: 0.8274, Recall: 0.8180, F1: 0.8215\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 42/50 - Train:\n",
      "Loss: -0.8163, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 42/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8229, Precision: 0.8227, Recall: 0.8118, F1: 0.8157\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 43/50 - Train:\n",
      "Loss: -0.8193, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 43/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8229, Precision: 0.8221, Recall: 0.8153, F1: 0.8179\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 0.8489583333333334\n",
      "Test Accuracy: 0.8281, Precision: 0.8279, Recall: 0.8190, F1: 0.8222\n",
      "Fold 5/5\n",
      "Length of train_loader: 782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 0.0290, Accuracy: 0.5923, Precision: 0.4848, Recall: 0.4994, F1: 0.3806\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.5885, Precision: 0.2943, Recall: 0.5000, F1: 0.3705\n",
      "Epoch 2/50 - Train:\n",
      "Loss: -0.0276, Accuracy: 0.6577, Precision: 0.7460, Recall: 0.5813, F1: 0.5380\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7552, Precision: 0.7482, Recall: 0.7473, F1: 0.7478\n",
      "Epoch 3/50 - Train:\n",
      "Loss: -0.1059, Accuracy: 0.7795, Precision: 0.7909, Recall: 0.7480, F1: 0.7564\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8542, Precision: 0.8516, Recall: 0.8456, F1: 0.8482\n",
      "Epoch 4/50 - Train:\n",
      "Loss: -0.1597, Accuracy: 0.7974, Precision: 0.8001, Recall: 0.7743, F1: 0.7816\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8438, Precision: 0.8392, Recall: 0.8502, F1: 0.8413\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 5/50 - Train:\n",
      "Loss: -0.2386, Accuracy: 0.8577, Precision: 0.8542, Recall: 0.8482, F1: 0.8509\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8594, Precision: 0.8597, Recall: 0.8482, F1: 0.8527\n",
      "Epoch 6/50 - Train:\n",
      "Loss: -0.2690, Accuracy: 0.8615, Precision: 0.8590, Recall: 0.8516, F1: 0.8548\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6510, Precision: 0.6974, Recall: 0.6859, F1: 0.6499\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 7/50 - Train:\n",
      "Loss: -0.2915, Accuracy: 0.8577, Precision: 0.8531, Recall: 0.8504, F1: 0.8517\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8646, Precision: 0.8689, Recall: 0.8507, F1: 0.8571\n",
      "Epoch 8/50 - Train:\n",
      "Loss: -0.3213, Accuracy: 0.8692, Precision: 0.8654, Recall: 0.8623, F1: 0.8638\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8177, Precision: 0.8209, Recall: 0.7979, F1: 0.8050\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 9/50 - Train:\n",
      "Loss: -0.3346, Accuracy: 0.8769, Precision: 0.8741, Recall: 0.8691, F1: 0.8714\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.6615, Precision: 0.7040, Recall: 0.6947, F1: 0.6607\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 10/50 - Train:\n",
      "Loss: -0.3751, Accuracy: 0.8974, Precision: 0.8953, Recall: 0.8909, F1: 0.8929\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8125, Precision: 0.8054, Recall: 0.8077, F1: 0.8064\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 11/50 - Train:\n",
      "Loss: -0.3805, Accuracy: 0.8872, Precision: 0.8846, Recall: 0.8804, F1: 0.8824\n",
      "Epoch 11/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7656, Precision: 0.7759, Recall: 0.7812, F1: 0.7653\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 12/50 - Train:\n",
      "Loss: -0.4302, Accuracy: 0.9167, Precision: 0.9152, Recall: 0.9112, F1: 0.9131\n",
      "Epoch 12/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7552, Precision: 0.7663, Recall: 0.7736, F1: 0.7547\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 13/50 - Train:\n",
      "Loss: -0.4517, Accuracy: 0.9321, Precision: 0.9319, Recall: 0.9267, F1: 0.9291\n",
      "Epoch 13/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7760, Precision: 0.7770, Recall: 0.7871, F1: 0.7743\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 14/50 - Train:\n",
      "Loss: -0.5014, Accuracy: 0.9513, Precision: 0.9502, Recall: 0.9483, F1: 0.9493\n",
      "Epoch 14/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.7760, Precision: 0.7884, Recall: 0.7945, F1: 0.7757\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 15/50 - Train:\n",
      "Loss: -0.5635, Accuracy: 0.9782, Precision: 0.9771, Recall: 0.9776, F1: 0.9774\n",
      "Epoch 15/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8229, Precision: 0.8200, Recall: 0.8286, F1: 0.8210\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 16/50 - Train:\n",
      "Loss: -0.5933, Accuracy: 0.9872, Precision: 0.9867, Recall: 0.9867, F1: 0.9867\n",
      "Epoch 16/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8333, Precision: 0.8328, Recall: 0.8203, F1: 0.8250\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 17/50 - Train:\n",
      "Loss: -0.5796, Accuracy: 0.9731, Precision: 0.9738, Recall: 0.9703, F1: 0.9720\n",
      "Epoch 17/50 - Validation:\n",
      "Loss: 0.0000, Accuracy: 0.8594, Precision: 0.8540, Recall: 0.8615, F1: 0.8565\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 0.8645833333333334\n",
      "Test Accuracy: 0.8594, Precision: 0.8533, Recall: 0.8613, F1: 0.8561\n",
      "Average metrics across all folds:\n",
      "Test Accuracy: 0.8273, Precision: 0.8343, Recall: 0.8212, F1: 0.8196\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5705276,
     "sourceId": 9652074,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25022.389189,
   "end_time": "2024-10-05T02:20:29.848670",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-04T19:23:27.459481",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
