{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "id": "d2318109eea87e37"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:50.947611Z",
     "start_time": "2024-10-31T09:29:50.368731Z"
    }
   },
   "source": "!pip install gpytorch",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.24.3)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:52.420165Z",
     "start_time": "2024-10-31T09:29:50.958504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torchvision.models as models\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "from torchvision import transforms\n",
    "from scipy.ndimage import rotate\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "id": "4bce4c846467806e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:52.442556Z",
     "start_time": "2024-10-31T09:29:52.441096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "92570869404a8eec",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GPU Check",
   "id": "94294e86f00a568e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:52.580174Z",
     "start_time": "2024-10-31T09:29:52.487779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(device)"
   ],
   "id": "301c4dda55ab0727",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations Initialization",
   "id": "1b1de66ae19acf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:52.603718Z",
     "start_time": "2024-10-31T09:29:52.589970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "CHANNELS = 1\n",
    "\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "\n",
    "MAX_SLICES = 60\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "INDUCING_POINTS = 128\n",
    "\n",
    "TARGET_LABELS = ['intraparenchymal']\n",
    "\n",
    "MODEL_PATH = 'results/trained_model.pth'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "## Configurations for ViT Model\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 1\n",
    "PATCH_SIZE = 32\n",
    "EMBEDDING_DIM = PATCH_SIZE * PATCH_SIZE * CHANNELS # (P^2 * C) -> (16^2 * 3) \n",
    "NUM_HEADS = 16\n",
    "NUM_LAYERS = 12\n",
    "MLP_SIZE = 3072"
   ],
   "id": "fd8fbbd479c06f26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.468957Z",
     "start_time": "2024-10-31T09:29:52.637956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "ROOT_DIR = '/kaggle/input/' if KAGGLE else None\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-mil-training/'\n",
    "DICOM_DIR = DATA_DIR + 'rsna-mil-training/'\n",
    "CSV_PATH = DATA_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_1000_scan_subset.csv'\n",
    "SLICE_LABEL_PATH = ROOT_DIR + \"sorted_training_dataset_with_labels.csv\" if KAGGLE else './data_analyze/sorted_training_dataset_with_labels.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "# Load patient scan labels\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "patient_slice_labels = pd.read_csv(SLICE_LABEL_PATH)"
   ],
   "id": "1653c98fac2448eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "ceacb6039d724e2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Windowing and Resizing",
   "id": "f24a4fb137bd8296"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.490032Z",
     "start_time": "2024-10-31T09:29:53.475777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    \n",
    "    # bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n",
    "    bsb_img = brain_img\n",
    "    # bsb_img = subdural_img\n",
    "    return bsb_img.astype(np.float16)"
   ],
   "id": "fa48995dd0cf4dbf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocess Slice",
   "id": "24b86e90eb33d0bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.533613Z",
     "start_time": "2024-10-31T09:29:53.517906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n",
    "    # Check if type of slice is dicom or an empty numpy array\n",
    "    if (type(slice) == np.ndarray):\n",
    "        slice = resize(slice, target_size, anti_aliasing=True)\n",
    "        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n",
    "        # return multichannel_slice.astype(np.float16)\n",
    "        return slice.astype(np.float16)\n",
    "    else:\n",
    "        slice = bsb_window(slice)\n",
    "        return slice.astype(np.float16)"
   ],
   "id": "bcd391ef69f65bc8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Dicom Images",
   "id": "543ba260f847b22d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.581489Z",
     "start_time": "2024-10-31T09:29:53.564067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_dicom_folder(folder_path):\n",
    "    slices = []\n",
    "    for filename in sorted(os.listdir(folder_path))[:MAX_SLICES]:  # Limit to MAX_SLICES\n",
    "        if filename.endswith(\".dcm\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            slices.append(ds)\n",
    "            \n",
    "    # Sort slices by images position (z-coordinate) in ascending order\n",
    "    slices = sorted(slices, key=lambda x: float(x.ImagePositionPatient[2]))\n",
    "    \n",
    "    # Pad with black images if necessary\n",
    "    while len(slices) < MAX_SLICES:\n",
    "        slices.append(np.zeros_like(slices[0].pixel_array))\n",
    "    \n",
    "    return slices[:MAX_SLICES]  # Ensure we return exactly MAX_SLICES"
   ],
   "id": "2aaecea664203429",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset and DataLoader",
   "id": "5662f73f129f3336"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Splitting Dataset",
   "id": "95387e2ceb6a1f8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.633420Z",
     "start_time": "2024-10-31T09:29:53.613417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # If any of the hemorrhage indicators is 1, the label is 1, otherwise 0\n",
    "    patient_scan_labels['label'] = patient_scan_labels[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any(axis=1).astype(int)\n",
    "\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['label']\n",
    "\n",
    "    # First, split off the test set\n",
    "    train_val_labels, test_labels = train_test_split(\n",
    "        patient_scan_labels, \n",
    "        test_size=test_size, \n",
    "        stratify=labels, \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Calculate the validation size relative to the train_val set\n",
    "    val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "    # Split the train_val set into train and validation sets\n",
    "    train_labels, val_labels = train_test_split(\n",
    "        train_val_labels, \n",
    "        test_size=val_size_adjusted, \n",
    "        stratify=train_val_labels['label'], \n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    return train_labels, val_labels, test_labels"
   ],
   "id": "5c8365d1b7c962ac",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MIL Data Preparation",
   "id": "1168e0ab7a85fe32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.683353Z",
     "start_time": "2024-10-31T09:29:53.663096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_patient_data(dicom_dir, row, num_instances=12, depth=5):\n",
    "    patient_id = row['patient_id'].replace('ID_', '')\n",
    "    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n",
    "    \n",
    "    folder_name = f\"{patient_id}_{study_instance_uid}\"\n",
    "    folder_path = os.path.join(dicom_dir, folder_name)\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        slices = read_dicom_folder(folder_path)\n",
    "        \n",
    "        # Ensure we have enough slices to create the specified instances\n",
    "        if len(slices) < depth * num_instances:\n",
    "            print(f\"Not enough slices for patient {patient_id}: found {len(slices)}, needed {depth * num_instances}\")\n",
    "            return None, None\n",
    "        \n",
    "        preprocessed_slices = [preprocess_slice(slice) for slice in slices]\n",
    "        \n",
    "        # Stack preprocessed slices into an array\n",
    "        preprocessed_slices = np.stack(preprocessed_slices, axis=0)  # (num_slices, height, width, channels)\n",
    "        \n",
    "        # Reshape to (num_instances, depth, height, width, channels)\n",
    "        # reshaped_slices = preprocessed_slices[:num_instances * depth].reshape(num_instances, depth, *preprocessed_slices.shape[1:])  # (num_instances, depth, height, width, channels)\n",
    "        \n",
    "        # Labeling remains consistent  \n",
    "        label = 1 if row[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].any() else 0\n",
    "        \n",
    "        # return reshaped_slices, label\n",
    "        return preprocessed_slices, label\n",
    "    \n",
    "    else:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return None, None"
   ],
   "id": "17e329207ca4b73d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Generator",
   "id": "650453a0c1cf4859"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.729203Z",
     "start_time": "2024-10-31T09:29:53.711161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        patient_study_instance = row['patient_id'] + '_' + row['study_instance_uid']\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            # Convert label to float32\n",
    "            label = np.array(label, dtype=np.float32)\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.float32), patient_study_instance\n",
    "        else:\n",
    "            return None, None, None  # Handle the case where the folder is not found\n",
    "\n",
    "class TestDatasetGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for testing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels):\n",
    "        self.data_dir = data_dir\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patient_scan_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.patient_scan_labels.iloc[idx]\n",
    "        patient_study_instance = row['patient_id'] + '_' + row['study_instance_uid']\n",
    "        preprocessed_slices, label = process_patient_data(self.data_dir, row)\n",
    "        \n",
    "        if preprocessed_slices is not None:\n",
    "            # Convert the list of numpy arrays to a single numpy array\n",
    "            preprocessed_slices = np.array(preprocessed_slices)  # Convert to numpy array\n",
    "            label = np.array(label, dtype=np.float32)\n",
    "            return torch.tensor(preprocessed_slices, dtype=torch.float32), torch.tensor(label, dtype=torch.float32), patient_study_instance\n",
    "        else:\n",
    "            return None, None, None  # Handle the case where the folder is not found"
   ],
   "id": "e3fde36af773550e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.777833Z",
     "start_time": "2024-10-31T09:29:53.758726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "id": "447417de08ff7a24",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Extraction using Vision Transformer",
   "id": "7566ee41d178f3c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Patch Embedding Layer",
   "id": "ecfe2a81dff370f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.826422Z",
     "start_time": "2024-10-31T09:29:53.806476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class PatchEmbedding(nn.Module):\n",
    "#     def __init__(self, img_size, in_channels, patch_size, embed_dim):\n",
    "#         super(PatchEmbedding, self).__init__()\n",
    "#         self.patch_size = patch_size\n",
    "#         self.embed_dim = embed_dim\n",
    "#         \n",
    "#         self.conv = nn.Conv2d(in_channels=in_channels, out_channels=embed_dim,\n",
    "#                               kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass to create patch embeddings.\n",
    "# \n",
    "#         Args:\n",
    "#             x (torch.Tensor): Input tensor of shape (N, C, H, W).\n",
    "# \n",
    "#         Returns:\n",
    "#             torch.Tensor: Output tensor of shape (N, num_patches, embed_dim).\n",
    "#         \"\"\"\n",
    "#         x = self.conv(x)  # Apply convolution to create patches \n",
    "#         # After convolution: shape (N, embed_dim, H/patch_size, W/patch_size)\n",
    "#         \n",
    "#         x = x.flatten(2)  # Flatten patches into a sequence \n",
    "#         # After flattening: shape (N, embed_dim, num_patches)\n",
    "# \n",
    "#         return x.transpose(1, 2)  # Rearrange dimensions for transformer input \n",
    "#         # Final output shape: (N, num_patches, embed_dim)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size:int, in_channels:int, patch_size:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                              out_channels=embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "\n",
    "        self.img_size = img_size\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size * num_instances, channels, height, width)\n",
    "        if len(x.shape) == 3: # If in_channels = 1\n",
    "            x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv(x)  # Apply convolution to create patches\n",
    "        \n",
    "        # Flatten patches into a sequence of embeddings\n",
    "        batch_size_num_instances = x.shape[0]\n",
    "        num_patches = (self.img_size // self.patch_size) ** 2\n",
    "        \n",
    "        return x.view(batch_size_num_instances, num_patches, self.embed_dim)  # Shape: (batch * instances, num_patches, embed_dim)"
   ],
   "id": "387922eed3f27118",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-Head Self-Attention Layer",
   "id": "3fef8fa8b9dcbcfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.872884Z",
     "start_time": "2024-10-31T09:29:53.854221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        assert type(x) == torch.Tensor, f\"Input to MSA block is not a PyTorch tensor but a {type(x)}\"\n",
    "        attn_output, attn_weights = self.multihead_attn(query=x, # query embeddings\n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        assert type(attn_output) == torch.Tensor, f\"Output of MSA block is not a PyTorch tensor but a {type(attn_output)}\"\n",
    "        return attn_output"
   ],
   "id": "682a09607a93d2d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Multi-Layer Perceptron Layer",
   "id": "d46a6d9d07fcf9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.923207Z",
     "start_time": "2024-10-31T09:29:53.903964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        assert type(x) == torch.Tensor, f\"Input to MLP block is not a PyTorch tensor but a {type(x)}\"\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ],
   "id": "81da7a8e63a3d445",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transformer Encoder Layer",
   "id": "7c3fc28e9f5fbcf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:53.971467Z",
     "start_time": "2024-10-31T09:29:53.951924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "\n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "\n",
    "    # 5. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        msa = self.msa_block(x)\n",
    "        assert type(msa) == torch.Tensor, f\"Output of MSA block is not a PyTorch tensor but a {type(msa)}\"\n",
    "    \n",
    "        x =  msa + x\n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x\n",
    "\n",
    "        return x"
   ],
   "id": "88d948b0cb733d0",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gaussian Process Layer",
   "id": "c6bac8b9687d58fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.019161Z",
     "start_time": "2024-10-31T09:29:54.000044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "id": "1ff17fe6ac031183",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention Layer",
   "id": "9be71066ce2c4529"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.068844Z",
     "start_time": "2024-10-31T09:29:54.048963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.ReLU(),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "id": "7cdaab992bf8214a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Vision Transformer Model",
   "id": "12c64876f3b99fcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.119419Z",
     "start_time": "2024-10-31T09:29:54.097080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "\n",
    "        # 3. Make the image size is divisible by the patch size\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(4 * 60, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(4 * 60, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "\n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(img_size=img_size,\n",
    "                                              in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embed_dim=embedding_dim)\n",
    "\n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "\n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get batch size and number of instances\n",
    "        batch_size, num_instances = x.shape[0], x.shape[1]\n",
    "    \n",
    "        # Create class token embedding and expand it to match the batch size and number of instances\n",
    "        class_token = self.class_embedding.expand(batch_size * num_instances, 1, self.embedding_dim)\n",
    "    \n",
    "        # Process each instance through the patch embedding layer\n",
    "        x = self.patch_embedding(x.view(-1, *x.shape[2:]))  # Flatten instances for patch embedding\n",
    "        assert x.shape[0] == batch_size * num_instances, f\"Number of instances is incorrect: {x.shape[0]} instances, expected {batch_size * num_instances}\"\n",
    "        assert x.shape[1] == self.num_patches, f\"Number of patches is incorrect: {x.shape[1]} patches, expected {self.num_patches}\"\n",
    "        \n",
    "        \n",
    "        # Concatenate class token with patch embeddings\n",
    "        x = torch.cat((class_token, x), dim=1)  # Concatenate along the last dimension\n",
    "        # Shape of x: (batch_size * num_instances, num_patches + 1, embed_dim)\n",
    "    \n",
    "        # Add position embedding\n",
    "        x = self.position_embedding + x\n",
    "    \n",
    "        # Apply dropout\n",
    "        x = self.embedding_dropout(x)\n",
    "    \n",
    "        # Pass through transformer encoder layers\n",
    "        x = self.transformer_encoder(x)\n",
    "    \n",
    "        # Aggregate outputs from all instances (e.g., mean pooling)\n",
    "        x = x.mean(dim=1)\n",
    "        # Shape of x: (batch_size, num_instances, embed_dim)\n",
    "        x = x.view(batch_size, num_instances, -1).mean(dim=1)\n",
    "    \n",
    "        # Run through classifier head\n",
    "        x = self.classifier(x)\n",
    "    \n",
    "        return x.squeeze(-1)\n"
   ],
   "id": "c9bc5a4e50f1590c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "f93da33db5a53161"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Training",
   "id": "d93c0c64ae4f7317"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.166012Z",
     "start_time": "2024-10-31T09:29:54.147286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0 \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch_data, batch_labels, _ in train_loader:\n",
    "        batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "        labels.extend(batch_labels.cpu().numpy())\n",
    "    return total_loss / len(train_loader), predictions, labels"
   ],
   "id": "9a68ec87ba665cdf",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Validation",
   "id": "cada3941afea0fc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.214406Z",
     "start_time": "2024-10-31T09:29:54.195472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, _ in valid_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "    return total_loss / len(valid_loader), predictions, labels"
   ],
   "id": "13c043190d651eb0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Helper Functions for Training",
   "id": "f215854066527ae5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.262859Z",
     "start_time": "2024-10-31T09:29:54.243481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions),\n",
    "        \"recall\": recall_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print performance metrics.\"\"\"\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, learning_rate, device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, \n",
    "                                              steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, val_loader, criterion, device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ],
   "id": "689bdcfa723a8bb4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Evaluation",
   "id": "4178ace8151f02fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.311687Z",
     "start_time": "2024-10-31T09:29:54.292377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, _ in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend((outputs > 0.5).cpu().detach().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(test_loader), predictions, labels"
   ],
   "id": "3690b1f13f879ac3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualize Functions ",
   "id": "e9c6c14953434ea5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ROC_AUC Curve",
   "id": "beb8da42dcd90566"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.359992Z",
     "start_time": "2024-10-31T09:29:54.339839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    # predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    # with torch.no_grad():\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, _ in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.float().to(device)\n",
    "\n",
    "            outputs, attention_weights = model(batch_data)\n",
    "            outputs = outputs.squeeze()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            labels.extend(batch_labels.cpu().numpy())\n",
    "            \n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ],
   "id": "fb72684a00048a41",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Confusion Matrix",
   "id": "610bf1600a5b83dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.410480Z",
     "start_time": "2024-10-31T09:29:54.389820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(model, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, data_loader, device)\n",
    "    \n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "id": "92beb7b92d4ffa1b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main",
   "id": "a7fd9b400896f6fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:54.457411Z",
     "start_time": "2024-10-31T09:29:54.437305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class()\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader)\n",
    "    \n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "id": "e27d9bf06f2d706a",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T09:29:56.393302Z",
     "start_time": "2024-10-31T09:29:54.489189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(mode='train'):\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    # Decrease the number of samples for training\n",
    "    train_labels = train_labels.sample(n=200, random_state=42)\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels)\n",
    "    \n",
    "    # Initialize the model, criterion, and optimizer\n",
    "    model = ViT(img_size=IMG_SIZE, \n",
    "                in_channels=CHANNELS,\n",
    "                patch_size=PATCH_SIZE,\n",
    "                num_transformer_layers=NUM_LAYERS,\n",
    "                embedding_dim=EMBEDDING_DIM,\n",
    "                mlp_size=MLP_SIZE,\n",
    "                num_heads=NUM_HEADS,\n",
    "                num_classes=NUM_CLASSES)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, LEARNING_RATE, device)\n",
    "        \n",
    "        torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    # Load best model\n",
    "    trained_model = model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    \n",
    "    # Evaluate model \n",
    "    test_loss, test_predictions, test_labels = evaluate_model(trained_model, test_loader, criterion, device)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    test_metrics = calculate_metrics(test_predictions, test_labels)\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print_metrics(test_metrics)\n",
    "    # Visualizations \n",
    "    plot_roc_curve(trained_model, test_loader, device)\n",
    "    plot_confusion_matrix(trained_model, test_loader, device)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        # Select only the required columns\n",
    "        required_columns = ['patient_id', 'study_instance_uid', 'label']\n",
    "        temp_test_labels = test_labels[required_columns]\n",
    "        \n",
    "        # Save results\n",
    "        results_df = get_test_results(trained_model, test_loader, temp_test_labels)\n",
    "        results_df.to_csv('results/results.csv', index=False)\n",
    "        print(results_df.head())   \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main(mode='train')"
   ],
   "id": "3fd004ea908cf248",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "layer_norm(): argument 'input' (position 1) must be Tensor, not torch.return_types.max",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 52\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[38;5;28mprint\u001B[39m(results_df\u001B[38;5;241m.\u001B[39mhead())   \n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 52\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 22\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(mode)\u001B[0m\n\u001B[1;32m     19\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mLEARNING_RATE)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 22\u001B[0m     trained_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNUM_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLEARNING_RATE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(trained_model\u001B[38;5;241m.\u001B[39mstate_dict(), MODEL_PATH)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Load best model\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[23], line 32\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, learning_rate, device)\u001B[0m\n\u001B[1;32m     28\u001B[0m best_model_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# Training phase\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m     train_loss, train_predictions, train_labels \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m calculate_metrics(train_predictions, train_labels)\n\u001B[1;32m     34\u001B[0m     print_epoch_stats(epoch, num_epochs, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_loss, train_metrics)\n",
      "Cell \u001B[0;32mIn[21], line 11\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_loader, criterion, optimizer, scheduler, device)\u001B[0m\n\u001B[1;32m      8\u001B[0m batch_data, batch_labels \u001B[38;5;241m=\u001B[39m batch_data\u001B[38;5;241m.\u001B[39mto(device), batch_labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     10\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 11\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, batch_labels)\n\u001B[1;32m     13\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[20], line 90\u001B[0m, in \u001B[0;36mViT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     87\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(batch_size, num_instances, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Run through classifier head\u001B[39;00m\n\u001B[0;32m---> 90\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/normalization.py:202\u001B[0m, in \u001B[0;36mLayerNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 202\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/functional.py:2576\u001B[0m, in \u001B[0;36mlayer_norm\u001B[0;34m(input, normalized_shape, weight, bias, eps)\u001B[0m\n\u001B[1;32m   2572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[1;32m   2573\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   2574\u001B[0m         layer_norm, (\u001B[38;5;28minput\u001B[39m, weight, bias), \u001B[38;5;28minput\u001B[39m, normalized_shape, weight\u001B[38;5;241m=\u001B[39mweight, bias\u001B[38;5;241m=\u001B[39mbias, eps\u001B[38;5;241m=\u001B[39meps\n\u001B[1;32m   2575\u001B[0m     )\n\u001B[0;32m-> 2576\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: layer_norm(): argument 'input' (position 1) must be Tensor, not torch.return_types.max"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Result",
   "id": "61176ae0b6c1a8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a781f3b75a30460e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8c2bbe44a864d0ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff19a5cb99f6153f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
