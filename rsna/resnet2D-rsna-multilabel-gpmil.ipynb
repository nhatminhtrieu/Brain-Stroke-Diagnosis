{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63c7c60f166ff15d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:19.830849Z",
     "start_time": "2025-02-14T05:17:19.828883Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install kagglehub\n",
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"nmtclone/rsna-ich-mil\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)\n",
    "# # Move from src to des\n",
    "# src = path + \"/rsna-ich-mil/\"\n",
    "# dest = \"/root/rsna-ich-mil/\"\n",
    "\n",
    "# mv = \"mv \" + src + \" \" + dest\n",
    "# mv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bdbae65ffe2d4",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c73c783a78358e70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:21.503197Z",
     "start_time": "2025-02-14T05:17:19.872728Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install gpytorch torchsummary iterative-stratification optuna pytorch_metric_learning wandb\n",
    "# !pip install torch pydicom pandas scikit-learn scikit-image numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4c7150606783a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.317302Z",
     "start_time": "2025-02-14T05:17:21.540651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_metric_learning import losses\n",
    "# from torch.cpu.amp import GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, multilabel_confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.mil_resnet import CNN_ATT_GP_Multilabel, CNN_ATT_GP, CNN_ATT_GP_MIML\n",
    "from utils import hard_negative_mining as hnm\n",
    "import gpytorch\n",
    "from layers.gaussian_process import SingletaskGPModel, PGLikelihood\n",
    "from utils.early_stopping import EarlyStoppingForOptimization, EarlyStopping\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23431713493d81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.324046Z",
     "start_time": "2025-02-14T05:17:23.322326Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d29f181db108e",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "## GPU Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95cd3ec1bd08262f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.401458Z",
     "start_time": "2025-02-14T05:17:23.370654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4090 is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14ef24429d9a7c44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.461375Z",
     "start_time": "2025-02-14T05:17:23.438423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a99588107b8a7c",
   "metadata": {},
   "source": [
    "## Seed Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f6a8e4782dd1f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.516898Z",
     "start_time": "2025-02-14T05:17:23.487398Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136a2675514fe22",
   "metadata": {},
   "source": [
    "## Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51c1e5c654a4035b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.561332Z",
     "start_time": "2025-02-14T05:17:23.536181Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "GP_MODEL = config['gp_model']\n",
    "GP_KERNEL = config['kernel_type']\n",
    "MODEL_TYPE = config['model_type']\n",
    "CONTRASTIVE_LEARNING = config['contrastive_learning']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "LEARNING_RATE_NGD = config['learning_rate_ngd']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "LIKELIHOOD_PATH = config['likelihood_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "511e5461c00b6284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.626657Z",
     "start_time": "2025-02-14T05:17:23.585954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote server.\n"
     ]
    }
   ],
   "source": [
    "KAGGLE = os.path.exists(('kaggle/input'))\n",
    "REMOTE_SERVER = os.path.exists(('/workspace/rsna-ich-mil'))\n",
    "ROOT_DIR = None\n",
    "# DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "if KAGGLE:\n",
    "  DATA_DIR = ROOT_DIR + 'rsna-mil-training/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv'\n",
    "elif REMOTE_SERVER:\n",
    "  DATA_DIR = '/root/rsna-ich-mil/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = '/workspace/training_dataset_1150_redundancy.csv'\n",
    "  print('Running on remote server.')\n",
    "else:\n",
    "  DATA_DIR = '../rsna-ich-mil/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = './data_analyze/training_dataset_3_redundancy.csv'\n",
    "\n",
    "# CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_2_redundancy.csv'\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "103520b3370c53c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.658217Z",
     "start_time": "2025-02-14T05:17:23.636156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_spacing</th>\n",
       "      <th>pixel_representation</th>\n",
       "      <th>window_center</th>\n",
       "      <th>window_width</th>\n",
       "      <th>rescale_intercept</th>\n",
       "      <th>rescale_slope</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>selected_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_766be7451.dcm', 'ID_d1d35ed25.dcm', 'ID_4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_005f241d</td>\n",
       "      <td>ID_07e2cf7b4b</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[66.486, 71.701, 76.915, 82.13, 87.344, 92.559...</td>\n",
       "      <td>[2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.3...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_0d5c28287.dcm', 'ID_dc5f5d774.dcm', 'ID_f...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0075b28c</td>\n",
       "      <td>ID_0373dbdd02</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[41.534897, 46.699394, 51.864895, 57.029396, 6...</td>\n",
       "      <td>[2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.7...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>ID_00760731</td>\n",
       "      <td>ID_006a2c59e4</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.25, 10.25, 15.25, 20.25, 25.25, 30.25, 35.2...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00859e11</td>\n",
       "      <td>ID_01f49be39f</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.452, 3.713, 8.877, 14.042, 19.206, 24.371,...</td>\n",
       "      <td>[5.16, 5.16, 5.16, 5.16, 5.16, 5.16, 5.17, 5.1...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00a3b735</td>\n",
       "      <td>ID_065682422f</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.48828125, 0.48828125]', '[0.48828125, 0.4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['[00036, 00036]', '[00036, 00036]', '[00036, ...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[69.9000244, 74.9000244, 79.9000244, 84.900024...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_766be7451.dcm', 'ID_d1d35ed25.dcm', 'ID_4...   \n",
       "1  ['ID_0d5c28287.dcm', 'ID_dc5f5d774.dcm', 'ID_f...   \n",
       "2  ['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...   \n",
       "3  ['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...   \n",
       "4  ['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_005f241d   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0075b28c   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...  ID_00760731   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00859e11   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00a3b735   \n",
       "\n",
       "  study_instance_uid  ...                                      pixel_spacing  \\\n",
       "0      ID_07e2cf7b4b  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "1      ID_0373dbdd02  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "2      ID_006a2c59e4  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "3      ID_01f49be39f  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "4      ID_065682422f  ...  ['[0.48828125, 0.48828125]', '[0.48828125, 0.4...   \n",
       "\n",
       "                                pixel_representation  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       window_center  \\\n",
       "0  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "1  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "2  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "3  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "4  ['[00036, 00036]', '[00036, 00036]', '[00036, ...   \n",
       "\n",
       "                                        window_width  \\\n",
       "0  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "1  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "2  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "3  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "4  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "\n",
       "                                   rescale_intercept  \\\n",
       "0  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "1  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "2  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "3  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "4  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "\n",
       "                                       rescale_slope patient_label  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             1   \n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             1   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "\n",
       "                                              z_axis  \\\n",
       "0  [66.486, 71.701, 76.915, 82.13, 87.344, 92.559...   \n",
       "1  [41.534897, 46.699394, 51.864895, 57.029396, 6...   \n",
       "2  [5.25, 10.25, 15.25, 20.25, 25.25, 30.25, 35.2...   \n",
       "3  [-1.452, 3.713, 8.877, 14.042, 19.206, 24.371,...   \n",
       "4  [69.9000244, 74.9000244, 79.9000244, 84.900024...   \n",
       "\n",
       "                                     slice_thickness  \\\n",
       "0  [2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.3...   \n",
       "1  [2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.7...   \n",
       "2  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "3  [5.16, 5.16, 5.16, 5.16, 5.16, 5.16, 5.17, 5.1...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                    selected_indices  \n",
       "0  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...  \n",
       "1  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...  \n",
       "2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_scan_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455fe5ada6aab91a",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6fa9768c6261c6c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.719225Z",
     "start_time": "2025-02-14T05:17:23.700990Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42, split_strategy='standard'):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset_for_multilabel(patient_scan_labels, test_size=0.15, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal',\n",
    "                                  'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']].values\n",
    "\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(msss.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels = patient_scan_labels.iloc[train_idx]\n",
    "        test_labels = patient_scan_labels.iloc[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(train_labels, labels[train_idx]))\n",
    "\n",
    "        train_labels_final = train_labels.iloc[train_idx]\n",
    "        val_labels = train_labels.iloc[val_idx]\n",
    "\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels_final = patient_scan_labels.iloc[train_idx]\n",
    "        val_labels = patient_scan_labels.iloc[val_idx]\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels_final, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd87c32a435bb6c2",
   "metadata": {},
   "source": [
    "## Dataset Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb6ad0267a437d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.771674Z",
     "start_time": "2025-02-14T05:17:23.748709Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetAugmentor:\n",
    "    def __init__(self, height, width, levels=2, seed=None):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.levels = levels  # Dynamic number of levels\n",
    "        self.seed = seed\n",
    "        self.params = []\n",
    "\n",
    "        # Create different levels of transforms based on the number of levels specified\n",
    "        for i in range(levels):\n",
    "            factor = (i + 1) / levels\n",
    "            self.params.append(\n",
    "                self._create_transform(\n",
    "                    degrees=int(15 * factor),\n",
    "                    translate_range=(0.2 * factor, 0.2 * factor),\n",
    "                    scale_range=(1 - 0.2 * factor, 1 + 0.2 * factor),\n",
    "                    brightness_range=0.2 * factor,\n",
    "                    contrast_range=0.2 * factor,\n",
    "                    blur_sigma_range=(0.5 * factor, 1.0 * factor),\n",
    "                    apply_elastic=(i >= levels // 2),\n",
    "                    level_name=f'level_{i + 1}'\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def _sample_value(self, value_range):\n",
    "        if isinstance(value_range, tuple):\n",
    "            random.seed(self.seed)\n",
    "            return random.uniform(value_range[0], value_range[1])\n",
    "        return value_range\n",
    "\n",
    "    def _create_transform(self, degrees, translate_range, scale_range, brightness_range, contrast_range, blur_sigma_range, apply_elastic, level_name):\n",
    "        print(f\"Creating '{level_name}' transform with parameters:\")\n",
    "        sampled_values = {\n",
    "            \"degrees\": abs(self._sample_value((-degrees, degrees))),\n",
    "            \"translate\": (abs(self._sample_value(translate_range[0])), abs(self._sample_value(translate_range[1]))),\n",
    "            \"scale\": self._sample_value(scale_range),\n",
    "            \"brightness\": self._sample_value(brightness_range),\n",
    "            \"contrast\": self._sample_value(contrast_range),\n",
    "            \"blur_sigma\": self._sample_value(blur_sigma_range),\n",
    "            \"apply_elastic\": apply_elastic\n",
    "        }\n",
    "\n",
    "        print(sampled_values)\n",
    "        return sampled_values\n",
    "\n",
    "    def apply_transform(self, image, level):\n",
    "        params = self.params[level]\n",
    "        transform = self._get_transform(params, channels=image.shape[0])\n",
    "        return transform(image)\n",
    "\n",
    "    def _get_transform(self, params, channels=3):\n",
    "        transform_list = [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomAffine(degrees=params[\"degrees\"], translate=params[\"translate\"], scale=(params[\"scale\"], params[\"scale\"])),\n",
    "            transforms.ColorJitter(brightness=params[\"brightness\"], contrast=params[\"contrast\"]),\n",
    "            transforms.GaussianBlur(kernel_size=(3, 3), sigma=params[\"blur_sigma\"]),\n",
    "            transforms.RandomApply([transforms.ElasticTransform()] if params[\"apply_elastic\"] else [], p=0.3),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(self.height),\n",
    "            transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)])\n",
    "        ]\n",
    "\n",
    "        # if channels == 3:\n",
    "        #     transform_list.extend([\n",
    "        #         transforms.Normalize(mean=[0.16774411, 0.1360026, 0.19076315], std=[0.3101935, 0.27605791, 0.30469988]),\n",
    "        #         transforms.RandomApply([self._channel_shuffle], p=0.3)\n",
    "        #     ])\n",
    "        # elif channels == 1:\n",
    "        #     transform_list.append(transforms.Normalize(mean=[0.16774411], std=[0.3101935]))\n",
    "\n",
    "        return transforms.Compose(transform_list)\n",
    "\n",
    "    def _channel_shuffle(self, tensor):\n",
    "        torch.manual_seed(self.seed)\n",
    "        channels = tensor.shape[0]\n",
    "        indices = torch.randperm(channels)\n",
    "        return tensor[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b152940e94f9db",
   "metadata": {},
   "source": [
    "## Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95714ff6dbf141d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.884672Z",
     "start_time": "2025-02-14T05:17:23.801940Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset_generators.RSNA_Dataset import MedicalScanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "641001eb61fc29a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.907148Z",
     "start_time": "2025-02-14T05:17:23.890663Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e25e1ec079499523",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:23.952713Z",
     "start_time": "2025-02-14T05:17:23.933705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'level_1' transform with parameters:\n",
      "{'degrees': 4.182803953736514, 'translate': (0.2, 0.2), 'scale': 1.0557707193831534, 'brightness': 0.2, 'contrast': 0.2, 'blur_sigma': 0.8197133992289418, 'apply_elastic': True}\n"
     ]
    }
   ],
   "source": [
    "augmentor = DatasetAugmentor(HEIGHT, WIDTH, levels=1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6fa432f40319916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.132130Z",
     "start_time": "2025-02-14T05:17:23.981640Z"
    }
   },
   "outputs": [],
   "source": [
    "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed295d372ecd7b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.156773Z",
     "start_time": "2025-02-14T05:17:24.139996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1b4c7e3de613b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.281711Z",
     "start_time": "2025-02-14T05:17:24.188197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 1, 224, 224]) torch.Size([28]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x, y, z, _ = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2472610f14aba210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.308931Z",
     "start_time": "2025-02-14T05:17:24.291727Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d87d9c",
   "metadata": {},
   "source": [
    "# Utils\n",
    "## Augment batch for CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98ec7fe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.358713Z",
     "start_time": "2025-02-14T05:17:24.336046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Version 2: Avg time taken: 0.05 seconds for 1 augmentation (w ResizedCrop)\n",
    "def augment_batch(batch_images):\n",
    "    if CHANNELS == 1:\n",
    "        batch_size, num_instances, channels, height, width = batch_images.shape\n",
    "    else:\n",
    "        batch_size, num_instances, height, width, channels = batch_images.shape\n",
    "\n",
    "    # Define augmentation transformations using GPU-compatible operations\n",
    "    aug_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((height, width), scale=(0.8, 1.1)),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4)], p=0.6),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]),\n",
    "    ])\n",
    "\n",
    "    # Apply transformations directly on the tensor without converting to PIL\n",
    "    augmented_batch = torch.empty_like(batch_images)  # Preallocate memory for augmented images\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(num_instances):\n",
    "            if CHANNELS == 1:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j])\n",
    "            else:\n",
    "                augmented_batch[i, j] = aug_transform(batch_images[i, j].permute(2, 0, 1)).permute(1, 2, 0)\n",
    "\n",
    "    return augmented_batch.cuda()  # Move the augmented batch to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c720d010",
   "metadata": {},
   "source": [
    "## NTXentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f30e7b3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.408167Z",
     "start_time": "2025-02-14T05:17:24.384034Z"
    }
   },
   "outputs": [],
   "source": [
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae72b56af12ff9e",
   "metadata": {},
   "source": [
    "# Training and Validation\n",
    "## Metrics Calculation\n",
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2dc70b55ffb5cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.457329Z",
     "start_time": "2025-02-14T05:17:24.431455Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro'),\n",
    "        \"auc\": roc_auc_score(labels, predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}, AUC: {metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc76a219f928f46",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfcd24879653952c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.494619Z",
     "start_time": "2025-02-14T05:17:24.477106Z"
    }
   },
   "outputs": [],
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    if CHANNELS == 1:\n",
    "        bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    else:\n",
    "        ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        ce_loss = ce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * kl_divergence\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab14997a6babf5",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fcbd6fe2f3f15450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.551003Z",
     "start_time": "2025-02-14T05:17:24.522598Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Add this import at the top of your file\n",
    "\n",
    "def train_epoch(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, optimizer, variational_ngd_optimizer, scheduler,\n",
    "                scaler, device):\n",
    "    total_loss = 0.0\n",
    "    total_nlls = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    # for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in tqdm(data_loader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if GP_MODEL == 'single_task':\n",
    "            if NUM_CLASSES != 1:\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    variational_ngd_optimizer[i].zero_grad()\n",
    "            else:\n",
    "                variational_ngd_optimizer.zero_grad()\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end': \n",
    "            if CONTRASTIVE_LEARNING:\n",
    "                aug_data_1 = augment_batch(batch_data)\n",
    "                aug_data_2 = augment_batch(batch_data)\n",
    "                batch_data = torch.cat([aug_data_1, aug_data_2], dim=0)\n",
    "                batch_labels = torch.cat([batch_labels, batch_labels], dim=0)\n",
    "                batch_multi_labels = torch.cat([batch_multi_labels, batch_multi_labels], dim=0)\n",
    "                batch_patient_labels = torch.cat([batch_patient_labels, batch_patient_labels], dim=0)\n",
    "            \n",
    "            outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "\n",
    "            if GP_MODEL == 'single_task' and NUM_CLASSES != 1:\n",
    "                loss = 0\n",
    "                gp_loss = 0\n",
    "                ntx_loss = 0 \n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    gp_loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "\n",
    "                if CONTRASTIVE_LEARNING:\n",
    "                    miner_func = hnm.ExamplePairMiner()\n",
    "                    hard_pairs = miner_func(outputs, batch_multi_labels)\n",
    "                    ntx_loss = criterion_cl(outputs, labels=batch_multi_labels, hard_pairs=hard_pairs)\n",
    "                    loss = 0.3 * criterion_bce(outputs, batch_multi_labels) + 0.3 * gp_loss + ntx_loss * 0.4 \n",
    "                else:   \n",
    "                    loss = 0.5 * criterion_bce(outputs, batch_multi_labels) + 0.5 * gp_loss\n",
    "\n",
    "                loss = loss.mean()\n",
    "\n",
    "                probs = [likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)]\n",
    "                probabilities = torch.stack(probs, dim=1)\n",
    "                preds = (probabilities >= THRESHOLD).int()\n",
    "\n",
    "            elif GP_MODEL == 'single_task' and NUM_CLASSES == 1:\n",
    "                if CONTRASTIVE_LEARNING:\n",
    "                    miner_func = hnm.ExamplePairMiner() \n",
    "                    hard_pairs = miner_func(outputs, batch_patient_labels)\n",
    "                    ntx_loss = criterion_cl(outputs, labels=batch_patient_labels, hard_pairs=hard_pairs)\n",
    "                    # loss = 0.3 * criterion_bce(outputs.squeeze(-1), batch_patient_labels) + 0.3 * mlls(gp_outputs, batch_patient_labels) + ntx_loss * 0.4\n",
    "                    loss = ntx_loss * 0.5 + 0.5 * criterion_bce(outputs.squeeze(-1), batch_patient_labels)\n",
    "                    loss = loss.mean()\n",
    "                    loss += -mlls(gp_outputs, batch_patient_labels) * 0.5\n",
    "                else:\n",
    "                    loss = -mlls(gp_outputs, batch_patient_labels) * 0.5 + 0.5 * criterion_bce(outputs.squeeze(-1), batch_patient_labels)\n",
    "                    \n",
    "                loss = loss.mean()\n",
    "                preds = likelihoods(gp_outputs).probs >= THRESHOLD\n",
    "                # preds = (torch.sigmoid(outputs) >= THRESHOLD).int()\n",
    "            else:\n",
    "                if NUM_CLASSES == 1:\n",
    "                    loss = -mlls(gp_outputs, batch_patient_labels) * 0.5 + 0.5 * criterion_bce(outputs.squeeze(-1), batch_patient_labels)\n",
    "                else:\n",
    "                    loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                loss = loss.mean()\n",
    "                preds = (outputs >= THRESHOLD).int()\n",
    "                # preds = (torch.sigmoid(outputs) >= THRESHOLD).int()\n",
    "                preds = likelihoods(gp_outputs).probs >= THRESHOLD\n",
    "\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if GP_MODEL == 'single_task':\n",
    "                if NUM_CLASSES != 1:\n",
    "                    for i in range(NUM_CLASSES):\n",
    "                        variational_ngd_optimizer[i].step()\n",
    "                else:\n",
    "                    variational_ngd_optimizer.step()\n",
    "\n",
    "\n",
    "        if NUM_CLASSES == 1:\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "        else:\n",
    "            labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def validate(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with (torch.inference_mode()):\n",
    "        # for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in tqdm(data_loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    loss = 0\n",
    "                    if NUM_CLASSES != 1:\n",
    "                        for i in range(NUM_CLASSES):\n",
    "                            loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                    \n",
    "                        loss.mean()\n",
    "                        loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                        probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                        preds = (probabilities >= THRESHOLD).int()\n",
    "                    else:\n",
    "                        loss = -mlls(gp_outputs, batch_patient_labels) * 0.5 + 0.5 * criterion_bce(outputs.squeeze(-1), batch_patient_labels)\n",
    "                        # loss = -mlls(gp_outputs, batch_patient_labels)\n",
    "                        loss = loss.mean()\n",
    "                        total_loss += loss.item()\n",
    "                        preds = likelihoods(gp_outputs).probs >= THRESHOLD\n",
    "                        # preds = (torch.sigmoid(outputs) >= THRESHOLD).int()\n",
    "                else:\n",
    "                    if NUM_CLASSES == 1:\n",
    "                        loss = -mlls(gp_outputs, batch_patient_labels) * 0.5 + 0.5 * criterion_bce(outputs.squeeze(-1), batch_patient_labels)\n",
    "                    else:\n",
    "                        loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    loss = loss.mean()\n",
    "                    total_loss += loss.item()\n",
    "                    # preds = (outputs >= THRESHOLD).int()\n",
    "                    preds = (torch.sigmoid(outputs) >= THRESHOLD).int()\n",
    "\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def train_model(model, likelihoods, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate,\n",
    "                device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    # Initialize Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "    if GP_MODEL == 'single_task':\n",
    "        if NUM_CLASSES != 1:\n",
    "            mlls = [gpytorch.mlls.VariationalELBO(likelihoods[i], model.gp_layers[i], num_data=len(train_loader.dataset)) for\n",
    "                    i in range(NUM_CLASSES)]\n",
    "            mlls = [mll.to(device) for mll in mlls]\n",
    "\n",
    "            variational_ngd_optimizer = [\n",
    "                gpytorch.optim.NGD(model.gp_layers[i].variational_parameters(), num_data=len(train_loader.dataset),\n",
    "                                   lr=LEARNING_RATE_NGD) for i in range(NUM_CLASSES)]\n",
    "        else:\n",
    "            mlls = gpytorch.mlls.VariationalELBO(likelihoods, model.gp_layers, num_data=len(train_loader.dataset))\n",
    "            mlls = mlls.to(device)\n",
    "            variational_ngd_optimizer = gpytorch.optim.NGD(model.gp_layers.variational_parameters(), num_data=len(train_loader.dataset),\n",
    "                                      lr=LEARNING_RATE_NGD)\n",
    "    else:\n",
    "        mlls = gpytorch.mlls.VariationalELBO(likelihoods, model.gp_layers, num_data=len(train_loader.dataset))\n",
    "        mlls = mlls.to(device)\n",
    "        variational_ngd_optimizer = None\n",
    "\n",
    "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * num_epochs, 0.75 * num_epochs], gamma=0.1)\n",
    "\n",
    "    # scaler = torch.amp.GradScaler('cuda')\n",
    "    scaler = None\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    best_likelihood_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, likelihoods, train_loader, criterion_cl, criterion_bce,\n",
    "                                                                  mlls, optimizer, variational_ngd_optimizer,\n",
    "                                                                  scheduler, scaler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss,\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "            \"train/auc\": train_metrics[\"auc\"]\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, likelihoods, val_loader, criterion_cl, criterion_bce, mlls,\n",
    "                                                         device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "            \"val/auc\": val_metrics[\"auc\"]\n",
    "        })\n",
    "\n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_metrics[\"auc\"], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['auc'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['auc']\n",
    "            best_model_state = model.state_dict()\n",
    "            best_likelihood_state = likelihoods.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state and best_likelihood_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        likelihoods.load_state_dict(best_likelihood_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model, likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95e72dfb2cda9d",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7888f48ee4094232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.599031Z",
     "start_time": "2025-02-14T05:17:24.575647Z"
    }
   },
   "outputs": [],
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, likelihoods, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    likelihoods = likelihoods.to(device)\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    if NUM_CLASSES > 1:\n",
    "                        probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    else:\n",
    "                        probabilities = likelihoods(gp_outputs).probs\n",
    "                    preds = (probabilities >= THRESHOLD).int()\n",
    "                    # preds = (torch.sigmoid(outputs) >= THRESHOLD).int()\n",
    "                else:\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    preds = (outputs >= THRESHOLD).int()\n",
    "                    probabilities = outputs\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "            probs.extend(probabilities.cpu().detach().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels), np.array(probs)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\",\n",
    "          f\"AUC: {metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de50808ab2ebc7",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1d5f53a2094c6b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.649597Z",
     "start_time": "2025-02-14T05:17:24.623563Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, _ = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    if NUM_CLASSES > 1:\n",
    "                        probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    else:\n",
    "                        probabilities = likelihoods(gp_outputs).probs\n",
    "                    preds = probabilities\n",
    "\n",
    "                else:\n",
    "                    outputs = torch.sigmoid(outputs)\n",
    "                    preds = outputs\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:  # Binary classification\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    if NUM_CLASSES == 1:\n",
    "        fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    else:\n",
    "        class_name = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "        for i in range(NUM_CLASSES):\n",
    "            fpr, tpr, _ = roc_curve(labels[:, i], predictions[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=2,\n",
    "                    label=f'Class {class_name[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# def plot_confusion_matrix(model, likelihoods, data_loader, device):\n",
    "#     \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "#     predictions, labels, _ = evaluate_model(model, likelihoods, data_loader, device)\n",
    "#\n",
    "#     cm = confusion_matrix(labels, predictions)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "#     disp.plot()\n",
    "#     plt.title('Confusion Matrix')\n",
    "#     plt.show()\n",
    "\n",
    "def plot_confusion_matrix(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot confusion matrix for classification tasks.\"\"\"\n",
    "    predictions, labels, _ = evaluate_model(model, likelihoods, data_loader, device)\n",
    "\n",
    "    if NUM_CLASSES > 1:  # Multi-label/multi-class\n",
    "        cm = multilabel_confusion_matrix(labels, predictions)\n",
    "        _, ax = plt.subplots(1, NUM_CLASSES, figsize=(15, 3))\n",
    "        for i in range(NUM_CLASSES):\n",
    "            ConfusionMatrixDisplay(cm[i]).plot(ax=ax[i])\n",
    "            ax[i].set_title(f'Class {i}')\n",
    "    else:  # Binary\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c41b10693f30c",
   "metadata": {},
   "source": [
    "# Model Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ecffe9db8405eaec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.697167Z",
     "start_time": "2025-02-14T05:17:24.675929Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path, params):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class(params)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991e918772ba07c",
   "metadata": {},
   "source": [
    "## NTXentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25461323dcbcca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.726652Z",
     "start_time": "2025-02-14T05:17:24.724138Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68eab105",
   "metadata": {},
   "source": [
    "## Augment For CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a57d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.779337Z",
     "start_time": "2025-02-14T05:17:24.775085Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e1729b50295387",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d96b3d0ff71c9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:17:24.859428Z",
     "start_time": "2025-02-14T05:17:24.827504Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def main(mode='train', use_cv=False, num_folds=5):\n",
    "    # os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    run_name = f\"experiment_{current_time}_{GP_MODEL}_refiner_fc_{PROJECTION_HIDDEN_DIM}_output_{PROJECTION_OUTPUT_DIM}_attention_{ATTENTION_HIDDEN_DIM}_kernel_{GP_KERNEL}_model_{MODEL_TYPE}\"\n",
    "\n",
    "    # Initialize W&B with a specific run name\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\", name=run_name)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    params = {\n",
    "        'channels': CHANNELS,  # Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
    "        'num_classes': NUM_CLASSES,  # Number of output classes for classification\n",
    "        'drop_prob': 0.5,  # Dropout probability\n",
    "        'inducing_points': INDUCING_POINTS,  # Number of inducing points for the Gaussian Process layer\n",
    "        'projection_location': PROJECTION_LOCATION,  # Choose from 'after_resnet', 'after_attention', or 'after_gp'\n",
    "        'projection_hidden_dim': PROJECTION_HIDDEN_DIM,  # Hidden dimension size for the projection head\n",
    "        'projection_output_dim': PROJECTION_OUTPUT_DIM,  # Output dimension size for the projection head\n",
    "        'attention_hidden_dim': ATTENTION_HIDDEN_DIM,  # Hidden dimension size for the attention head\n",
    "        'gp_model': GP_MODEL,\n",
    "        'kernel_type': GP_KERNEL,\n",
    "        'model_type': MODEL_TYPE,\n",
    "        'contrastive_learning': CONTRASTIVE_LEARNING\n",
    "    }\n",
    "\n",
    "    if use_cv == 0:\n",
    "\n",
    "        train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=0.0)\n",
    "        test_labels = pd.read_csv('/workspace/testing_dataset_150_redundancy.csv')\n",
    "        # train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "\n",
    "        train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "        val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "        test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            if GP_MODEL == 'single_task':\n",
    "                model = CNN_ATT_GP_Multilabel(params)\n",
    "                if NUM_CLASSES != 1:\n",
    "                    likelihood = nn.ModuleList([PGLikelihood() for _ in range(NUM_CLASSES)])\n",
    "                else:\n",
    "                    # likelihood = PGLikelihood()\n",
    "                    likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "            elif GP_MODEL == 'multi_task':\n",
    "                model = CNN_ATT_GP_MIML(params)\n",
    "                # likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=NUM_CLASSES)\n",
    "                likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "\n",
    "\n",
    "        criterion_cl = NTXentLoss(0.5)\n",
    "        if NUM_CLASSES == 1:\n",
    "            pos_weights = torch.tensor([2.0]).to(DEVICE)\n",
    "        else:\n",
    "            pos_weights = torch.tensor([2.0] * NUM_CLASSES).to(DEVICE)\n",
    "        criterion_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "        criterion_bce_wll = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        if mode == 'train':\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                wandb.watch(model)  # Watch the model to log gradients and parameters\n",
    "                trained_model, likelihood = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                                            optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "                predictions, labels, _ = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "                metrics = calculate_metrics(predictions, labels)\n",
    "                wandb.log(metrics)\n",
    "                print_metrics(metrics)\n",
    "\n",
    "                plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "                plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "                torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "                torch.save(likelihood.state_dict(), LIKELIHOOD_PATH)\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            if GP_MODEL == 'single_task':\n",
    "                trained_model = load_model(CNN_ATT_GP_Multilabel, MODEL_PATH, params)\n",
    "            elif GP_MODEL == 'multi_task':\n",
    "                trained_model = load_model(CNN_ATT_GP_MIML, MODEL_PATH, params)\n",
    "            else:\n",
    "                trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "            predictions, labels, probs = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "\n",
    "        metrics = calculate_metrics(predictions, labels)\n",
    "        wandb.log(metrics)\n",
    "        print_metrics(metrics)\n",
    "\n",
    "        plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "        plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "\n",
    "\n",
    "        print(\"\\nProbabilities for each row in the test set:\")\n",
    "        for i, prob in enumerate(probs):\n",
    "            if NUM_CLASSES == 1:\n",
    "                is_correct = predictions[i] == labels[i]\n",
    "                print(f\"Row {i+1}: Probs: {prob.item():.4f} | Prediction: {predictions[i]} | Label: {labels[i]} | Result: {'True' if is_correct else 'False'}\")\n",
    "            else:\n",
    "                is_correct = np.array_equal(predictions[i], labels[i])\n",
    "                print(f\"Row {i+1}: Probs: {prob.tolist()} | Prediction: {predictions[i].tolist()} | Label: {labels[i].tolist()} | Result: {'True' if is_correct else 'False'}\")\n",
    "\n",
    "    else:\n",
    "        if NUM_CLASSES == 1:\n",
    "            # Split initial holdout test set first\n",
    "            trainval_labels, test_labels, _ = split_dataset(\n",
    "                patient_scan_labels,\n",
    "                val_size=TEST_SIZE, \n",
    "                test_size=0.0\n",
    "            )\n",
    "            test_labels = pd.read_csv('/workspace/testing_dataset_150_redundancy.csv')\n",
    "        else:\n",
    "            # Split initial holdout test set first\n",
    "            trainval_labels, test_labels, _ = split_dataset_for_multilabel(\n",
    "                patient_scan_labels,\n",
    "                val_size=TEST_SIZE,\n",
    "                test_size=0.0\n",
    "            )\n",
    "        \n",
    "        # Setup CV using trainval data\n",
    "        kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "        fold_metrics = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(trainval_labels)):\n",
    "            print(f\"\\nFold {fold_idx+1}/{num_folds}\")\n",
    "            \n",
    "            # Create fold splits\n",
    "            train_labels_fold = trainval_labels.iloc[train_idx]\n",
    "            val_labels_fold = trainval_labels.iloc[val_idx]\n",
    "\n",
    "            # Create data loaders\n",
    "            train_loader = get_train_loader(dicom_dir, train_labels_fold, TRAIN_BATCH_SIZE)\n",
    "            val_loader = get_train_loader(dicom_dir, val_labels_fold, VALID_BATCH_SIZE)\n",
    "            test_loader = get_test_loader(dicom_dir, test_labels, TEST_BATCH_SIZE)\n",
    "\n",
    "            # Initialize model\n",
    "            if GP_MODEL == 'single_task':\n",
    "                model = CNN_ATT_GP_Multilabel(params)\n",
    "                if NUM_CLASSES != 1:\n",
    "                    likelihood = nn.ModuleList([PGLikelihood() for _ in range(NUM_CLASSES)])\n",
    "                else:\n",
    "                    likelihood = PGLikelihood()\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "            elif GP_MODEL == 'multi_task':\n",
    "                model = CNN_ATT_GP_MIML(params)\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=NUM_CLASSES)\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "\n",
    "            criterion_cl = NTXentLoss(0.5)\n",
    "            if NUM_CLASSES == 1:\n",
    "                pos_weights = torch.tensor([2.0]).to(DEVICE)\n",
    "            else:\n",
    "                pos_weights = torch.tensor([2.0] * NUM_CLASSES).to(DEVICE)\n",
    "            criterion_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "            # Train model\n",
    "            wandb.init(project=\"MIL_Resnet_ICH\", name=f\"{run_name}_fold_{fold_idx+1}\")\n",
    "            wandb.watch(model)\n",
    "            trained_model, likelihood = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                                        optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "            # Evaluate model\n",
    "            predictions, labels, _ = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "            metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "            # Log metrics\n",
    "            wandb.log(metrics)\n",
    "            print_metrics(metrics)\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "            plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "            plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "\n",
    "            # Save model\n",
    "            torch.save(trained_model.state_dict(), f\"{MODEL_PATH}_fold_{fold_idx+1}\")\n",
    "            torch.save(likelihood.state_dict(), f\"{LIKELIHOOD_PATH}_fold_{fold_idx+1}\")\n",
    "            \n",
    "        # Calculate average metrics across all folds\n",
    "        avg_metrics = {}\n",
    "        for metric in fold_metrics[0].keys():\n",
    "            avg_metrics[metric] = np.mean([fold[metric] for fold in fold_metrics])\n",
    "        print(\"\\nAverage Metrics Across All Folds:\")\n",
    "        print_metrics(avg_metrics)\n",
    "        wandb.log(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727df5a4cc7fb68",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81609ad761d85f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T05:24:44.431632Z",
     "start_time": "2025-02-14T05:17:24.896153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:13<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train:\n",
      "Loss: 0.4962, Accuracy: 0.6044, Precision: 0.5732, Recall: 0.5530, F1: 0.5409, AUC: 0.5530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Validation:\n",
      "Loss: 0.2518, Accuracy: 0.7688, Precision: 0.7683, Recall: 0.7564, F1: 0.7597, AUC: 0.7564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train:\n",
      "Loss: 0.2137, Accuracy: 0.7912, Precision: 0.7885, Recall: 0.7717, F1: 0.7773, AUC: 0.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Validation:\n",
      "Loss: 0.2539, Accuracy: 0.7500, Precision: 0.8000, Recall: 0.7273, F1: 0.7253, AUC: 0.7273\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train:\n",
      "Loss: 0.1847, Accuracy: 0.7912, Precision: 0.7841, Recall: 0.7788, F1: 0.7811, AUC: 0.7788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Validation:\n",
      "Loss: 0.6864, Accuracy: 0.6125, Precision: 0.7223, Recall: 0.6474, F1: 0.5893, AUC: 0.6474\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train:\n",
      "Loss: 0.0497, Accuracy: 0.8529, Precision: 0.8508, Recall: 0.8412, F1: 0.8452, AUC: 0.8412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Validation:\n",
      "Loss: 0.2438, Accuracy: 0.8000, Precision: 0.7983, Recall: 0.8050, F1: 0.7985, AUC: 0.8050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train:\n",
      "Loss: -0.0075, Accuracy: 0.8853, Precision: 0.8811, Recall: 0.8802, F1: 0.8807, AUC: 0.8802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Validation:\n",
      "Loss: 0.3310, Accuracy: 0.7375, Precision: 0.7775, Recall: 0.7551, F1: 0.7348, AUC: 0.7551\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train:\n",
      "Loss: -0.1039, Accuracy: 0.9206, Precision: 0.9171, Recall: 0.9181, F1: 0.9176, AUC: 0.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Validation:\n",
      "Loss: 0.2066, Accuracy: 0.8250, Precision: 0.8243, Recall: 0.8285, F1: 0.8243, AUC: 0.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train:\n",
      "Loss: -0.1612, Accuracy: 0.9368, Precision: 0.9355, Recall: 0.9328, F1: 0.9341, AUC: 0.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Validation:\n",
      "Loss: 0.3896, Accuracy: 0.7875, Precision: 0.7875, Recall: 0.7904, F1: 0.7870, AUC: 0.7904\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train:\n",
      "Loss: -0.1980, Accuracy: 0.9456, Precision: 0.9428, Recall: 0.9443, F1: 0.9436, AUC: 0.9443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Validation:\n",
      "Loss: 0.7432, Accuracy: 0.6000, Precision: 0.7457, Recall: 0.6351, F1: 0.5640, AUC: 0.6351\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train:\n",
      "Loss: -0.2219, Accuracy: 0.9574, Precision: 0.9570, Recall: 0.9542, F1: 0.9555, AUC: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Validation:\n",
      "Loss: 0.5061, Accuracy: 0.7375, Precision: 0.7755, Recall: 0.7587, F1: 0.7360, AUC: 0.7587\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train:\n",
      "Loss: -0.1708, Accuracy: 0.9279, Precision: 0.9253, Recall: 0.9248, F1: 0.9251, AUC: 0.9248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Validation:\n",
      "Loss: 0.4460, Accuracy: 0.7500, Precision: 0.7760, Recall: 0.7711, F1: 0.7498, AUC: 0.7711\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train:\n",
      "Loss: -0.2884, Accuracy: 0.9750, Precision: 0.9733, Recall: 0.9749, F1: 0.9741, AUC: 0.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Validation:\n",
      "Loss: 0.1570, Accuracy: 0.8500, Precision: 0.8600, Recall: 0.8395, F1: 0.8445, AUC: 0.8395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train:\n",
      "Loss: -0.2726, Accuracy: 0.9588, Precision: 0.9583, Recall: 0.9560, F1: 0.9571, AUC: 0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Validation:\n",
      "Loss: 0.1509, Accuracy: 0.8438, Precision: 0.8551, Recall: 0.8340, F1: 0.8384, AUC: 0.8340\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train:\n",
      "Loss: -0.3615, Accuracy: 0.9897, Precision: 0.9896, Recall: 0.9890, F1: 0.9893, AUC: 0.9890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Validation:\n",
      "Loss: 0.1906, Accuracy: 0.8250, Precision: 0.8222, Recall: 0.8222, F1: 0.8222, AUC: 0.8222\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:11<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train:\n",
      "Loss: -0.3790, Accuracy: 0.9897, Precision: 0.9880, Recall: 0.9908, F1: 0.9893, AUC: 0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Validation:\n",
      "Loss: 0.4285, Accuracy: 0.8125, Precision: 0.8362, Recall: 0.7980, F1: 0.8025, AUC: 0.7980\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(mode='train', use_cv=True, num_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
