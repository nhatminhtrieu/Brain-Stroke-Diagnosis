{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1408f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kagglehub\n",
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"nmtclone/rsna-ich-mil\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)\n",
    "# # Move from src to des\n",
    "# src = path + \"/rsna-ich-mil/\"\n",
    "# dest = \"/root/rsna-ich-mil/\"\n",
    "\n",
    "# mv = \"mv \" + src + \" \" + dest\n",
    "# mv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d276949d5508e0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:16.048752Z",
     "start_time": "2025-01-18T07:51:14.638698Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /usr/local/lib/python3.10/dist-packages (1.14)\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Collecting iterative-stratification\n",
      "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
      "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (from gpytorch) (0.2.37)\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.15.1)\n",
      "Requirement already satisfied: linear-operator>=0.6 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (0.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.24.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.37)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.6->gpytorch) (2.1.0+cu118)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping->gpytorch) (0.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->linear-operator>=0.6->gpytorch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: torchsummary, iterative-stratification\n",
      "Successfully installed iterative-stratification-0.1.9 torchsummary-1.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting pydicom\n",
      "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Collecting pillow>=10.1 (from scikit-image)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2025.1.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_image-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m213.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2025.1.10-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tifffile, pydicom, pillow, opencv-python, lazy-loader, imageio, scikit-image\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.3.0\n",
      "    Uninstalling Pillow-9.3.0:\n",
      "      Successfully uninstalled Pillow-9.3.0\n",
      "Successfully installed imageio-2.37.0 lazy-loader-0.4 opencv-python-4.11.0.86 pillow-11.1.0 pydicom-3.0.1 scikit-image-0.25.1 tifffile-2025.1.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpytorch torchsummary iterative-stratification optuna\n",
    "!pip install torch pydicom pandas scikit-learn scikit-image numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ef154a52b21180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.560689Z",
     "start_time": "2025-01-18T07:51:16.064590Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_metric_learning import losses\n",
    "# from torch.cpu.amp import GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.mil_resnet import CNN_ATT_GP_Multilabel, CNN_ATT_GP, CNN_ATT_GP_MIML\n",
    "from utils import hard_negative_mining as hnm\n",
    "import gpytorch\n",
    "from layers.gaussian_process import SingletaskGPModel, PGLikelihood\n",
    "from utils.early_stopping import EarlyStoppingForOptimization, EarlyStopping\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883dbd33ec48f4eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.587691Z",
     "start_time": "2025-01-18T07:51:17.586278Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30384497eb03d169",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "## GPU Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227b560b8a74cc42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.659324Z",
     "start_time": "2025-01-18T07:51:17.636654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A40 is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca87a30cd68134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.724650Z",
     "start_time": "2025-01-18T07:51:17.701381Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccf613094b85ac",
   "metadata": {},
   "source": [
    "## Seed Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873b638dce5c401c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.771809Z",
     "start_time": "2025-01-18T07:51:17.747929Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79def387733d3d",
   "metadata": {},
   "source": [
    "## Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b815f3cbcc84d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.810048Z",
     "start_time": "2025-01-18T07:51:17.791955Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "GP_MODEL = config['gp_model']\n",
    "GP_KERNEL = config['kernel_type']\n",
    "MODEL_TYPE = config['model_type']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42cc42e05675da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.156143Z",
     "start_time": "2025-01-18T07:51:17.844421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote server.\n"
     ]
    }
   ],
   "source": [
    "KAGGLE = os.path.exists(('kaggle/input'))\n",
    "REMOTE_SERVER = os.path.exists(('/workspace/rsna-ich-mil'))\n",
    "ROOT_DIR = None\n",
    "# DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "if KAGGLE:\n",
    "  DATA_DIR = ROOT_DIR + 'rsna-mil-training/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv'\n",
    "elif REMOTE_SERVER:\n",
    "  DATA_DIR = '/root/.cache/kagglehub/datasets/nmtclone/rsna-ich-mil/versions/4/rsna-ich-mil/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = '/workspace/Brain-Stroke-Diagnosis/rsna/data_analyze/training_dataset_2_redundancy_1150_for_kaggle.csv'\n",
    "  print('Running on remote server.')\n",
    "else:\n",
    "  DATA_DIR = '../rsna-ich-mil/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = './data_analyze/training_dataset_2_redundancy.csv'\n",
    "\n",
    "# CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_2_redundancy.csv'\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a5deb68c8ee212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.184313Z",
     "start_time": "2025-01-18T07:51:18.162388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>...</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>selected_indices</th>\n",
       "      <th>patient_any</th>\n",
       "      <th>patient_subdural</th>\n",
       "      <th>patient_epidural</th>\n",
       "      <th>patient_intraparenchymal</th>\n",
       "      <th>patient_intraventricular</th>\n",
       "      <th>patient_subarachnoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_766be7451.dcm', 'ID_d1d35ed25.dcm', 'ID_4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_005f241d</td>\n",
       "      <td>ID_07e2cf7b4b</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[66.486, 71.701, 76.915, 82.13, 87.344, 92.559...</td>\n",
       "      <td>[2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.3...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_0d5c28287.dcm', 'ID_dc5f5d774.dcm', 'ID_f...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0075b28c</td>\n",
       "      <td>ID_0373dbdd02</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[41.534897, 46.699394, 51.864895, 57.029396, 6...</td>\n",
       "      <td>[2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.7...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>ID_00760731</td>\n",
       "      <td>ID_006a2c59e4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[5.25, 10.25, 15.25, 20.25, 25.25, 30.25, 35.2...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00859e11</td>\n",
       "      <td>ID_01f49be39f</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.452, 3.713, 8.877, 14.042, 19.206, 24.371,...</td>\n",
       "      <td>[5.16, 5.16, 5.16, 5.16, 5.16, 5.16, 5.17, 5.1...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00a3b735</td>\n",
       "      <td>ID_065682422f</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[69.9000244, 74.9000244, 79.9000244, 84.900024...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_766be7451.dcm', 'ID_d1d35ed25.dcm', 'ID_4...   \n",
       "1  ['ID_0d5c28287.dcm', 'ID_dc5f5d774.dcm', 'ID_f...   \n",
       "2  ['ID_080e55858.dcm', 'ID_ad9ea42be.dcm', 'ID_9...   \n",
       "3  ['ID_6407b752d.dcm', 'ID_bff0001cf.dcm', 'ID_1...   \n",
       "4  ['ID_db255faea.dcm', 'ID_68ba9321f.dcm', 'ID_d...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_005f241d   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0075b28c   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...  ID_00760731   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00859e11   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00a3b735   \n",
       "\n",
       "  study_instance_uid  ... patient_label  \\\n",
       "0      ID_07e2cf7b4b  ...             0   \n",
       "1      ID_0373dbdd02  ...             0   \n",
       "2      ID_006a2c59e4  ...             1   \n",
       "3      ID_01f49be39f  ...             1   \n",
       "4      ID_065682422f  ...             0   \n",
       "\n",
       "                                              z_axis  \\\n",
       "0  [66.486, 71.701, 76.915, 82.13, 87.344, 92.559...   \n",
       "1  [41.534897, 46.699394, 51.864895, 57.029396, 6...   \n",
       "2  [5.25, 10.25, 15.25, 20.25, 25.25, 30.25, 35.2...   \n",
       "3  [-1.452, 3.713, 8.877, 14.042, 19.206, 24.371,...   \n",
       "4  [69.9000244, 74.9000244, 79.9000244, 84.900024...   \n",
       "\n",
       "                                     slice_thickness  \\\n",
       "0  [2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.61, 2.3...   \n",
       "1  [2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.58, 2.7...   \n",
       "2  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "3  [5.16, 5.16, 5.16, 5.16, 5.16, 5.16, 5.17, 5.1...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                    selected_indices patient_any  \\\n",
       "0  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...           0   \n",
       "1  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...           0   \n",
       "2  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...           1   \n",
       "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...           1   \n",
       "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...           0   \n",
       "\n",
       "  patient_subdural patient_epidural patient_intraparenchymal  \\\n",
       "0                0                0                        0   \n",
       "1                0                0                        0   \n",
       "2                1                0                        1   \n",
       "3                0                0                        1   \n",
       "4                0                0                        0   \n",
       "\n",
       "   patient_intraventricular patient_subarachnoid  \n",
       "0                         0                    0  \n",
       "1                         0                    0  \n",
       "2                         1                    1  \n",
       "3                         0                    0  \n",
       "4                         0                    0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_scan_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e439462d55e8b",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2031b17e255c735c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.265261Z",
     "start_time": "2025-01-18T07:51:18.247018Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset_for_multilabel(patient_scan_labels, test_size=0.15, val_size=0.25, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal',\n",
    "                                  'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']].values\n",
    "\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(msss.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels = patient_scan_labels.iloc[train_idx]\n",
    "        test_labels = patient_scan_labels.iloc[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(train_labels, labels[train_idx]))\n",
    "\n",
    "        train_labels_final = train_labels.iloc[train_idx]\n",
    "        val_labels = train_labels.iloc[val_idx]\n",
    "\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels_final = patient_scan_labels.iloc[train_idx]\n",
    "        val_labels = patient_scan_labels.iloc[val_idx]\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels_final, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862775f3577f1ef",
   "metadata": {},
   "source": [
    "## Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f02933b0ad10a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.431472Z",
     "start_time": "2025-01-18T07:51:18.276156Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset_generators.RSNA_Dataset import MedicalScanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950255932ed8b97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.480820Z",
     "start_time": "2025-01-18T07:51:18.464440Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "934d4da31f781dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:20.865026Z",
     "start_time": "2025-01-18T07:51:18.507271Z"
    }
   },
   "outputs": [],
   "source": [
    "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1afebc6c8b190b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:20.886686Z",
     "start_time": "2025-01-18T07:51:20.869263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb16a6208d8e0d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.008869Z",
     "start_time": "2025-01-18T07:51:20.923693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 1, 224, 224]) torch.Size([28]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x, y, z, _ = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beea527737fe7ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.042401Z",
     "start_time": "2025-01-18T07:51:21.026572Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True,\n",
    "                      drop_last=True)\n",
    "\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7396c943d24607",
   "metadata": {},
   "source": [
    "# Training and Validation\n",
    "## Metrics Calculation\n",
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "386345f4492a4646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.086225Z",
     "start_time": "2025-01-18T07:51:21.069736Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e07ba62418d236",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06a6fe162be7161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.140108Z",
     "start_time": "2025-01-18T07:51:21.123373Z"
    }
   },
   "outputs": [],
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    if CHANNELS == 1:\n",
    "        bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    else:\n",
    "        ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        ce_loss = ce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * kl_divergence\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9d852dea260a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c288284804281a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.199768Z",
     "start_time": "2025-01-18T07:51:21.168617Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, optimizer, variational_ngd_optimizer, scheduler,\n",
    "                scaler, device):\n",
    "    total_loss = 0.0\n",
    "    total_nlls = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if GP_MODEL == 'single_task':\n",
    "            for i in range(NUM_CLASSES):\n",
    "                variational_ngd_optimizer[i].zero_grad()\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "\n",
    "            if GP_MODEL == 'single_task':\n",
    "                loss = 0\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                loss.mean()\n",
    "                loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "\n",
    "                probs = [likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)]\n",
    "                probabilities = torch.stack(probs, dim=1)\n",
    "                preds = (probabilities >= 0.5).int()\n",
    "\n",
    "            else:\n",
    "                loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                loss = loss.mean()\n",
    "                preds = (outputs >= 0.5).int()\n",
    "\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if GP_MODEL == 'single_task':\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    variational_ngd_optimizer[i].step()\n",
    "            scheduler.step()\n",
    "\n",
    "        if NUM_CLASSES == 1:\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "        else:\n",
    "            labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def validate(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    loss = 0\n",
    "                    for i in range(NUM_CLASSES):\n",
    "                        loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                    loss.mean()\n",
    "                    loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    loss = loss.mean()\n",
    "                    total_loss += loss.item()\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "\n",
    "\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def train_model(model, likelihoods, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate,\n",
    "                device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    # Initialize Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "    if GP_MODEL == 'single_task':\n",
    "        mlls = [gpytorch.mlls.VariationalELBO(likelihoods[i], model.gp_layers[i], num_data=len(train_loader.dataset)) for\n",
    "                i in range(NUM_CLASSES)]\n",
    "        mlls = [mll.to(device) for mll in mlls]\n",
    "\n",
    "        variational_ngd_optimizer = [\n",
    "            gpytorch.optim.NGD(model.gp_layers[i].variational_parameters(), num_data=len(train_loader.dataset),\n",
    "                               lr=0.01) for i in range(NUM_CLASSES)]\n",
    "    else:\n",
    "        mlls = gpytorch.mlls.VariationalELBO(likelihoods, model.gp_layers, num_data=len(train_loader.dataset))\n",
    "        mlls = mlls.to(device)\n",
    "        variational_ngd_optimizer = None\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * num_epochs, 0.75 * num_epochs], gamma=0.1)\n",
    "\n",
    "    # scaler = torch.amp.GradScaler('cuda')\n",
    "    scaler = None\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, likelihoods, train_loader, criterion_cl, criterion_bce,\n",
    "                                                                  mlls, optimizer, variational_ngd_optimizer,\n",
    "                                                                  scheduler, scaler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss,\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, likelihoods, val_loader, criterion_cl, criterion_bce, mlls,\n",
    "                                                         device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_metrics[\"accuracy\"], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0299f417b29b1",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e193f0efccd4b5a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.246784Z",
     "start_time": "2025-01-18T07:51:21.226860Z"
    }
   },
   "outputs": [],
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, likelihoods, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8615707218fcdd04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.298173Z",
     "start_time": "2025-01-18T07:51:21.276270Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, _ = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:  # Binary classification\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, likelihoods, data_loader, device)\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7348e9d5406c83",
   "metadata": {},
   "source": [
    "# Model Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3875223d716184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.344368Z",
     "start_time": "2025-01-18T07:51:21.324483Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path, params):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class(params)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fdb7f6cd302d7",
   "metadata": {},
   "source": [
    "## NTXentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e681882fb0a1ef75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.395638Z",
     "start_time": "2025-01-18T07:51:21.372160Z"
    }
   },
   "outputs": [],
   "source": [
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e00d1173682fbb",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a3e7980201a6cc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.446287Z",
     "start_time": "2025-01-18T07:51:21.422533Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def main(mode='train'):\n",
    "    # os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    run_name = f\"experiment_{current_time}_{GP_MODEL}_refiner_fc_{PROJECTION_HIDDEN_DIM}_output_{PROJECTION_OUTPUT_DIM}_attention_{ATTENTION_HIDDEN_DIM}_kernel_{GP_KERNEL}_model_{MODEL_TYPE}\"\n",
    "\n",
    "    # Initialize W&B with a specific run name\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\", name=run_name)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    # train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    # test_labels = pd.read_csv('./data_analyze/testing_dataset_150_redundancy.csv')\n",
    "    train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    params = {\n",
    "        'channels': CHANNELS,  # Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
    "        'num_classes': NUM_CLASSES,  # Number of output classes for classification\n",
    "        'drop_prob': 0.5,  # Dropout probability\n",
    "        'inducing_points': INDUCING_POINTS,  # Number of inducing points for the Gaussian Process layer\n",
    "        'projection_location': PROJECTION_LOCATION,  # Choose from 'after_resnet', 'after_attention', or 'after_gp'\n",
    "        'projection_hidden_dim': PROJECTION_HIDDEN_DIM,  # Hidden dimension size for the projection head\n",
    "        'projection_output_dim': PROJECTION_OUTPUT_DIM,  # Output dimension size for the projection head\n",
    "        'attention_hidden_dim': ATTENTION_HIDDEN_DIM,  # Hidden dimension size for the attention head\n",
    "        'gp_model': GP_MODEL,\n",
    "        'kernel_type': GP_KERNEL,\n",
    "        'model_type': MODEL_TYPE\n",
    "    }\n",
    "\n",
    "    if TRAINING_TYPE == 'end_to_end':\n",
    "        # Instantiate the CNN_GP_ATT model with the specified parameters\n",
    "        if NUM_CLASSES == 1:\n",
    "            model = CNN_ATT_GP(params)\n",
    "            likelihood = PGLikelihood()\n",
    "            optimizer = optim.Adam([\n",
    "                {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "            ])\n",
    "        else:\n",
    "            if GP_MODEL == 'single_task':\n",
    "                model = CNN_ATT_GP_Multilabel(params)\n",
    "                likelihood = nn.ModuleList([PGLikelihood() for _ in range(NUM_CLASSES)])\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "            else:\n",
    "                model = CNN_ATT_GP_MIML(params)\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=NUM_CLASSES)\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "\n",
    "\n",
    "    criterion_cl = NTXentLoss(0.5)\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()\n",
    "    if GP_MODEL == 'single_task':\n",
    "        if NUM_CLASSES == 1:\n",
    "            pos_weights = torch.tensor([5.0]).to(DEVICE)\n",
    "        else:\n",
    "            pos_weights = torch.tensor([5.0] * NUM_CLASSES).to(DEVICE)\n",
    "        criterion_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    criterion_bce_wll = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if mode == 'train':\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            wandb.watch(model)  # Watch the model to log gradients and parameters\n",
    "            trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                                        optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "            predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "            metrics = calculate_metrics(predictions, labels)\n",
    "            wandb.log(metrics)\n",
    "            print_metrics(metrics)\n",
    "\n",
    "            # plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "            # plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "            torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    # if TRAINING_TYPE == 'end_to_end':\n",
    "    #     trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "    #     predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    #\n",
    "    # metrics = calculate_metrics(predictions, labels)\n",
    "    # wandb.log(metrics)\n",
    "    # print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed10b588911041",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1498cc78442856e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:52.962642Z",
     "start_time": "2025-01-18T07:51:21.478343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuynhsikha2003\u001b[0m (\u001b[33mhuynhsikha2003-i-h-c-qu-c-gia-tp-hcm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Brain-Stroke-Diagnosis/rsna/wandb/run-20250130_181458-rairb813</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/rairb813' target=\"_blank\">experiment_20250130_1814_single_task_refiner_fc_256_output_128_attention_512_kernel_rbf_model_resnet18</a></strong> to <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/rairb813' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/rairb813</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 66.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: -0.8241, Accuracy: 0.5292, Precision: 0.1366, Recall: 0.0190, F1: 0.0333\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: -1.0389, Accuracy: 0.6000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "Epoch 2/50 - Train:\n",
      "Loss: -1.2229, Accuracy: 0.4681, Precision: 0.2188, Recall: 0.0822, F1: 0.0957\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: -1.3173, Accuracy: 0.4000, Precision: 0.1925, Recall: 0.1606, F1: 0.1670\n",
      "EarlyStopping counter: 1 out of 20\n",
      "Epoch 3/50 - Train:\n",
      "Loss: -1.6747, Accuracy: 0.5083, Precision: 0.5416, Recall: 0.2023, F1: 0.2591\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: -1.4757, Accuracy: 0.4500, Precision: 0.4338, Recall: 0.4589, F1: 0.4425\n",
      "EarlyStopping counter: 2 out of 20\n",
      "Epoch 4/50 - Train:\n",
      "Loss: -1.9190, Accuracy: 0.5208, Precision: 0.5041, Recall: 0.3460, F1: 0.4019\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: -0.8504, Accuracy: 0.2792, Precision: 0.3604, Recall: 0.6515, F1: 0.4587\n",
      "EarlyStopping counter: 3 out of 20\n",
      "Epoch 5/50 - Train:\n",
      "Loss: -2.3637, Accuracy: 0.5736, Precision: 0.5307, Recall: 0.4015, F1: 0.4426\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: -0.6331, Accuracy: 0.2667, Precision: 0.2869, Recall: 0.5652, F1: 0.3785\n",
      "EarlyStopping counter: 4 out of 20\n",
      "Epoch 6/50 - Train:\n",
      "Loss: -2.6122, Accuracy: 0.5792, Precision: 0.4747, Recall: 0.4137, F1: 0.4410\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: -1.6382, Accuracy: 0.3750, Precision: 0.4073, Recall: 0.3955, F1: 0.3889\n",
      "EarlyStopping counter: 5 out of 20\n",
      "Epoch 7/50 - Train:\n",
      "Loss: -2.8936, Accuracy: 0.6028, Precision: 0.5855, Recall: 0.5141, F1: 0.5317\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: -2.1909, Accuracy: 0.5708, Precision: 0.5310, Recall: 0.4727, F1: 0.4768\n",
      "EarlyStopping counter: 6 out of 20\n",
      "Epoch 8/50 - Train:\n",
      "Loss: -3.0461, Accuracy: 0.6264, Precision: 0.6212, Recall: 0.5492, F1: 0.5794\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: -2.0948, Accuracy: 0.5375, Precision: 0.5084, Recall: 0.4450, F1: 0.4275\n",
      "EarlyStopping counter: 7 out of 20\n",
      "Epoch 9/50 - Train:\n",
      "Loss: -3.2263, Accuracy: 0.6653, Precision: 0.6549, Recall: 0.5856, F1: 0.6152\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: -0.6121, Accuracy: 0.2167, Precision: 0.3824, Recall: 0.6206, F1: 0.4480\n",
      "EarlyStopping counter: 8 out of 20\n",
      "Epoch 10/50 - Train:\n",
      "Loss: -3.4219, Accuracy: 0.6583, Precision: 0.6427, Recall: 0.6082, F1: 0.6242\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: -2.3486, Accuracy: 0.6125, Precision: 0.6032, Recall: 0.4138, F1: 0.4595\n",
      "Epoch 11/50 - Train:\n",
      "Loss: -3.6199, Accuracy: 0.7333, Precision: 0.6932, Recall: 0.6473, F1: 0.6679\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa15a462b988a19",
   "metadata": {},
   "source": [
    "# Parse patients labels for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751f98b2c61a964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:53.120646Z",
     "start_time": "2025-01-18T22:06:53.059517Z"
    }
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "#\n",
    "# df = pd.read_csv('./data_analyze/training_dataset_1_redundancy.csv')\n",
    "# # Parsing string columns into lists\n",
    "# columns_to_parse = ['any', 'subdural', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid']\n",
    "# for col in columns_to_parse:\n",
    "#     df[col] = df[col].apply(ast.literal_eval)\n",
    "#\n",
    "# # Creating new columns with prefix 'patient_' based on the parsed data\n",
    "# for col in columns_to_parse:\n",
    "#     df[f'patient_{col}'] = df[col].apply(lambda x: int(any(x)))\n",
    "# # Save to csv\n",
    "# df.to_csv('./data_analyze/training_dataset_2_redundancy.csv', index=False)\n",
    "#\n",
    "# # Displaying the updated DataFrame\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90589331e42a2436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:53.768929Z",
     "start_time": "2025-01-18T22:06:53.298540Z"
    }
   },
   "outputs": [],
   "source": [
    "patient_scan_labels = pd.read_csv('./data_analyze/training_dataset_2_redundancy.csv', nrows=1150)\n",
    "\n",
    "train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "\n",
    "# Define new columns for hemorrhage types (these should be in your original DataFrame)\n",
    "hemorrhage_columns = ['patient_any', 'patient_subdural', 'patient_epidural', 'patient_intraparenchymal', 'patient_intraventricular', 'patient_subarachnoid']\n",
    "\n",
    "# Function to count labels and unique patients\n",
    "def count_hemorrhage_types(data):\n",
    "    counts = {col: data[col].sum() for col in hemorrhage_columns}\n",
    "    unique_patients = {col: data[data[col] == 1].shape[0] for col in hemorrhage_columns}\n",
    "    return counts, unique_patients\n",
    "\n",
    "# Count for training set\n",
    "train_counts, train_unique_patients = count_hemorrhage_types(train_labels)\n",
    "\n",
    "# Count for validation set\n",
    "val_counts, val_unique_patients = count_hemorrhage_types(val_labels)\n",
    "\n",
    "# Count for test set\n",
    "test_counts, test_unique_patients = count_hemorrhage_types(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406bd06ed83e5f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:53.800056Z",
     "start_time": "2025-01-18T22:06:53.779854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hemorrhage Type</th>\n",
       "      <th>Train Counts</th>\n",
       "      <th>Validation Counts</th>\n",
       "      <th>Test Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient_any</td>\n",
       "      <td>5626</td>\n",
       "      <td>1876</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient_subdural</td>\n",
       "      <td>2417</td>\n",
       "      <td>803</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patient_epidural</td>\n",
       "      <td>223</td>\n",
       "      <td>74</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient_intraparenchymal</td>\n",
       "      <td>3350</td>\n",
       "      <td>1117</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patient_intraventricular</td>\n",
       "      <td>2338</td>\n",
       "      <td>779</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>patient_subarachnoid</td>\n",
       "      <td>2464</td>\n",
       "      <td>822</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Hemorrhage Type  Train Counts  Validation Counts  Test Counts\n",
       "0               patient_any          5626               1876         1324\n",
       "1          patient_subdural          2417                803          561\n",
       "2          patient_epidural           223                 74           52\n",
       "3  patient_intraparenchymal          3350               1117          788\n",
       "4  patient_intraventricular          2338                779          550\n",
       "5      patient_subarachnoid          2464                822          580"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the summary data for the table\n",
    "summary_data = {\n",
    "    'Hemorrhage Type': ['patient_any',\n",
    "                        'patient_subdural',\n",
    "                        'patient_epidural',\n",
    "                        'patient_intraparenchymal',\n",
    "                        'patient_intraventricular',\n",
    "                        'patient_subarachnoid'],\n",
    "\n",
    "    # Counts from each dataset\n",
    "    'Train Counts': [train_counts[key] for key in train_counts],\n",
    "\n",
    "    # Validation counts\n",
    "    'Validation Counts': [val_counts[key] for key in val_counts],\n",
    "\n",
    "    # Test counts\n",
    "    'Test Counts': [test_counts[key] for key in test_counts],\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the summary table\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display the summary table\n",
    "summary_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
