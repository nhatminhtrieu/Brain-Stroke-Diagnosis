{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1408f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:53.120076Z",
     "start_time": "2025-02-05T15:34:53.117746Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install kagglehub\n",
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"nmtclone/rsna-ich-mil\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)\n",
    "# # Move from src to des\n",
    "# src = path + \"/rsna-ich-mil/\"\n",
    "# dest = \"/root/rsna-ich-mil/\"\n",
    "\n",
    "# mv = \"mv \" + src + \" \" + dest\n",
    "# mv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d276949d5508e0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:54.747578Z",
     "start_time": "2025-02-05T15:34:53.167903Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /usr/local/lib/python3.10/dist-packages (1.14)\n",
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
      "Requirement already satisfied: pytorch_metric_learning in /usr/local/lib/python3.10/dist-packages (2.8.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.6)\n",
      "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (from gpytorch) (0.2.37)\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.15.1)\n",
      "Requirement already satisfied: linear-operator>=0.6 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (0.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.24.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.37)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_metric_learning) (2.1.0+cu118)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (2.1.0)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping->gpytorch) (0.1.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch_metric_learning) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: pydicom in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (11.1.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2025.1.10)\n",
      "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gpytorch torchsummary iterative-stratification optuna pytorch_metric_learning wandb\n",
    "!pip install torch pydicom pandas scikit-learn scikit-image numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ef154a52b21180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:56.455049Z",
     "start_time": "2025-02-05T15:34:54.781677Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_metric_learning import losses\n",
    "# from torch.cpu.amp import GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models.mil_resnet import CNN_ATT_GP_Multilabel, CNN_ATT_GP, CNN_ATT_GP_MIML\n",
    "from utils import hard_negative_mining as hnm\n",
    "import gpytorch\n",
    "from layers.gaussian_process import SingletaskGPModel, PGLikelihood\n",
    "from utils.early_stopping import EarlyStoppingForOptimization, EarlyStopping\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883dbd33ec48f4eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:56.460899Z",
     "start_time": "2025-02-05T15:34:56.459340Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30384497eb03d169",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "## GPU Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b560b8a74cc42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:56.538096Z",
     "start_time": "2025-02-05T15:34:56.510064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA A40 is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca87a30cd68134e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:56.606122Z",
     "start_time": "2025-02-05T15:34:56.579747Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccf613094b85ac",
   "metadata": {},
   "source": [
    "## Seed Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873b638dce5c401c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:56.660870Z",
     "start_time": "2025-02-05T15:34:56.631919Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79def387733d3d",
   "metadata": {},
   "source": [
    "## Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b815f3cbcc84d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:56.712853Z",
     "start_time": "2025-02-05T15:34:56.682209Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "GP_MODEL = config['gp_model']\n",
    "GP_KERNEL = config['kernel_type']\n",
    "MODEL_TYPE = config['model_type']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42cc42e05675da7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:57.044812Z",
     "start_time": "2025-02-05T15:34:56.737051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote server.\n"
     ]
    }
   ],
   "source": [
    "KAGGLE = os.path.exists(('kaggle/input'))\n",
    "REMOTE_SERVER = os.path.exists(('/workspace/rsna-ich-mil'))\n",
    "ROOT_DIR = None\n",
    "# DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "if KAGGLE:\n",
    "  DATA_DIR = ROOT_DIR + 'rsna-mil-training/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv'\n",
    "elif REMOTE_SERVER:\n",
    "  DATA_DIR = '/root/rsna-ich-mil/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = '/workspace/Brain-Stroke-Diagnosis/rsna/data_analyze/training_dataset_2_redundancy.csv'\n",
    "  print('Running on remote server.')\n",
    "else:\n",
    "  DATA_DIR = '../rsna-ich-mil/'\n",
    "  DICOM_DIR = DATA_DIR\n",
    "  CSV_PATH = './data_analyze/training_dataset_2_redundancy.csv'\n",
    "\n",
    "# CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_2_redundancy.csv'\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5deb68c8ee212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:57.072964Z",
     "start_time": "2025-02-05T15:34:57.051911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>...</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>selected_indices</th>\n",
       "      <th>patient_any</th>\n",
       "      <th>patient_subdural</th>\n",
       "      <th>patient_epidural</th>\n",
       "      <th>patient_intraparenchymal</th>\n",
       "      <th>patient_intraventricular</th>\n",
       "      <th>patient_subarachnoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0002cd41</td>\n",
       "      <td>ID_66929e09d4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[38.484, 43.517, 48.549, 53.582, 58.614, 63.64...</td>\n",
       "      <td>[2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00054f3f</td>\n",
       "      <td>ID_8a449ae31b</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[71.9000244, 76.9000244, 81.9000244, 86.900024...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0006d192</td>\n",
       "      <td>ID_25690b4725</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[41.921, 49.421, 56.921, 64.421, 71.921, 79.42...</td>\n",
       "      <td>[3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00086119</td>\n",
       "      <td>ID_fdde2979b0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.556, 40.757, 45.959, 51.16, 56.362, 61.563...</td>\n",
       "      <td>[2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_000e5623</td>\n",
       "      <td>ID_9a4be35b9a</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...   \n",
       "1  ['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...   \n",
       "2  ['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...   \n",
       "3  ['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...   \n",
       "4  ['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0002cd41   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00054f3f   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0006d192   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00086119   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_000e5623   \n",
       "\n",
       "  study_instance_uid  ... patient_label  \\\n",
       "0      ID_66929e09d4  ...             0   \n",
       "1      ID_8a449ae31b  ...             0   \n",
       "2      ID_25690b4725  ...             0   \n",
       "3      ID_fdde2979b0  ...             0   \n",
       "4      ID_9a4be35b9a  ...             0   \n",
       "\n",
       "                                              z_axis  \\\n",
       "0  [38.484, 43.517, 48.549, 53.582, 58.614, 63.64...   \n",
       "1  [71.9000244, 76.9000244, 81.9000244, 86.900024...   \n",
       "2  [41.921, 49.421, 56.921, 64.421, 71.921, 79.42...   \n",
       "3  [35.556, 40.757, 45.959, 51.16, 56.362, 61.563...   \n",
       "4  [272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...   \n",
       "\n",
       "                                     slice_thickness  \\\n",
       "0  [2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...   \n",
       "1  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "2  [3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...   \n",
       "3  [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                    selected_indices patient_any  \\\n",
       "0  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...           0   \n",
       "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...           0   \n",
       "2  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...           0   \n",
       "3  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...           0   \n",
       "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...           0   \n",
       "\n",
       "  patient_subdural patient_epidural patient_intraparenchymal  \\\n",
       "0                0                0                        0   \n",
       "1                0                0                        0   \n",
       "2                0                0                        0   \n",
       "3                0                0                        0   \n",
       "4                0                0                        0   \n",
       "\n",
       "   patient_intraventricular patient_subarachnoid  \n",
       "0                         0                    0  \n",
       "1                         0                    0  \n",
       "2                         0                    0  \n",
       "3                         0                    0  \n",
       "4                         0                    0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_scan_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e439462d55e8b",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2031b17e255c735c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:57.135062Z",
     "start_time": "2025-02-05T15:34:57.117005Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset_for_multilabel(patient_scan_labels, test_size=0.15, val_size=0.25, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal',\n",
    "                                  'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']].values\n",
    "\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(msss.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels = patient_scan_labels.iloc[train_idx]\n",
    "        test_labels = patient_scan_labels.iloc[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(train_labels, labels[train_idx]))\n",
    "\n",
    "        train_labels_final = train_labels.iloc[train_idx]\n",
    "        val_labels = train_labels.iloc[val_idx]\n",
    "\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels_final = patient_scan_labels.iloc[train_idx]\n",
    "        val_labels = patient_scan_labels.iloc[val_idx]\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels_final, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862775f3577f1ef",
   "metadata": {},
   "source": [
    "## Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6f02933b0ad10a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:57.250518Z",
     "start_time": "2025-02-05T15:34:57.163836Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset_generators.RSNA_Dataset import MedicalScanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "950255932ed8b97f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:57.292382Z",
     "start_time": "2025-02-05T15:34:57.271472Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "934d4da31f781dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:59.811090Z",
     "start_time": "2025-02-05T15:34:57.338107Z"
    }
   },
   "outputs": [],
   "source": [
    "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1afebc6c8b190b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:59.835373Z",
     "start_time": "2025-02-05T15:34:59.818430Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21735"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb16a6208d8e0d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:59.943047Z",
     "start_time": "2025-02-05T15:34:59.866736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 1, 224, 224]) torch.Size([28]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "x, y, z, _ = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beea527737fe7ca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:34:59.968335Z",
     "start_time": "2025-02-05T15:34:59.951532Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True,\n",
    "                      drop_last=True)\n",
    "\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7396c943d24607",
   "metadata": {},
   "source": [
    "# Training and Validation\n",
    "## Metrics Calculation\n",
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "386345f4492a4646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.022029Z",
     "start_time": "2025-02-05T15:35:00.004880Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e07ba62418d236",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e06a6fe162be7161",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.066390Z",
     "start_time": "2025-02-05T15:35:00.048263Z"
    }
   },
   "outputs": [],
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    if CHANNELS == 1:\n",
    "        bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    else:\n",
    "        ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        ce_loss = ce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * kl_divergence\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9d852dea260a",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c288284804281a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.130411Z",
     "start_time": "2025-02-05T15:35:00.106942Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Add this import at the top of your file\n",
    "\n",
    "def train_epoch(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, optimizer, variational_ngd_optimizer, scheduler,\n",
    "                scaler, device):\n",
    "    total_loss = 0.0\n",
    "    total_nlls = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    # for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in tqdm(data_loader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if GP_MODEL == 'single_task':\n",
    "            for i in range(NUM_CLASSES):\n",
    "                variational_ngd_optimizer[i].zero_grad()\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "\n",
    "            if GP_MODEL == 'single_task':\n",
    "                loss = 0\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                loss.mean()\n",
    "                loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "\n",
    "                probs = [likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)]\n",
    "                probabilities = torch.stack(probs, dim=1)\n",
    "                preds = (probabilities >= 0.5).int()\n",
    "\n",
    "            else:\n",
    "                loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                loss = loss.mean()\n",
    "                preds = (outputs >= 0.5).int()\n",
    "\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if GP_MODEL == 'single_task':\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    variational_ngd_optimizer[i].step()\n",
    "            scheduler.step()\n",
    "\n",
    "        if NUM_CLASSES == 1:\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "        else:\n",
    "            labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def validate(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in tqdm(data_loader):\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    loss = 0\n",
    "                    for i in range(NUM_CLASSES):\n",
    "                        loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                    loss.mean()\n",
    "                    loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    loss = loss.mean()\n",
    "                    total_loss += loss.item()\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "\n",
    "\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def train_model(model, likelihoods, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate,\n",
    "                device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    # Initialize Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=20, verbose=True)\n",
    "\n",
    "    if GP_MODEL == 'single_task':\n",
    "        mlls = [gpytorch.mlls.VariationalELBO(likelihoods[i], model.gp_layers[i], num_data=len(train_loader.dataset)) for\n",
    "                i in range(NUM_CLASSES)]\n",
    "        mlls = [mll.to(device) for mll in mlls]\n",
    "\n",
    "        variational_ngd_optimizer = [\n",
    "            gpytorch.optim.NGD(model.gp_layers[i].variational_parameters(), num_data=len(train_loader.dataset),\n",
    "                               lr=0.01) for i in range(NUM_CLASSES)]\n",
    "    else:\n",
    "        mlls = gpytorch.mlls.VariationalELBO(likelihoods, model.gp_layers, num_data=len(train_loader.dataset))\n",
    "        mlls = mlls.to(device)\n",
    "        variational_ngd_optimizer = None\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * num_epochs, 0.75 * num_epochs], gamma=0.1)\n",
    "\n",
    "    # scaler = torch.amp.GradScaler('cuda')\n",
    "    scaler = None\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, likelihoods, train_loader, criterion_cl, criterion_bce,\n",
    "                                                                  mlls, optimizer, variational_ngd_optimizer,\n",
    "                                                                  scheduler, scaler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss,\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, likelihoods, val_loader, criterion_cl, criterion_bce, mlls,\n",
    "                                                         device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_metrics[\"f1\"], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0299f417b29b1",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e193f0efccd4b5a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.173178Z",
     "start_time": "2025-02-05T15:35:00.151232Z"
    }
   },
   "outputs": [],
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, likelihoods, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8615707218fcdd04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.220474Z",
     "start_time": "2025-02-05T15:35:00.199338Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, _ = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:  # Binary classification\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, likelihoods, data_loader, device)\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7348e9d5406c83",
   "metadata": {},
   "source": [
    "# Model Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3875223d716184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.267212Z",
     "start_time": "2025-02-05T15:35:00.247367Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path, params):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class(params)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fdb7f6cd302d7",
   "metadata": {},
   "source": [
    "## NTXentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e681882fb0a1ef75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.314973Z",
     "start_time": "2025-02-05T15:35:00.293718Z"
    }
   },
   "outputs": [],
   "source": [
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e00d1173682fbb",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a3e7980201a6cc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T15:35:00.367603Z",
     "start_time": "2025-02-05T15:35:00.340329Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def main(mode='train'):\n",
    "    # os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    run_name = f\"experiment_{current_time}_{GP_MODEL}_refiner_fc_{PROJECTION_HIDDEN_DIM}_output_{PROJECTION_OUTPUT_DIM}_attention_{ATTENTION_HIDDEN_DIM}_kernel_{GP_KERNEL}_model_{MODEL_TYPE}\"\n",
    "\n",
    "    # Initialize W&B with a specific run name\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\", name=run_name)\n",
    "\n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    # train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    # test_labels = pd.read_csv('./data_analyze/testing_dataset_150_redundancy.csv')\n",
    "    train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    params = {\n",
    "        'channels': CHANNELS,  # Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
    "        'num_classes': NUM_CLASSES,  # Number of output classes for classification\n",
    "        'drop_prob': 0.5,  # Dropout probability\n",
    "        'inducing_points': INDUCING_POINTS,  # Number of inducing points for the Gaussian Process layer\n",
    "        'projection_location': PROJECTION_LOCATION,  # Choose from 'after_resnet', 'after_attention', or 'after_gp'\n",
    "        'projection_hidden_dim': PROJECTION_HIDDEN_DIM,  # Hidden dimension size for the projection head\n",
    "        'projection_output_dim': PROJECTION_OUTPUT_DIM,  # Output dimension size for the projection head\n",
    "        'attention_hidden_dim': ATTENTION_HIDDEN_DIM,  # Hidden dimension size for the attention head\n",
    "        'gp_model': GP_MODEL,\n",
    "        'kernel_type': GP_KERNEL,\n",
    "        'model_type': MODEL_TYPE\n",
    "    }\n",
    "\n",
    "    if TRAINING_TYPE == 'end_to_end':\n",
    "        # Instantiate the CNN_GP_ATT model with the specified parameters\n",
    "        if NUM_CLASSES == 1:\n",
    "            model = CNN_ATT_GP(params)\n",
    "            likelihood = PGLikelihood()\n",
    "            optimizer = optim.Adam([\n",
    "                {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "            ])\n",
    "        else:\n",
    "            if GP_MODEL == 'single_task':\n",
    "                model = CNN_ATT_GP_Multilabel(params)\n",
    "                likelihood = nn.ModuleList([PGLikelihood() for _ in range(NUM_CLASSES)])\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "            elif GP_MODEL == 'multi_task':\n",
    "                model = CNN_ATT_GP_MIML(params)\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=NUM_CLASSES)\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "\n",
    "\n",
    "    criterion_cl = NTXentLoss(0.5)\n",
    "    if NUM_CLASSES == 1:\n",
    "        pos_weights = torch.tensor([5.0]).to(DEVICE)\n",
    "    else:\n",
    "        pos_weights = torch.tensor([5.0] * NUM_CLASSES).to(DEVICE)\n",
    "    criterion_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    criterion_bce_wll = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if mode == 'train':\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            wandb.watch(model)  # Watch the model to log gradients and parameters\n",
    "            trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                                        optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "            predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "            metrics = calculate_metrics(predictions, labels)\n",
    "            wandb.log(metrics)\n",
    "            print_metrics(metrics)\n",
    "\n",
    "            # plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "            # plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "            torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    # if TRAINING_TYPE == 'end_to_end':\n",
    "    #     trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "    #     predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    #\n",
    "    # metrics = calculate_metrics(predictions, labels)\n",
    "    # wandb.log(metrics)\n",
    "    # print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed10b588911041",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498cc78442856e0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-05T15:35:00.397280Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuynhsikha2003\u001b[0m (\u001b[33mhuynhsikha2003-i-h-c-qu-c-gia-tp-hcm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Brain-Stroke-Diagnosis/rsna/wandb/run-20250206_043531-bvrbqo5a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/bvrbqo5a' target=\"_blank\">experiment_20250206_0435_multi_task_refiner_fc_256_output_128_attention_512_kernel_rbf_model_resnext50</a></strong> to <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/bvrbqo5a' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/bvrbqo5a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/workspace/Brain-Stroke-Diagnosis/rsna/../dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:20<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 6.6297, Accuracy: 0.2982, Precision: 0.3747, Recall: 0.4939, F1: 0.4214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:13<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Validation:\n",
      "Loss: 5.9246, Accuracy: 0.4080, Precision: 0.4162, Recall: 0.6707, F1: 0.5116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Train:\n",
      "Loss: 5.4077, Accuracy: 0.4673, Precision: 0.4482, Recall: 0.6370, F1: 0.5257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:05<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Validation:\n",
      "Loss: 4.9556, Accuracy: 0.5336, Precision: 0.4875, Recall: 0.6085, F1: 0.5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Train:\n",
      "Loss: 4.5526, Accuracy: 0.5181, Precision: 0.4800, Recall: 0.6709, F1: 0.5585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:02<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Validation:\n",
      "Loss: 4.2889, Accuracy: 0.5914, Precision: 0.5437, Recall: 0.5630, F1: 0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Train:\n",
      "Loss: 3.9903, Accuracy: 0.5549, Precision: 0.5602, Recall: 0.6992, F1: 0.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:02<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Validation:\n",
      "Loss: 3.9129, Accuracy: 0.5705, Precision: 0.5165, Recall: 0.6210, F1: 0.5607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Train:\n",
      "Loss: 3.7184, Accuracy: 0.5824, Precision: 0.5677, Recall: 0.7219, F1: 0.6092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:03<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Validation:\n",
      "Loss: 3.7288, Accuracy: 0.5204, Precision: 0.4937, Recall: 0.6357, F1: 0.5538\n",
      "EarlyStopping counter: 1 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:20<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Train:\n",
      "Loss: 3.5878, Accuracy: 0.5896, Precision: 0.5610, Recall: 0.7265, F1: 0.6165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:03<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Validation:\n",
      "Loss: 3.6123, Accuracy: 0.4447, Precision: 0.4725, Recall: 0.7247, F1: 0.5428\n",
      "EarlyStopping counter: 2 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Train:\n",
      "Loss: 3.4951, Accuracy: 0.5933, Precision: 0.5603, Recall: 0.7246, F1: 0.6172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:03<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Validation:\n",
      "Loss: 3.5290, Accuracy: 0.5039, Precision: 0.4893, Recall: 0.6994, F1: 0.5578\n",
      "EarlyStopping counter: 3 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:21<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Train:\n",
      "Loss: 3.4054, Accuracy: 0.5961, Precision: 0.5773, Recall: 0.7314, F1: 0.6255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Validation:\n",
      "Loss: 3.4374, Accuracy: 0.6272, Precision: 0.5684, Recall: 0.6240, F1: 0.5931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Train:\n",
      "Loss: 3.2892, Accuracy: 0.6078, Precision: 0.5744, Recall: 0.7372, F1: 0.6335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:04<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Validation:\n",
      "Loss: 3.2888, Accuracy: 0.5221, Precision: 0.5287, Recall: 0.7116, F1: 0.5862\n",
      "EarlyStopping counter: 1 out of 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Train:\n",
      "Loss: 3.1663, Accuracy: 0.6197, Precision: 0.6040, Recall: 0.7417, F1: 0.6460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:04<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Validation:\n",
      "Loss: 3.1677, Accuracy: 0.6042, Precision: 0.5655, Recall: 0.6361, F1: 0.5944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Train:\n",
      "Loss: 3.0195, Accuracy: 0.6320, Precision: 0.6199, Recall: 0.7564, F1: 0.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:02<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Validation:\n",
      "Loss: 3.0169, Accuracy: 0.6272, Precision: 0.5700, Recall: 0.7225, F1: 0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1731/1731 [21:18<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Train:\n",
      "Loss: 2.7861, Accuracy: 0.6502, Precision: 0.6212, Recall: 0.7649, F1: 0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288/288 [02:03<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Validation:\n",
      "Loss: 2.7805, Accuracy: 0.6437, Precision: 0.6340, Recall: 0.6651, F1: 0.6365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1731 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
