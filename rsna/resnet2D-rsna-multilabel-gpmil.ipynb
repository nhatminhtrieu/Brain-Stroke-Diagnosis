{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import Libraries",
   "id": "c8d276949d5508e0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:21.536453Z",
     "start_time": "2025-01-17T06:20:20.127480Z"
    }
   },
   "source": [
    "from sympy.stats.rv import probability\n",
    "!pip install gpytorch\n",
    "!pip install wandb"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n",
      "Requirement already satisfied: wandb in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (0.18.5)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.21.12)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.17.0)\r\n",
      "Requirement already satisfied: setproctitle in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (72.1.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.070204Z",
     "start_time": "2025-01-17T06:20:21.549391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_metric_learning import losses\n",
    "from torch.cpu.amp import GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from models.mil_resnet import CNN_ATT_GP_Multilabel, CNN_ATT_GP, CNN_ATT_GP_MIML\n",
    "from utils import hard_negative_mining as hnm\n",
    "import gpytorch\n",
    "from layers.gaussian_process import SingletaskGPModel, PGLikelihood\n",
    "from utils.early_stopping import EarlyStoppingForOptimization, EarlyStopping\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "id": "d1ef154a52b21180",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.094987Z",
     "start_time": "2025-01-17T06:20:23.093413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "883dbd33ec48f4eb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Configurations\n",
    "## GPU Configurations"
   ],
   "id": "30384497eb03d169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.152299Z",
     "start_time": "2025-01-17T06:20:23.136279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ],
   "id": "227b560b8a74cc42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.209394Z",
     "start_time": "2025-01-17T06:20:23.188322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "bca87a30cd68134e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seed Everything",
   "id": "e7ccf613094b85ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.258227Z",
     "start_time": "2025-01-17T06:20:23.234567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything()"
   ],
   "id": "873b638dce5c401c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Constants and Hyperparameters",
   "id": "aa79def387733d3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.308160Z",
     "start_time": "2025-01-17T06:20:23.283164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "GP_MODEL = config['gp_model']\n",
    "GP_KERNEL = config['kernel_type']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']"
   ],
   "id": "a4b815f3cbcc84d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.647521Z",
     "start_time": "2025-01-17T06:20:23.332705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "ROOT_DIR = '/kaggle/input/rsna-mil-training/' if KAGGLE else None\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "DICOM_DIR = DATA_DIR\n",
    "CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_1_redundancy.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "\n",
    "# Load patient scan labels\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)"
   ],
   "id": "f42cc42e05675da7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.675904Z",
     "start_time": "2025-01-17T06:20:23.654670Z"
    }
   },
   "cell_type": "code",
   "source": "patient_scan_labels.head()",
   "id": "58a5deb68c8ee212",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...   \n",
       "1  ['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...   \n",
       "2  ['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...   \n",
       "3  ['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...   \n",
       "4  ['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0002cd41   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00054f3f   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0006d192   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00086119   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_000e5623   \n",
       "\n",
       "  study_instance_uid  ...                                      pixel_spacing  \\\n",
       "0      ID_66929e09d4  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "1      ID_8a449ae31b  ...  ['[0.435546875, 0.435546875]', '[0.435546875, ...   \n",
       "2      ID_25690b4725  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "3      ID_fdde2979b0  ...  ['[0.488281, 0.488281]', '[0.488281, 0.488281]...   \n",
       "4      ID_9a4be35b9a  ...  ['[0.44921875, 0.44921875]', '[0.44921875, 0.4...   \n",
       "\n",
       "                                pixel_representation  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                       window_center  \\\n",
       "0  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "1  ['[00036, 00036]', '[00036, 00036]', '[00036, ...   \n",
       "2  ['40', '40', '40', '40', '40', '40', '40', '40...   \n",
       "3  ['30', '30', '30', '30', '30', '30', '30', '30...   \n",
       "4  ['[00036, 00036]', '[00036, 00036]', '[00036, ...   \n",
       "\n",
       "                                        window_width  \\\n",
       "0  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "1  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "2  ['150', '150', '150', '150', '150', '150', '15...   \n",
       "3  ['80', '80', '80', '80', '80', '80', '80', '80...   \n",
       "4  ['[00080, 00080]', '[00080, 00080]', '[00080, ...   \n",
       "\n",
       "                                   rescale_intercept  \\\n",
       "0  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "1  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "2  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "3  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "4  [-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...   \n",
       "\n",
       "                                       rescale_slope patient_label  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...             0   \n",
       "\n",
       "                                              z_axis  \\\n",
       "0  [38.484, 43.517, 48.549, 53.582, 58.614, 63.64...   \n",
       "1  [71.9000244, 76.9000244, 81.9000244, 86.900024...   \n",
       "2  [41.921, 49.421, 56.921, 64.421, 71.921, 79.42...   \n",
       "3  [35.556, 40.757, 45.959, 51.16, 56.362, 61.563...   \n",
       "4  [272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...   \n",
       "\n",
       "                                     slice_thickness  \\\n",
       "0  [2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...   \n",
       "1  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "2  [3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...   \n",
       "3  [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                    selected_indices  \n",
       "0  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...  \n",
       "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "2  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...  \n",
       "3  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...  \n",
       "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_spacing</th>\n",
       "      <th>pixel_representation</th>\n",
       "      <th>window_center</th>\n",
       "      <th>window_width</th>\n",
       "      <th>rescale_intercept</th>\n",
       "      <th>rescale_slope</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>selected_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0002cd41</td>\n",
       "      <td>ID_66929e09d4</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[38.484, 43.517, 48.549, 53.582, 58.614, 63.64...</td>\n",
       "      <td>[2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00054f3f</td>\n",
       "      <td>ID_8a449ae31b</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.435546875, 0.435546875]', '[0.435546875, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['[00036, 00036]', '[00036, 00036]', '[00036, ...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[71.9000244, 76.9000244, 81.9000244, 86.900024...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0006d192</td>\n",
       "      <td>ID_25690b4725</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['40', '40', '40', '40', '40', '40', '40', '40...</td>\n",
       "      <td>['150', '150', '150', '150', '150', '150', '15...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[41.921, 49.421, 56.921, 64.421, 71.921, 79.42...</td>\n",
       "      <td>[3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00086119</td>\n",
       "      <td>ID_fdde2979b0</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.488281, 0.488281]', '[0.488281, 0.488281]...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>['30', '30', '30', '30', '30', '30', '30', '30...</td>\n",
       "      <td>['80', '80', '80', '80', '80', '80', '80', '80...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.556, 40.757, 45.959, 51.16, 56.362, 61.563...</td>\n",
       "      <td>[2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_000e5623</td>\n",
       "      <td>ID_9a4be35b9a</td>\n",
       "      <td>...</td>\n",
       "      <td>['[0.44921875, 0.44921875]', '[0.44921875, 0.4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>['[00036, 00036]', '[00036, 00036]', '[00036, ...</td>\n",
       "      <td>['[00080, 00080]', '[00080, 00080]', '[00080, ...</td>\n",
       "      <td>[-1024.0, -1024.0, -1024.0, -1024.0, -1024.0, ...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "## Splitting Data"
   ],
   "id": "d23e439462d55e8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.733296Z",
     "start_time": "2025-01-17T06:20:23.715217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels\n",
    "\n",
    "# from skmultilearn.model_selection import iterative_train_test_split\n",
    "#\n",
    "# def split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "#     # Extract the labels from the DataFrame\n",
    "#     # labels = patient_scan_labels[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']].values\n",
    "#\n",
    "#     labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal', 'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']]\n",
    "#\n",
    "#     if test_size > 0:\n",
    "#         train_labels, test_labels = iterative_train_test_split(\n",
    "#             patient_scan_labels,\n",
    "#             labels,\n",
    "#             test_size=test_size\n",
    "#         )\n",
    "#\n",
    "#         train_labels, val_labels = iterative_train_test_split(\n",
    "#             train_labels,\n",
    "#             labels,\n",
    "#             test_size=val_size\n",
    "#         )\n",
    "#\n",
    "#     else:\n",
    "#         train_labels, val_labels = train_test_split(\n",
    "#             patient_scan_labels,\n",
    "#             test_size=val_size,\n",
    "#             stratify=np.array([tuple(row) for row in labels]),  # Stratify based on multi-labels\n",
    "#             random_state=random_state\n",
    "#         )\n",
    "#         test_labels = None\n",
    "#\n",
    "#     return train_labels, val_labels, test_labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset_for_multilabel(patient_scan_labels, test_size=0.15, val_size=0.25, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal',\n",
    "                                  'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']].values\n",
    "\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(msss.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels = patient_scan_labels.iloc[train_idx]\n",
    "        test_labels = patient_scan_labels.iloc[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(train_labels, labels[train_idx]))\n",
    "\n",
    "        train_labels_final = train_labels.iloc[train_idx]\n",
    "        val_labels = train_labels.iloc[val_idx]\n",
    "\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels_final = patient_scan_labels.iloc[train_idx]\n",
    "        val_labels = patient_scan_labels.iloc[val_idx]\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels_final, val_labels, test_labels"
   ],
   "id": "2031b17e255c735c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Generator",
   "id": "1862775f3577f1ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.911373Z",
     "start_time": "2025-01-17T06:20:23.757983Z"
    }
   },
   "cell_type": "code",
   "source": "from dataset_generators.RSNA_Dataset import MedicalScanDataset",
   "id": "c6f02933b0ad10a9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:23.943803Z",
     "start_time": "2025-01-17T06:20:23.928071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ],
   "id": "950255932ed8b97f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.347913Z",
     "start_time": "2025-01-17T06:20:23.967056Z"
    }
   },
   "cell_type": "code",
   "source": "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)",
   "id": "934d4da31f781dd3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.376159Z",
     "start_time": "2025-01-17T06:20:26.357988Z"
    }
   },
   "cell_type": "code",
   "source": "len(original_dataset)",
   "id": "ba1afebc6c8b190b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21735"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.503819Z",
     "start_time": "2025-01-17T06:20:26.405150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y, z, _ = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ],
   "id": "eb16a6208d8e0d67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 1, 224, 224]) torch.Size([28]) torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.531785Z",
     "start_time": "2025-01-17T06:20:26.515332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True,\n",
    "                      drop_last=True)\n",
    "\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "id": "beea527737fe7ca1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training and Validation\n",
    "## Metrics Calculation\n",
    "### Performance Metrics"
   ],
   "id": "bc7396c943d24607"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.572302Z",
     "start_time": "2025-01-17T06:20:26.554262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "id": "386345f4492a4646",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss Function",
   "id": "a9e07ba62418d236"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.630424Z",
     "start_time": "2025-01-17T06:20:26.600754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    if CHANNELS == 1:\n",
    "        bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    else:\n",
    "        ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        ce_loss = ce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * kl_divergence\n",
    "\n",
    "    return total_loss"
   ],
   "id": "e06a6fe162be7161",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Loop",
   "id": "9be9d852dea260a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.676673Z",
     "start_time": "2025-01-17T06:20:26.651508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, optimizer, variational_ngd_optimizer, scheduler,\n",
    "                scaler, device):\n",
    "    total_loss = 0.0\n",
    "    total_nlls = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if GP_MODEL == 'single_task':\n",
    "            for i in range(NUM_CLASSES):\n",
    "                variational_ngd_optimizer[i].zero_grad()\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "\n",
    "            if GP_MODEL == 'single_task':\n",
    "                loss = 0\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                loss.mean()\n",
    "                loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "\n",
    "                probs = [likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)]\n",
    "                probabilities = torch.stack(probs, dim=1)\n",
    "                preds = (probabilities >= 0.5).int()\n",
    "\n",
    "            else:\n",
    "                loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                loss = loss.mean()\n",
    "                preds = (outputs >= 0.5).int()\n",
    "\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if GP_MODEL == 'single_task':\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    variational_ngd_optimizer[i].step()\n",
    "            scheduler.step()\n",
    "\n",
    "        if NUM_CLASSES == 1:\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "        else:\n",
    "            labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def validate(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    loss = 0\n",
    "                    for i in range(NUM_CLASSES):\n",
    "                        loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                    loss.mean()\n",
    "                    loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    loss = loss.mean()\n",
    "                    total_loss += loss.item()\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "\n",
    "\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def train_model(model, likelihoods, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate,\n",
    "                device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    # Initialize Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "    if GP_MODEL == 'single_task':\n",
    "        mlls = [gpytorch.mlls.VariationalELBO(likelihoods[i], model.gp_layers[i], num_data=len(train_loader.dataset)) for\n",
    "                i in range(NUM_CLASSES)]\n",
    "        mlls = [mll.to(device) for mll in mlls]\n",
    "\n",
    "        variational_ngd_optimizer = [\n",
    "            gpytorch.optim.NGD(model.gp_layers[i].variational_parameters(), num_data=len(train_loader.dataset),\n",
    "                               lr=0.01) for i in range(NUM_CLASSES)]\n",
    "    else:\n",
    "        mlls = gpytorch.mlls.VariationalELBO(likelihoods, model.gp_layers, num_data=len(train_loader.dataset))\n",
    "        mlls = mlls.to(device)\n",
    "        variational_ngd_optimizer = None\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * num_epochs, 0.75 * num_epochs], gamma=0.1)\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, likelihoods, train_loader, criterion_cl, criterion_bce,\n",
    "                                                                  mlls, optimizer, variational_ngd_optimizer,\n",
    "                                                                  scheduler, scaler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss,\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, likelihoods, val_loader, criterion_cl, criterion_bce, mlls,\n",
    "                                                         device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_metrics[\"accuracy\"], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model"
   ],
   "id": "7c288284804281a1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "id": "9ee0299f417b29b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.721380Z",
     "start_time": "2025-01-17T06:20:26.701872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, likelihoods, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "id": "e193f0efccd4b5a6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.794728Z",
     "start_time": "2025-01-17T06:20:26.750508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, _ = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:  # Binary classification\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, likelihoods, data_loader, device)\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "id": "8615707218fcdd04",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Helper Functions",
   "id": "5d7348e9d5406c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.822377Z",
     "start_time": "2025-01-17T06:20:26.805875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path, params):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class(params)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "id": "c3875223d716184",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NTXentLoss",
   "id": "5b3fdb7f6cd302d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.871960Z",
     "start_time": "2025-01-17T06:20:26.847747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ],
   "id": "e681882fb0a1ef75",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main",
   "id": "d3e00d1173682fbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T06:20:26.914172Z",
     "start_time": "2025-01-17T06:20:26.893108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(mode='train'):\n",
    "    # os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    # train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    test_labels = pd.read_csv('./data_analyze/testing_dataset_150_redundancy.csv')\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    params = {\n",
    "        'channels': CHANNELS,  # Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
    "        'num_classes': NUM_CLASSES,  # Number of output classes for classification\n",
    "        'drop_prob': 0.5,  # Dropout probability\n",
    "        'inducing_points': INDUCING_POINTS,  # Number of inducing points for the Gaussian Process layer\n",
    "        'projection_location': PROJECTION_LOCATION,  # Choose from 'after_resnet', 'after_attention', or 'after_gp'\n",
    "        'projection_hidden_dim': PROJECTION_HIDDEN_DIM,  # Hidden dimension size for the projection head\n",
    "        'projection_output_dim': PROJECTION_OUTPUT_DIM,  # Output dimension size for the projection head\n",
    "        'attention_hidden_dim': ATTENTION_HIDDEN_DIM,  # Hidden dimension size for the attention head\n",
    "        'gp_model': GP_MODEL,\n",
    "        'kernel_type': GP_KERNEL\n",
    "    }\n",
    "\n",
    "    if TRAINING_TYPE == 'end_to_end':\n",
    "        # Instantiate the CNN_GP_ATT model with the specified parameters\n",
    "        if NUM_CLASSES == 1:\n",
    "            model = CNN_ATT_GP(params)\n",
    "            likelihood = PGLikelihood()\n",
    "            optimizer = optim.Adam([\n",
    "                {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "            ])\n",
    "        else:\n",
    "            if GP_MODEL == 'single_task':\n",
    "                model = CNN_ATT_GP_Multilabel(params)\n",
    "                likelihood = nn.ModuleList([PGLikelihood() for _ in range(NUM_CLASSES)])\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "            else:\n",
    "                model = CNN_ATT_GP_MIML(params)\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=NUM_CLASSES)\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "\n",
    "\n",
    "    criterion_cl = NTXentLoss(0.5)\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()\n",
    "    criterion_bce_wll = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if mode == 'train':\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            wandb.watch(model)  # Watch the model to log gradients and parameters\n",
    "            trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                                        optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "            predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "            metrics = calculate_metrics(predictions, labels)\n",
    "            wandb.log(metrics)\n",
    "            print_metrics(metrics)\n",
    "\n",
    "            # plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "            # plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "            torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    # if TRAINING_TYPE == 'end_to_end':\n",
    "    #     trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "    #     predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    #\n",
    "    # metrics = calculate_metrics(predictions, labels)\n",
    "    # wandb.log(metrics)\n",
    "    # print_metrics(metrics)"
   ],
   "id": "7a3e7980201a6cc5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Results",
   "id": "c7ed10b588911041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:12:28.522562Z",
     "start_time": "2025-01-17T06:20:26.944197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ],
   "id": "1498cc78442856e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mhuynhsikha2003\u001B[0m (\u001B[33mhuynhsikha2003-i-h-c-qu-c-gia-tp-hcm\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/media/hskha23/Kha/Brain-Stroke-Diagnosis/rsna/wandb/run-20250117_132027-f6truhur</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/f6truhur' target=\"_blank\">classic-firebrand-448</a></strong> to <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/f6truhur' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/f6truhur</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 7.4728, Accuracy: 0.5821, Precision: 0.5342, Recall: 0.0879, F1: 0.1421\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: 7.1792, Accuracy: 0.5858, Precision: 0.5022, Recall: 0.2688, F1: 0.3435\n",
      "Epoch 2/50 - Train:\n",
      "Loss: 6.9208, Accuracy: 0.5900, Precision: 0.6761, Recall: 0.2828, F1: 0.3594\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: 6.6093, Accuracy: 0.5996, Precision: 0.6618, Recall: 0.3321, F1: 0.4092\n",
      "Epoch 3/50 - Train:\n",
      "Loss: 6.2434, Accuracy: 0.6114, Precision: 0.6859, Recall: 0.3361, F1: 0.4275\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: 5.8193, Accuracy: 0.6073, Precision: 0.6472, Recall: 0.4307, F1: 0.5021\n",
      "Epoch 4/50 - Train:\n",
      "Loss: 5.4020, Accuracy: 0.6258, Precision: 0.6895, Recall: 0.3808, F1: 0.4748\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: 4.9895, Accuracy: 0.6384, Precision: 0.6800, Recall: 0.4041, F1: 0.4960\n",
      "Epoch 5/50 - Train:\n",
      "Loss: 4.6516, Accuracy: 0.6431, Precision: 0.6983, Recall: 0.4207, F1: 0.5118\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: 4.3432, Accuracy: 0.6449, Precision: 0.7039, Recall: 0.3877, F1: 0.4916\n",
      "Epoch 6/50 - Train:\n",
      "Loss: 4.1184, Accuracy: 0.6609, Precision: 0.7077, Recall: 0.4588, F1: 0.5476\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: 3.9312, Accuracy: 0.6605, Precision: 0.7272, Recall: 0.3795, F1: 0.4850\n",
      "Epoch 7/50 - Train:\n",
      "Loss: 3.7826, Accuracy: 0.6767, Precision: 0.7156, Recall: 0.4878, F1: 0.5723\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: 3.6777, Accuracy: 0.6574, Precision: 0.6814, Recall: 0.4520, F1: 0.5378\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 8/50 - Train:\n",
      "Loss: 3.5791, Accuracy: 0.6945, Precision: 0.7228, Recall: 0.5170, F1: 0.5962\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: 3.5280, Accuracy: 0.6611, Precision: 0.6884, Recall: 0.4582, F1: 0.5412\n",
      "Epoch 9/50 - Train:\n",
      "Loss: 3.4457, Accuracy: 0.7097, Precision: 0.7362, Recall: 0.5475, F1: 0.6233\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: 3.4199, Accuracy: 0.6615, Precision: 0.6958, Recall: 0.4771, F1: 0.5471\n",
      "Epoch 10/50 - Train:\n",
      "Loss: 3.3476, Accuracy: 0.7271, Precision: 0.7469, Recall: 0.5720, F1: 0.6440\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: 3.3421, Accuracy: 0.6434, Precision: 0.6137, Recall: 0.6051, F1: 0.6081\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 11/50 - Train:\n",
      "Loss: 3.2581, Accuracy: 0.7462, Precision: 0.7514, Recall: 0.5990, F1: 0.6635\n",
      "Epoch 11/50 - Validation:\n",
      "Loss: 3.2528, Accuracy: 0.6675, Precision: 0.6853, Recall: 0.4919, F1: 0.5646\n",
      "Epoch 12/50 - Train:\n",
      "Loss: 3.1718, Accuracy: 0.7677, Precision: 0.7618, Recall: 0.6260, F1: 0.6852\n",
      "Epoch 12/50 - Validation:\n",
      "Loss: 3.1790, Accuracy: 0.6546, Precision: 0.6495, Recall: 0.5579, F1: 0.5988\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 13/50 - Train:\n",
      "Loss: 3.0855, Accuracy: 0.7887, Precision: 0.7733, Recall: 0.6525, F1: 0.7061\n",
      "Epoch 13/50 - Validation:\n",
      "Loss: 3.1020, Accuracy: 0.6784, Precision: 0.6703, Recall: 0.5283, F1: 0.5859\n",
      "Epoch 14/50 - Train:\n",
      "Loss: 2.9955, Accuracy: 0.8151, Precision: 0.7836, Recall: 0.6802, F1: 0.7272\n",
      "Epoch 14/50 - Validation:\n",
      "Loss: 3.0284, Accuracy: 0.6683, Precision: 0.6842, Recall: 0.4975, F1: 0.5520\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 15/50 - Train:\n",
      "Loss: 2.9046, Accuracy: 0.8347, Precision: 0.7906, Recall: 0.7005, F1: 0.7421\n",
      "Epoch 15/50 - Validation:\n",
      "Loss: 2.9637, Accuracy: 0.6797, Precision: 0.7347, Recall: 0.4442, F1: 0.5463\n",
      "Epoch 16/50 - Train:\n",
      "Loss: 2.8095, Accuracy: 0.8597, Precision: 0.8306, Recall: 0.7306, F1: 0.7632\n",
      "Epoch 16/50 - Validation:\n",
      "Loss: 2.8862, Accuracy: 0.6754, Precision: 0.6735, Recall: 0.5051, F1: 0.5724\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 17/50 - Train:\n",
      "Loss: 2.7174, Accuracy: 0.8783, Precision: 0.9696, Recall: 0.7507, F1: 0.7821\n",
      "Epoch 17/50 - Validation:\n",
      "Loss: 2.7836, Accuracy: 0.6707, Precision: 0.7176, Recall: 0.5276, F1: 0.5875\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 18/50 - Train:\n",
      "Loss: 2.6122, Accuracy: 0.8986, Precision: 0.9763, Recall: 0.7690, F1: 0.7962\n",
      "Epoch 18/50 - Validation:\n",
      "Loss: 2.6810, Accuracy: 0.6670, Precision: 0.7278, Recall: 0.5497, F1: 0.5948\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 19/50 - Train:\n",
      "Loss: 2.4622, Accuracy: 0.9012, Precision: 0.9292, Recall: 0.7740, F1: 0.8019\n",
      "Epoch 19/50 - Validation:\n",
      "Loss: 2.5450, Accuracy: 0.6745, Precision: 0.6483, Recall: 0.5414, F1: 0.5846\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 20/50 - Train:\n",
      "Loss: 2.3161, Accuracy: 0.9069, Precision: 0.9447, Recall: 0.7869, F1: 0.8150\n",
      "Epoch 20/50 - Validation:\n",
      "Loss: 2.4382, Accuracy: 0.6769, Precision: 0.7002, Recall: 0.5542, F1: 0.5984\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 21/50 - Train:\n",
      "Loss: 2.2056, Accuracy: 0.9285, Precision: 0.9706, Recall: 0.8167, F1: 0.8499\n",
      "Epoch 21/50 - Validation:\n",
      "Loss: 2.3433, Accuracy: 0.6648, Precision: 0.7214, Recall: 0.5949, F1: 0.6181\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 22/50 - Train:\n",
      "Loss: 2.1065, Accuracy: 0.9380, Precision: 0.9769, Recall: 0.8347, F1: 0.8699\n",
      "Epoch 22/50 - Validation:\n",
      "Loss: 2.2823, Accuracy: 0.7001, Precision: 0.7576, Recall: 0.5393, F1: 0.6202\n",
      "Epoch 23/50 - Train:\n",
      "Loss: 2.0137, Accuracy: 0.9494, Precision: 0.9822, Recall: 0.8592, F1: 0.8954\n",
      "Epoch 23/50 - Validation:\n",
      "Loss: 2.1915, Accuracy: 0.6819, Precision: 0.7566, Recall: 0.5261, F1: 0.6177\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 24/50 - Train:\n",
      "Loss: 1.9245, Accuracy: 0.9567, Precision: 0.9880, Recall: 0.8867, F1: 0.9220\n",
      "Epoch 24/50 - Validation:\n",
      "Loss: 2.1043, Accuracy: 0.6745, Precision: 0.7175, Recall: 0.5789, F1: 0.6225\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 25/50 - Train:\n",
      "Loss: 1.8361, Accuracy: 0.9711, Precision: 0.9887, Recall: 0.9224, F1: 0.9495\n",
      "Epoch 25/50 - Validation:\n",
      "Loss: 2.0270, Accuracy: 0.6789, Precision: 0.6955, Recall: 0.5951, F1: 0.6378\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 26/50 - Train:\n",
      "Loss: 1.7561, Accuracy: 0.9733, Precision: 0.9931, Recall: 0.9390, F1: 0.9626\n",
      "Epoch 26/50 - Validation:\n",
      "Loss: 1.9760, Accuracy: 0.6881, Precision: 0.7367, Recall: 0.5509, F1: 0.6155\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 27/50 - Train:\n",
      "Loss: 1.6896, Accuracy: 0.9579, Precision: 0.9873, Recall: 0.9221, F1: 0.9505\n",
      "Epoch 27/50 - Validation:\n",
      "Loss: 1.9194, Accuracy: 0.6786, Precision: 0.7091, Recall: 0.5512, F1: 0.6096\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 28/50 - Train:\n",
      "Loss: 1.6118, Accuracy: 0.9709, Precision: 0.9879, Recall: 0.9443, F1: 0.9642\n",
      "Epoch 28/50 - Validation:\n",
      "Loss: 1.8550, Accuracy: 0.6800, Precision: 0.7102, Recall: 0.5345, F1: 0.6094\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 29/50 - Train:\n",
      "Loss: 1.5362, Accuracy: 0.9873, Precision: 0.9953, Recall: 0.9736, F1: 0.9839\n",
      "Epoch 29/50 - Validation:\n",
      "Loss: 1.7990, Accuracy: 0.6692, Precision: 0.6697, Recall: 0.6004, F1: 0.6299\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 30/50 - Train:\n",
      "Loss: 1.4692, Accuracy: 0.9938, Precision: 0.9967, Recall: 0.9833, F1: 0.9898\n",
      "Epoch 30/50 - Validation:\n",
      "Loss: 1.7549, Accuracy: 0.6926, Precision: 0.7577, Recall: 0.5370, F1: 0.6187\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 31/50 - Train:\n",
      "Loss: 1.4085, Accuracy: 0.9939, Precision: 0.9975, Recall: 0.9880, F1: 0.9927\n",
      "Epoch 31/50 - Validation:\n",
      "Loss: 1.6881, Accuracy: 0.6918, Precision: 0.7414, Recall: 0.5814, F1: 0.6410\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 32/50 - Train:\n",
      "Loss: 1.3592, Accuracy: 0.9867, Precision: 0.9955, Recall: 0.9805, F1: 0.9878\n",
      "Epoch 32/50 - Validation:\n",
      "Loss: 1.6393, Accuracy: 0.6933, Precision: 0.7338, Recall: 0.5689, F1: 0.6292\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 0.7001104565537555\n",
      "Test Accuracy: 0.8267, Precision: 0.9167, Recall: 0.8475, F1: 0.8778\n",
      "Error loading model from results/trained_model.pth: list index out of range\n",
      "Initializing model with random weights instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[24], line 80\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(mode)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TRAINING_TYPE \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mend_to_end\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     79\u001B[0m     trained_model \u001B[38;5;241m=\u001B[39m load_model(CNN_ATT_GP, MODEL_PATH, params)\n\u001B[0;32m---> 80\u001B[0m     predictions, labels \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrained_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     82\u001B[0m metrics \u001B[38;5;241m=\u001B[39m calculate_metrics(predictions, labels)\n\u001B[1;32m     83\u001B[0m wandb\u001B[38;5;241m.\u001B[39mlog(metrics)\n",
      "Cell \u001B[0;32mIn[20], line 6\u001B[0m, in \u001B[0;36mevaluate_model\u001B[0;34m(model, likelihoods, data_loader, device)\u001B[0m\n\u001B[1;32m      4\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m----> 6\u001B[0m \u001B[43mlikelihoods\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval\u001B[49m()\n\u001B[1;32m      8\u001B[0m predictions \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      9\u001B[0m labels \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'DataLoader' object has no attribute 'eval'"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87b2888b7a6a55f6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
