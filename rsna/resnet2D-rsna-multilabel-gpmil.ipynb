{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import Libraries",
   "id": "c8d276949d5508e0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:16.048752Z",
     "start_time": "2025-01-18T07:51:14.638698Z"
    }
   },
   "source": [
    "from sympy.stats.rv import probability\n",
    "!pip install gpytorch\n",
    "!pip install wandb"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpytorch in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (1.13)\r\n",
      "Requirement already satisfied: jaxtyping==0.2.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.2.19)\r\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.3.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (1.10.1)\r\n",
      "Requirement already satisfied: linear-operator>=0.5.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gpytorch) (0.5.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\r\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\r\n",
      "Requirement already satisfied: filelock in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.2)\r\n",
      "Requirement already satisfied: networkx in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.20.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==3.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.3->gpytorch) (12.6.20)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.3)\r\n",
      "Requirement already satisfied: wandb in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (0.18.5)\r\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (3.1.43)\r\n",
      "Requirement already satisfied: platformdirs in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.2.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.21.12)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.32.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (2.17.0)\r\n",
      "Requirement already satisfied: setproctitle in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (72.1.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from wandb) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.560689Z",
     "start_time": "2025-01-18T07:51:16.064590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from pytorch_metric_learning import losses\n",
    "from torch.cpu.amp import GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from models.mil_resnet import CNN_ATT_GP_Multilabel, CNN_ATT_GP, CNN_ATT_GP_MIML\n",
    "from utils import hard_negative_mining as hnm\n",
    "import gpytorch\n",
    "from layers.gaussian_process import SingletaskGPModel, PGLikelihood\n",
    "from utils.early_stopping import EarlyStoppingForOptimization, EarlyStopping\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ],
   "id": "d1ef154a52b21180",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.587691Z",
     "start_time": "2025-01-18T07:51:17.586278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "883dbd33ec48f4eb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Configurations\n",
    "## GPU Configurations"
   ],
   "id": "30384497eb03d169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.659324Z",
     "start_time": "2025-01-18T07:51:17.636654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize GPU Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "print(device)"
   ],
   "id": "227b560b8a74cc42",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.724650Z",
     "start_time": "2025-01-18T07:51:17.701381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "bca87a30cd68134e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Seed Everything",
   "id": "e7ccf613094b85ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.771809Z",
     "start_time": "2025-01-18T07:51:17.747929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything()"
   ],
   "id": "873b638dce5c401c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Constants and Hyperparameters",
   "id": "aa79def387733d3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:17.810048Z",
     "start_time": "2025-01-18T07:51:17.791955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "GP_MODEL = config['gp_model']\n",
    "GP_KERNEL = config['kernel_type']\n",
    "MODEL_TYPE = config['model_type']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']"
   ],
   "id": "a4b815f3cbcc84d1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.156143Z",
     "start_time": "2025-01-18T07:51:17.844421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Kaggle and local switch\n",
    "KAGGLE = os.path.exists('/kaggle')\n",
    "print(\"Running on Kaggle\" if KAGGLE else \"Running locally\")\n",
    "ROOT_DIR = '/kaggle/input/rsna-mil-training/' if KAGGLE else None\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "DICOM_DIR = DATA_DIR\n",
    "CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_2_redundancy.csv'\n",
    "\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR\n",
    "\n",
    "# Load patient scan labels\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)"
   ],
   "id": "f42cc42e05675da7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.184313Z",
     "start_time": "2025-01-18T07:51:18.162388Z"
    }
   },
   "cell_type": "code",
   "source": "patient_scan_labels.head()",
   "id": "58a5deb68c8ee212",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            filename  \\\n",
       "0  ['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...   \n",
       "1  ['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...   \n",
       "2  ['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...   \n",
       "3  ['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...   \n",
       "4  ['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                 any  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            epidural  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraparenchymal  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                    intraventricular  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                        subarachnoid  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            subdural   patient_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0002cd41   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00054f3f   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_0006d192   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_00086119   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ID_000e5623   \n",
       "\n",
       "  study_instance_uid  ... patient_label  \\\n",
       "0      ID_66929e09d4  ...             0   \n",
       "1      ID_8a449ae31b  ...             0   \n",
       "2      ID_25690b4725  ...             0   \n",
       "3      ID_fdde2979b0  ...             0   \n",
       "4      ID_9a4be35b9a  ...             0   \n",
       "\n",
       "                                              z_axis  \\\n",
       "0  [38.484, 43.517, 48.549, 53.582, 58.614, 63.64...   \n",
       "1  [71.9000244, 76.9000244, 81.9000244, 86.900024...   \n",
       "2  [41.921, 49.421, 56.921, 64.421, 71.921, 79.42...   \n",
       "3  [35.556, 40.757, 45.959, 51.16, 56.362, 61.563...   \n",
       "4  [272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...   \n",
       "\n",
       "                                     slice_thickness  \\\n",
       "0  [2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...   \n",
       "1  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "2  [3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...   \n",
       "3  [2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...   \n",
       "4  [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...   \n",
       "\n",
       "                                    selected_indices patient_any  \\\n",
       "0  [1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...           0   \n",
       "1  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...           0   \n",
       "2  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...           0   \n",
       "3  [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...           0   \n",
       "4  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...           0   \n",
       "\n",
       "  patient_subdural patient_epidural patient_intraparenchymal  \\\n",
       "0                0                0                        0   \n",
       "1                0                0                        0   \n",
       "2                0                0                        0   \n",
       "3                0                0                        0   \n",
       "4                0                0                        0   \n",
       "\n",
       "   patient_intraventricular patient_subarachnoid  \n",
       "0                         0                    0  \n",
       "1                         0                    0  \n",
       "2                         0                    0  \n",
       "3                         0                    0  \n",
       "4                         0                    0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>labels</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>...</th>\n",
       "      <th>patient_label</th>\n",
       "      <th>z_axis</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>selected_indices</th>\n",
       "      <th>patient_any</th>\n",
       "      <th>patient_subdural</th>\n",
       "      <th>patient_epidural</th>\n",
       "      <th>patient_intraparenchymal</th>\n",
       "      <th>patient_intraventricular</th>\n",
       "      <th>patient_subarachnoid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ID_37f32aed2.dcm', 'ID_d61a6a7b9.dcm', 'ID_4...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0002cd41</td>\n",
       "      <td>ID_66929e09d4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[38.484, 43.517, 48.549, 53.582, 58.614, 63.64...</td>\n",
       "      <td>[2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.52, 2.5...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 16, 17, 18, 19, 20...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ID_138d275c8.dcm', 'ID_447fa09d9.dcm', 'ID_0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00054f3f</td>\n",
       "      <td>ID_8a449ae31b</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[71.9000244, 76.9000244, 81.9000244, 86.900024...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['ID_520df89aa.dcm', 'ID_3b87d36d0.dcm', 'ID_9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_0006d192</td>\n",
       "      <td>ID_25690b4725</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[41.921, 49.421, 56.921, 64.421, 71.921, 79.42...</td>\n",
       "      <td>[3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.75, 3.7...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ID_203ef1efe.dcm', 'ID_0cec86087.dcm', 'ID_1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_00086119</td>\n",
       "      <td>ID_fdde2979b0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.556, 40.757, 45.959, 51.16, 56.362, 61.563...</td>\n",
       "      <td>[2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, 2.6, ...</td>\n",
       "      <td>[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 20, 21, 22...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ID_0785539ea.dcm', 'ID_30c100dbc.dcm', 'ID_3...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>ID_000e5623</td>\n",
       "      <td>ID_9a4be35b9a</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>[272.0, 277.0, 282.0, 287.0, 292.0, 297.0, 302...</td>\n",
       "      <td>[5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "## Splitting Data"
   ],
   "id": "d23e439462d55e8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.265261Z",
     "start_time": "2025-01-18T07:51:18.247018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset_for_multilabel(patient_scan_labels, test_size=0.15, val_size=0.25, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal',\n",
    "                                  'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']].values\n",
    "\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(msss.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels = patient_scan_labels.iloc[train_idx]\n",
    "        test_labels = patient_scan_labels.iloc[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(train_labels, labels[train_idx]))\n",
    "\n",
    "        train_labels_final = train_labels.iloc[train_idx]\n",
    "        val_labels = train_labels.iloc[val_idx]\n",
    "\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels_final = patient_scan_labels.iloc[train_idx]\n",
    "        val_labels = patient_scan_labels.iloc[val_idx]\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels_final, val_labels, test_labels"
   ],
   "id": "2031b17e255c735c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Generator",
   "id": "1862775f3577f1ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.431472Z",
     "start_time": "2025-01-18T07:51:18.276156Z"
    }
   },
   "cell_type": "code",
   "source": "from dataset_generators.RSNA_Dataset import MedicalScanDataset",
   "id": "c6f02933b0ad10a9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:18.480820Z",
     "start_time": "2025-01-18T07:51:18.464440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)"
   ],
   "id": "950255932ed8b97f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:20.865026Z",
     "start_time": "2025-01-18T07:51:18.507271Z"
    }
   },
   "cell_type": "code",
   "source": "original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)",
   "id": "934d4da31f781dd3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:20.886686Z",
     "start_time": "2025-01-18T07:51:20.869263Z"
    }
   },
   "cell_type": "code",
   "source": "len(original_dataset)",
   "id": "ba1afebc6c8b190b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21735"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.008869Z",
     "start_time": "2025-01-18T07:51:20.923693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x, y, z, _ = original_dataset[0]\n",
    "print(x.shape, y.shape, z.shape)"
   ],
   "id": "eb16a6208d8e0d67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 1, 224, 224]) torch.Size([28]) torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.042401Z",
     "start_time": "2025-01-18T07:51:21.026572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True,\n",
    "                      drop_last=True)\n",
    "\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "id": "beea527737fe7ca1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training and Validation\n",
    "## Metrics Calculation\n",
    "### Performance Metrics"
   ],
   "id": "bc7396c943d24607"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.086225Z",
     "start_time": "2025-01-18T07:51:21.069736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "id": "386345f4492a4646",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss Function",
   "id": "a9e07ba62418d236"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.140108Z",
     "start_time": "2025-01-18T07:51:21.123373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    if CHANNELS == 1:\n",
    "        bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "        bce_loss = bce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    else:\n",
    "        ce_loss_fn = nn.CrossEntropyLoss()\n",
    "        ce_loss = ce_loss_fn(outputs.squeeze(), target.float())\n",
    "        kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "        total_loss = (1 - alpha) * ce_loss + alpha * kl_divergence\n",
    "\n",
    "    return total_loss"
   ],
   "id": "e06a6fe162be7161",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Loop",
   "id": "9be9d852dea260a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.199768Z",
     "start_time": "2025-01-18T07:51:21.168617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, optimizer, variational_ngd_optimizer, scheduler,\n",
    "                scaler, device):\n",
    "    total_loss = 0.0\n",
    "    total_nlls = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "        batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if GP_MODEL == 'single_task':\n",
    "            for i in range(NUM_CLASSES):\n",
    "                variational_ngd_optimizer[i].zero_grad()\n",
    "\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "\n",
    "            if GP_MODEL == 'single_task':\n",
    "                loss = 0\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                loss.mean()\n",
    "                loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "\n",
    "                probs = [likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)]\n",
    "                probabilities = torch.stack(probs, dim=1)\n",
    "                preds = (probabilities >= 0.5).int()\n",
    "\n",
    "            else:\n",
    "                loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                loss = loss.mean()\n",
    "                preds = (outputs >= 0.5).int()\n",
    "\n",
    "            predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if GP_MODEL == 'single_task':\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    variational_ngd_optimizer[i].step()\n",
    "            scheduler.step()\n",
    "\n",
    "        if NUM_CLASSES == 1:\n",
    "            labels.extend(batch_patient_labels.cpu().numpy())\n",
    "        else:\n",
    "            labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def validate(model, likelihoods, data_loader, criterion_cl, criterion_bce, mlls, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    alpha = 0.5\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    loss = 0\n",
    "                    for i in range(NUM_CLASSES):\n",
    "                        loss += -mlls[i](gp_outputs[i], batch_multi_labels[:, i])\n",
    "                    loss.mean()\n",
    "                    loss += 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    loss = -mlls(gp_outputs, batch_multi_labels) * 0.5 + 0.5 * criterion_bce(outputs, batch_multi_labels)\n",
    "                    loss = loss.mean()\n",
    "                    total_loss += loss.item()\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "\n",
    "\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "    return total_loss / len(data_loader), predictions, labels\n",
    "\n",
    "\n",
    "def train_model(model, likelihoods, train_loader, val_loader, criterion_cl, criterion_bce, optimizer, num_epochs, learning_rate,\n",
    "                device='cuda'):\n",
    "    \"\"\"Train the model and return the best model based on validation accuracy.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    likelihoods.train()\n",
    "\n",
    "    # Initialize Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "    if GP_MODEL == 'single_task':\n",
    "        mlls = [gpytorch.mlls.VariationalELBO(likelihoods[i], model.gp_layers[i], num_data=len(train_loader.dataset)) for\n",
    "                i in range(NUM_CLASSES)]\n",
    "        mlls = [mll.to(device) for mll in mlls]\n",
    "\n",
    "        variational_ngd_optimizer = [\n",
    "            gpytorch.optim.NGD(model.gp_layers[i].variational_parameters(), num_data=len(train_loader.dataset),\n",
    "                               lr=0.01) for i in range(NUM_CLASSES)]\n",
    "    else:\n",
    "        mlls = gpytorch.mlls.VariationalELBO(likelihoods, model.gp_layers, num_data=len(train_loader.dataset))\n",
    "        mlls = mlls.to(device)\n",
    "        variational_ngd_optimizer = None\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.5 * num_epochs, 0.75 * num_epochs], gamma=0.1)\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        train_loss, train_predictions, train_labels = train_epoch(model, likelihoods, train_loader, criterion_cl, criterion_bce,\n",
    "                                                                  mlls, optimizer, variational_ngd_optimizer,\n",
    "                                                                  scheduler, scaler, device)\n",
    "        train_metrics = calculate_metrics(train_predictions, train_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"train\", train_loss, train_metrics)\n",
    "        # Log training metrics to W&B\n",
    "        wandb.log({\n",
    "            \"train/loss\": train_loss,\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_predictions, val_labels = validate(model, likelihoods, val_loader, criterion_cl, criterion_bce, mlls,\n",
    "                                                         device)\n",
    "        val_metrics = calculate_metrics(val_predictions, val_labels)\n",
    "        print_epoch_stats(epoch, num_epochs, \"validation\", val_loss, val_metrics)\n",
    "        # Log validation metrics to W&B\n",
    "        wandb.log({\n",
    "            \"val/loss\": val_loss,\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "        })\n",
    "\n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_metrics[\"accuracy\"], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        # Save best model\n",
    "        if val_metrics['accuracy'] > best_val_accuracy:\n",
    "            best_val_accuracy = val_metrics['accuracy']\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    # Optionally log the best model to W&B (if desired)\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy}')\n",
    "    wandb.log_artifact(wandb.Artifact(\"best_model\", type=\"model\", metadata={\"accuracy\": best_val_accuracy}))\n",
    "\n",
    "    return model"
   ],
   "id": "7c288284804281a1",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Evaluation",
   "id": "9ee0299f417b29b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.246784Z",
     "start_time": "2025-01-18T07:51:21.226860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Model Evaluation Functions\n",
    "def evaluate_model(model, likelihoods, data_loader, device='cuda'):\n",
    "    \"\"\"Evaluate the model on the given data loader.\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, att_outputs = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print the calculated metrics.\"\"\"\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, \"\n",
    "          f\"Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "id": "e193f0efccd4b5a6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.298173Z",
     "start_time": "2025-01-18T07:51:21.276270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Visualization Functions\n",
    "def plot_roc_curve(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the ROC curve for the model predictions.\"\"\"\n",
    "    model.eval()\n",
    "    likelihoods.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels, batch_patient_labels, batch_multi_labels in data_loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_patient_labels = batch_patient_labels.float().to(device)\n",
    "            batch_multi_labels = batch_multi_labels.float().to(device)\n",
    "\n",
    "            if TRAINING_TYPE == 'end_to_end':\n",
    "                outputs, gp_outputs, _ = model(batch_data)\n",
    "                if GP_MODEL == 'single_task':\n",
    "                    probabilities = torch.stack([likelihoods[i](gp_outputs[i]).probs for i in range(NUM_CLASSES)], dim=1)\n",
    "                    preds = (probabilities >= 0.5).int()\n",
    "\n",
    "                else:\n",
    "                    preds = (outputs >= 0.5).int()\n",
    "                predictions.extend(preds.cpu().detach().numpy())\n",
    "\n",
    "            if NUM_CLASSES == 1:  # Binary classification\n",
    "                labels.extend(batch_patient_labels.cpu().numpy())\n",
    "            else:\n",
    "                labels.extend(batch_multi_labels.cpu().numpy())\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, likelihoods, data_loader, device):\n",
    "    \"\"\"Plot the confusion matrix for the model predictions.\"\"\"\n",
    "    predictions, labels = evaluate_model(model, likelihoods, data_loader, device)\n",
    "\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ],
   "id": "8615707218fcdd04",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Helper Functions",
   "id": "5d7348e9d5406c83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.344368Z",
     "start_time": "2025-01-18T07:51:21.324483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Data Processing Functions\n",
    "def load_model(model_class, model_path, params):\n",
    "    \"\"\"Load a trained model from a file.\"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "    model = model_class(params)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=torch.device('cuda'), weights_only=True)\n",
    "        if not state_dict:\n",
    "            raise ValueError(f\"The state dictionary loaded from {model_path} is empty\")\n",
    "        model.load_state_dict(state_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {str(e)}\")\n",
    "        print(\"Initializing model with random weights instead.\")\n",
    "        return model  # Return the model with random initialization\n",
    "\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "def get_test_results(model, test_loader, test_labels, device=DEVICE):\n",
    "    \"\"\"Get test results including patient information.\"\"\"\n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    results = []\n",
    "    for i, row in enumerate(test_labels.itertuples(index=False)):\n",
    "        result = {col: getattr(row, col) for col in test_labels.columns}\n",
    "        result['prediction'] = predictions[i]\n",
    "        results.append(result)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ],
   "id": "c3875223d716184",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NTXentLoss",
   "id": "5b3fdb7f6cd302d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.395638Z",
     "start_time": "2025-01-18T07:51:21.372160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NTXentLoss(losses.NTXentLoss):\n",
    "    def __init__(self, temperature, **kwargs):\n",
    "        super().__init__(temperature=temperature, **kwargs)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings, labels=None, hard_pairs=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        if labels == None:\n",
    "            # Self-supervised labels\n",
    "            batch_size = feature_vectors_normalized.size(0) // 2  # Assuming equal size for both embeddings\n",
    "            labels = torch.cat([torch.arange(batch_size), torch.arange(batch_size)], dim=0)\n",
    "\n",
    "        # Compute logits\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "\n",
    "        if labels == None:\n",
    "            return losses.NTXentLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        if hard_pairs == None:\n",
    "            return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels))\n",
    "        return losses.SupConLoss(temperature=self.temperature)(logits, torch.squeeze(labels), hard_pairs)"
   ],
   "id": "e681882fb0a1ef75",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main",
   "id": "d3e00d1173682fbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T07:51:21.446287Z",
     "start_time": "2025-01-18T07:51:21.422533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main(mode='train'):\n",
    "    # os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=\"MIL_Resnet_ICH\")\n",
    "\n",
    "    # Log hyperparameters\n",
    "    config = wandb.config\n",
    "    config.learning_rate = LEARNING_RATE\n",
    "    config.batch_size = TRAIN_BATCH_SIZE\n",
    "    config.num_epochs = NUM_EPOCHS\n",
    "\n",
    "    # train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "    # test_labels = pd.read_csv('./data_analyze/testing_dataset_150_redundancy.csv')\n",
    "    train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "\n",
    "    train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "    val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "    test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)\n",
    "\n",
    "    params = {\n",
    "        'channels': CHANNELS,  # Number of input channels (e.g., 1 for grayscale, 3 for RGB)\n",
    "        'num_classes': NUM_CLASSES,  # Number of output classes for classification\n",
    "        'drop_prob': 0.5,  # Dropout probability\n",
    "        'inducing_points': INDUCING_POINTS,  # Number of inducing points for the Gaussian Process layer\n",
    "        'projection_location': PROJECTION_LOCATION,  # Choose from 'after_resnet', 'after_attention', or 'after_gp'\n",
    "        'projection_hidden_dim': PROJECTION_HIDDEN_DIM,  # Hidden dimension size for the projection head\n",
    "        'projection_output_dim': PROJECTION_OUTPUT_DIM,  # Output dimension size for the projection head\n",
    "        'attention_hidden_dim': ATTENTION_HIDDEN_DIM,  # Hidden dimension size for the attention head\n",
    "        'gp_model': GP_MODEL,\n",
    "        'kernel_type': GP_KERNEL,\n",
    "        'model_type': MODEL_TYPE\n",
    "    }\n",
    "\n",
    "    if TRAINING_TYPE == 'end_to_end':\n",
    "        # Instantiate the CNN_GP_ATT model with the specified parameters\n",
    "        if NUM_CLASSES == 1:\n",
    "            model = CNN_ATT_GP(params)\n",
    "            likelihood = PGLikelihood()\n",
    "            optimizer = optim.Adam([\n",
    "                {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "            ])\n",
    "        else:\n",
    "            if GP_MODEL == 'single_task':\n",
    "                model = CNN_ATT_GP_Multilabel(params)\n",
    "                likelihood = nn.ModuleList([PGLikelihood() for _ in range(NUM_CLASSES)])\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "            else:\n",
    "                model = CNN_ATT_GP_MIML(params)\n",
    "                likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=NUM_CLASSES)\n",
    "                optimizer = optim.Adam([\n",
    "                    {'params': model.parameters(), 'lr': config.learning_rate},\n",
    "                    {'params': likelihood.parameters(), 'lr': config.learning_rate}\n",
    "                ])\n",
    "\n",
    "\n",
    "    criterion_cl = NTXentLoss(0.5)\n",
    "    criterion_bce = nn.BCEWithLogitsLoss()\n",
    "    criterion_bce_wll = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if mode == 'train':\n",
    "        if TRAINING_TYPE == 'end_to_end':\n",
    "            wandb.watch(model)  # Watch the model to log gradients and parameters\n",
    "            trained_model = train_model(model, likelihood, train_loader, val_loader, criterion_cl, criterion_bce,\n",
    "                                        optimizer, config.num_epochs, config.learning_rate, DEVICE)\n",
    "\n",
    "            predictions, labels = evaluate_model(trained_model, likelihood, test_loader, DEVICE)\n",
    "            metrics = calculate_metrics(predictions, labels)\n",
    "            wandb.log(metrics)\n",
    "            print_metrics(metrics)\n",
    "\n",
    "            # plot_roc_curve(trained_model, likelihood, test_loader, DEVICE)\n",
    "            # plot_confusion_matrix(trained_model, likelihood, test_loader, DEVICE)\n",
    "            torch.save(trained_model.state_dict(), MODEL_PATH)\n",
    "\n",
    "    # if TRAINING_TYPE == 'end_to_end':\n",
    "    #     trained_model = load_model(CNN_ATT_GP, MODEL_PATH, params)\n",
    "    #     predictions, labels = evaluate_model(trained_model, test_loader, DEVICE)\n",
    "    #\n",
    "    # metrics = calculate_metrics(predictions, labels)\n",
    "    # wandb.log(metrics)\n",
    "    # print_metrics(metrics)"
   ],
   "id": "7a3e7980201a6cc5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Results",
   "id": "c7ed10b588911041"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:52.962642Z",
     "start_time": "2025-01-18T07:51:21.478343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(mode='train')"
   ],
   "id": "1498cc78442856e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mhuynhsikha2003\u001B[0m (\u001B[33mhuynhsikha2003-i-h-c-qu-c-gia-tp-hcm\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/media/hskha23/Kha/Brain-Stroke-Diagnosis/rsna/wandb/run-20250118_145122-mlcdqonm</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/mlcdqonm' target=\"_blank\">trim-terrain-451</a></strong> to <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/mlcdqonm' target=\"_blank\">https://wandb.ai/huynhsikha2003-i-h-c-qu-c-gia-tp-hcm/MIL_Resnet_ICH/runs/mlcdqonm</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train:\n",
      "Loss: 8.9168, Accuracy: 0.5689, Precision: 0.5580, Recall: 0.1129, F1: 0.1686\n",
      "Epoch 1/50 - Validation:\n",
      "Loss: 8.6291, Accuracy: 0.5700, Precision: 0.4761, Recall: 0.1981, F1: 0.2653\n",
      "Epoch 2/50 - Train:\n",
      "Loss: 8.2961, Accuracy: 0.5882, Precision: 0.6450, Recall: 0.2661, F1: 0.3486\n",
      "Epoch 2/50 - Validation:\n",
      "Loss: 7.9225, Accuracy: 0.6135, Precision: 0.6915, Recall: 0.2541, F1: 0.3376\n",
      "Epoch 3/50 - Train:\n",
      "Loss: 6.6668, Accuracy: 0.6150, Precision: 0.6880, Recall: 0.3116, F1: 0.4050\n",
      "Epoch 3/50 - Validation:\n",
      "Loss: 5.0716, Accuracy: 0.6241, Precision: 0.6985, Recall: 0.3530, F1: 0.4452\n",
      "Epoch 4/50 - Train:\n",
      "Loss: 4.4070, Accuracy: 0.6284, Precision: 0.7130, Recall: 0.3406, F1: 0.4387\n",
      "Epoch 4/50 - Validation:\n",
      "Loss: 3.9355, Accuracy: 0.6386, Precision: 0.6936, Recall: 0.3958, F1: 0.4924\n",
      "Epoch 5/50 - Train:\n",
      "Loss: 3.7034, Accuracy: 0.6388, Precision: 0.7060, Recall: 0.3785, F1: 0.4724\n",
      "Epoch 5/50 - Validation:\n",
      "Loss: 3.5286, Accuracy: 0.6460, Precision: 0.6811, Recall: 0.4564, F1: 0.5394\n",
      "Epoch 6/50 - Train:\n",
      "Loss: 3.4406, Accuracy: 0.6458, Precision: 0.7115, Recall: 0.4094, F1: 0.5019\n",
      "Epoch 6/50 - Validation:\n",
      "Loss: 3.3625, Accuracy: 0.6402, Precision: 0.7728, Recall: 0.3032, F1: 0.4081\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 7/50 - Train:\n",
      "Loss: 3.2772, Accuracy: 0.6552, Precision: 0.7112, Recall: 0.4367, F1: 0.5256\n",
      "Epoch 7/50 - Validation:\n",
      "Loss: 3.1969, Accuracy: 0.6534, Precision: 0.7725, Recall: 0.3683, F1: 0.4591\n",
      "Epoch 8/50 - Train:\n",
      "Loss: 3.1012, Accuracy: 0.6690, Precision: 0.7209, Recall: 0.4679, F1: 0.5566\n",
      "Epoch 8/50 - Validation:\n",
      "Loss: 3.0170, Accuracy: 0.6638, Precision: 0.7773, Recall: 0.3646, F1: 0.4668\n",
      "Epoch 9/50 - Train:\n",
      "Loss: 2.8917, Accuracy: 0.6911, Precision: 0.7318, Recall: 0.4998, F1: 0.5851\n",
      "Epoch 9/50 - Validation:\n",
      "Loss: 2.7828, Accuracy: 0.6774, Precision: 0.7821, Recall: 0.4088, F1: 0.5210\n",
      "Epoch 10/50 - Train:\n",
      "Loss: 2.6402, Accuracy: 0.7051, Precision: 0.7369, Recall: 0.5244, F1: 0.6056\n",
      "Epoch 10/50 - Validation:\n",
      "Loss: 2.4898, Accuracy: 0.6880, Precision: 0.7117, Recall: 0.5686, F1: 0.6269\n",
      "Epoch 11/50 - Train:\n",
      "Loss: 2.2521, Accuracy: 0.7084, Precision: 0.7410, Recall: 0.5277, F1: 0.6089\n",
      "Epoch 11/50 - Validation:\n",
      "Loss: 2.0741, Accuracy: 0.6948, Precision: 0.7635, Recall: 0.4758, F1: 0.5667\n",
      "Epoch 12/50 - Train:\n",
      "Loss: 1.9008, Accuracy: 0.7211, Precision: 0.7456, Recall: 0.5472, F1: 0.6250\n",
      "Epoch 12/50 - Validation:\n",
      "Loss: 1.7819, Accuracy: 0.6976, Precision: 0.7373, Recall: 0.5121, F1: 0.5939\n",
      "Epoch 13/50 - Train:\n",
      "Loss: 1.5577, Accuracy: 0.7229, Precision: 0.7452, Recall: 0.5571, F1: 0.6317\n",
      "Epoch 13/50 - Validation:\n",
      "Loss: 1.4013, Accuracy: 0.7175, Precision: 0.7295, Recall: 0.5647, F1: 0.6323\n",
      "Epoch 14/50 - Train:\n",
      "Loss: 1.2255, Accuracy: 0.7301, Precision: 0.7418, Recall: 0.5699, F1: 0.6396\n",
      "Epoch 14/50 - Validation:\n",
      "Loss: 1.1054, Accuracy: 0.7249, Precision: 0.7192, Recall: 0.5868, F1: 0.6445\n",
      "Epoch 15/50 - Train:\n",
      "Loss: 0.9204, Accuracy: 0.7351, Precision: 0.7450, Recall: 0.5805, F1: 0.6479\n",
      "Epoch 15/50 - Validation:\n",
      "Loss: 0.8609, Accuracy: 0.6935, Precision: 0.7598, Recall: 0.4770, F1: 0.5642\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Epoch 16/50 - Train:\n",
      "Loss: 0.6495, Accuracy: 0.7432, Precision: 0.7497, Recall: 0.5893, F1: 0.6555\n",
      "Epoch 16/50 - Validation:\n",
      "Loss: 0.6958, Accuracy: 0.7225, Precision: 0.7058, Recall: 0.6050, F1: 0.6487\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Epoch 17/50 - Train:\n",
      "Loss: 0.4427, Accuracy: 0.7478, Precision: 0.7478, Recall: 0.5970, F1: 0.6597\n",
      "Epoch 17/50 - Validation:\n",
      "Loss: 0.4931, Accuracy: 0.7039, Precision: 0.7097, Recall: 0.5741, F1: 0.6304\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Epoch 18/50 - Train:\n",
      "Loss: 0.2718, Accuracy: 0.7574, Precision: 0.7533, Recall: 0.6103, F1: 0.6702\n",
      "Epoch 18/50 - Validation:\n",
      "Loss: 0.5339, Accuracy: 0.7132, Precision: 0.7001, Recall: 0.5961, F1: 0.6416\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Epoch 19/50 - Train:\n",
      "Loss: 0.1356, Accuracy: 0.7579, Precision: 0.7521, Recall: 0.6154, F1: 0.6730\n",
      "Epoch 19/50 - Validation:\n",
      "Loss: 0.3788, Accuracy: 0.7203, Precision: 0.7331, Recall: 0.5529, F1: 0.6253\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Epoch 20/50 - Train:\n",
      "Loss: 0.0306, Accuracy: 0.7657, Precision: 0.7543, Recall: 0.6206, F1: 0.6769\n",
      "Epoch 20/50 - Validation:\n",
      "Loss: 0.3434, Accuracy: 0.7138, Precision: 0.7545, Recall: 0.5156, F1: 0.6022\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Epoch 21/50 - Train:\n",
      "Loss: -0.0780, Accuracy: 0.7722, Precision: 0.7584, Recall: 0.6287, F1: 0.6829\n",
      "Epoch 21/50 - Validation:\n",
      "Loss: 0.2954, Accuracy: 0.7223, Precision: 0.7088, Recall: 0.5944, F1: 0.6427\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Epoch 22/50 - Train:\n",
      "Loss: -0.1801, Accuracy: 0.7813, Precision: 0.7609, Recall: 0.6421, F1: 0.6925\n",
      "Epoch 22/50 - Validation:\n",
      "Loss: 0.2757, Accuracy: 0.7138, Precision: 0.7278, Recall: 0.5639, F1: 0.6286\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Epoch 23/50 - Train:\n",
      "Loss: -0.2426, Accuracy: 0.7789, Precision: 0.7573, Recall: 0.6419, F1: 0.6910\n",
      "Epoch 23/50 - Validation:\n",
      "Loss: 0.3126, Accuracy: 0.7188, Precision: 0.7086, Recall: 0.5938, F1: 0.6450\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Epoch 24/50 - Train:\n",
      "Loss: -0.3313, Accuracy: 0.7863, Precision: 0.7594, Recall: 0.6514, F1: 0.6974\n",
      "Epoch 24/50 - Validation:\n",
      "Loss: 0.2636, Accuracy: 0.7173, Precision: 0.7298, Recall: 0.5614, F1: 0.6283\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered.\n",
      "Best Validation Accuracy: 0.7248700173310225\n",
      "Test Accuracy: 0.7202, Precision: 0.7295, Recall: 0.5630, F1: 0.6291\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parse patients labels for multi-label classification",
   "id": "eaa15a462b988a19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:53.120646Z",
     "start_time": "2025-01-18T22:06:53.059517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import ast\n",
    "#\n",
    "# df = pd.read_csv('./data_analyze/training_dataset_1_redundancy.csv')\n",
    "# # Parsing string columns into lists\n",
    "# columns_to_parse = ['any', 'subdural', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid']\n",
    "# for col in columns_to_parse:\n",
    "#     df[col] = df[col].apply(ast.literal_eval)\n",
    "#\n",
    "# # Creating new columns with prefix 'patient_' based on the parsed data\n",
    "# for col in columns_to_parse:\n",
    "#     df[f'patient_{col}'] = df[col].apply(lambda x: int(any(x)))\n",
    "# # Save to csv\n",
    "# df.to_csv('./data_analyze/training_dataset_2_redundancy.csv', index=False)\n",
    "#\n",
    "# # Displaying the updated DataFrame\n",
    "# df.head(10)"
   ],
   "id": "d751f98b2c61a964",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:53.768929Z",
     "start_time": "2025-01-18T22:06:53.298540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "patient_scan_labels = pd.read_csv('./data_analyze/training_dataset_2_redundancy.csv')\n",
    "\n",
    "train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "\n",
    "# Define new columns for hemorrhage types (these should be in your original DataFrame)\n",
    "hemorrhage_columns = ['patient_any', 'patient_subdural', 'patient_epidural', 'patient_intraparenchymal', 'patient_intraventricular', 'patient_subarachnoid']\n",
    "\n",
    "# Function to count labels and unique patients\n",
    "def count_hemorrhage_types(data):\n",
    "    counts = {col: data[col].sum() for col in hemorrhage_columns}\n",
    "    unique_patients = {col: data[data[col] == 1].shape[0] for col in hemorrhage_columns}\n",
    "    return counts, unique_patients\n",
    "\n",
    "# Count for training set\n",
    "train_counts, train_unique_patients = count_hemorrhage_types(train_labels)\n",
    "\n",
    "# Count for validation set\n",
    "val_counts, val_unique_patients = count_hemorrhage_types(val_labels)\n",
    "\n",
    "# Count for test set\n",
    "test_counts, test_unique_patients = count_hemorrhage_types(test_labels)\n",
    "\n",
    "# # Display results\n",
    "# print(\"Training Set Counts:\", train_counts)\n",
    "# print(\"Training Set Unique Patients:\", train_unique_patients)\n",
    "#\n",
    "# print(\"Validation Set Counts:\", val_counts)\n",
    "# print(\"Validation Set Unique Patients:\", val_unique_patients)\n",
    "#\n",
    "# print(\"Test Set Counts:\", test_counts)\n",
    "# print(\"Test Set Unique Patients:\", test_unique_patients)"
   ],
   "id": "90589331e42a2436",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T22:06:53.800056Z",
     "start_time": "2025-01-18T22:06:53.779854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creating the summary data for the table\n",
    "summary_data = {\n",
    "    'Hemorrhage Type': ['patient_any',\n",
    "                        'patient_subdural',\n",
    "                        'patient_epidural',\n",
    "                        'patient_intraparenchymal',\n",
    "                        'patient_intraventricular',\n",
    "                        'patient_subarachnoid'],\n",
    "\n",
    "    # Counts from each dataset\n",
    "    'Train Counts': [train_counts[key] for key in train_counts],\n",
    "\n",
    "    # Validation counts\n",
    "    'Validation Counts': [val_counts[key] for key in val_counts],\n",
    "\n",
    "    # Test counts\n",
    "    'Test Counts': [test_counts[key] for key in test_counts],\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the summary table\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display the summary table\n",
    "summary_table"
   ],
   "id": "4406bd06ed83e5f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Hemorrhage Type  Train Counts  Validation Counts  Test Counts\n",
       "0               patient_any          5626               1876         1324\n",
       "1          patient_subdural          2417                803          561\n",
       "2          patient_epidural           223                 74           52\n",
       "3  patient_intraparenchymal          3350               1117          788\n",
       "4  patient_intraventricular          2338                779          550\n",
       "5      patient_subarachnoid          2464                822          580"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hemorrhage Type</th>\n",
       "      <th>Train Counts</th>\n",
       "      <th>Validation Counts</th>\n",
       "      <th>Test Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>patient_any</td>\n",
       "      <td>5626</td>\n",
       "      <td>1876</td>\n",
       "      <td>1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient_subdural</td>\n",
       "      <td>2417</td>\n",
       "      <td>803</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patient_epidural</td>\n",
       "      <td>223</td>\n",
       "      <td>74</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient_intraparenchymal</td>\n",
       "      <td>3350</td>\n",
       "      <td>1117</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>patient_intraventricular</td>\n",
       "      <td>2338</td>\n",
       "      <td>779</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>patient_subarachnoid</td>\n",
       "      <td>2464</td>\n",
       "      <td>822</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
