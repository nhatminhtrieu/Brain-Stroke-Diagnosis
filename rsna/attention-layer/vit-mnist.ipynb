{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Layer",
   "id": "f1711c54d09b9230"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Lib",
   "id": "b6ba25d5a0cf8a44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.xpu import device\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "id": "25da59d31bcdb53b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set a fixed seed value\n",
    "seed_value = 40\n",
    "# Set the random seed for Python's built-in random module\n",
    "random.seed(seed_value)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed_value)\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# If using CUDA, set the seed for GPU as well (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ],
   "id": "6f8f562307b5aaa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "28588d167ac9d9a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configurations",
   "id": "cf316b002181517b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10\n",
    "img_size = 28  # MNIST images are 28x28 pixels\n",
    "num_classes = 1  # Digits from 0 to 9\n",
    "patch_size = 7  # Size of each patch (7x7)\n",
    "embedding_dim = 7*7  # Dimensionality of the embeddings\n",
    "num_heads = 7  # Number of attention heads\n",
    "num_layers = 6  # Number of transformer layers\n",
    "dropout_rate = 0.1  # Dropout rate for regularization"
   ],
   "id": "575f3a2d858c68c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "4494dba04676a5b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self, mnist_data, n_bags=1000, min_instances=3, max_instances=5):\n",
    "        self.mnist_data = mnist_data\n",
    "        self.n_bags = n_bags\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.empty_image = torch.zeros(1, 28, 28)  # Create an empty image tensor (1x28x28)\n",
    "\n",
    "    def create_bags(self):\n",
    "        bags = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(self.n_bags):\n",
    "            # Randomly choose a number of instances for the bag\n",
    "            n_instances = np.random.randint(self.min_instances, self.max_instances + 1)\n",
    "            \n",
    "            # Randomly select instances from the dataset\n",
    "            bag_indices = np.random.choice(len(self.mnist_data), n_instances, replace=False)\n",
    "            bag_images = [self.mnist_data[i][0] for i in bag_indices]\n",
    "            \n",
    "            # Determine the label: 1 if any instance is '9', else 0\n",
    "            label = 1 if any(self.mnist_data[i][1] == 9 for i in bag_indices) else 0\n",
    "            \n",
    "            # Convert images to tensors and pad to ensure exactly 7 instances\n",
    "            bag_images_tensors = [ToTensor()(img) for img in bag_images]\n",
    "            while len(bag_images_tensors) < 7:\n",
    "                bag_images_tensors.append(self.empty_image)  # Pad with empty image\n",
    "            \n",
    "            bags.append(torch.stack(bag_images_tensors))\n",
    "            labels.append(label)\n",
    "\n",
    "        return bags, labels\n",
    "\n",
    "class TrainDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=1000):\n",
    "        super().__init__(mnist_data, n_bags)\n",
    "\n",
    "class TestDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=500):  # Example: fewer bags for testing\n",
    "        super().__init__(mnist_data, n_bags)"
   ],
   "id": "9e66debf05a0c599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set seed for random number generators\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "\n",
    "# Create training dataset generator and generate bags\n",
    "train_generator = TrainDatasetGenerator(mnist_dataset)\n",
    "train_bags, train_labels = train_generator.create_bags()\n",
    "train_loader = DataLoader(list(zip(train_bags, train_labels)), batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# Create test dataset generator and generate bags\n",
    "test_generator = TestDatasetGenerator(mnist_dataset)\n",
    "test_bags, test_labels = test_generator.create_bags()\n",
    "test_loader = DataLoader(list(zip(test_bags, test_labels)), batch_size=32, shuffle=False, drop_last=True)"
   ],
   "id": "7583f9456994b469"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Patch Embeddings",
   "id": "199eaaf7a3803e01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, in_channels, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to create patch embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (N, num_patches, embed_dim).\n",
    "        \"\"\"\n",
    "        # Handle case of 1-channel image (grayscale)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "            \n",
    "        x = self.conv(x)  # Apply convolution to create patches \n",
    "        # After convolution: shape (N, embed_dim, H/patch_size, W/patch_size)\n",
    "        \n",
    "        x = x.flatten(2)  # Flatten patches into a sequence \n",
    "        # After flattening: shape (N, embed_dim, num_patches)\n",
    "        # print(f'Shape of x after patch embedding: {x.shape}')\n",
    "\n",
    "        return x.transpose(1, 2)  # Rearrange dimensions for transformer input \n",
    "        # Final output shape: (N, num_patches, embed_dim)"
   ],
   "id": "86f8f672d70f60dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multi-Head Self-Attention Layer",
   "id": "ca76d447440b557a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, attn_weights = self.multihead_attn(query=x, # query embeddings\n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=True) # do we need the weights or just the layer outputs?\n",
    "        assert type(attn_output) == torch.Tensor, \"The MSA block output should be a PyTorch tensor.\"\n",
    "        return attn_output"
   ],
   "id": "33c6c3612d948daf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MLP Block Layer",
   "id": "d395e6398041751f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ],
   "id": "c767ff2be0cbcea0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer Encoder Layer",
   "id": "6be12279ce7861af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "\n",
    "    # 5. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        # x =  self.msa_block(x) + x\n",
    "        attn_output = self.msa_block(x)\n",
    "        assert type(attn_output) == torch.Tensor, \"The MSA block output should be a PyTorch tensor.\"\n",
    "        x = attn_output + x\n",
    "\n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x\n",
    "\n",
    "        return x"
   ],
   "id": "998cb49e4c49c215"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attention Layer",
   "id": "22237d6865cf75f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        # Sequential model for attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # Shape: (input_dim, hidden_dim)\n",
    "            nn.Tanh(),                          # Activation function\n",
    "            nn.Linear(hidden_dim, 1)           # Shape: (hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = self.attention(x)  \n",
    "        # Shape of attention_weights: (batch_size, num_instances, 1)\n",
    "\n",
    "        weights = F.softmax(attention_weights, dim=1)  \n",
    "        # Shape of weights: (batch_size, num_instances, 1)\n",
    "\n",
    "        # Element-wise multiplication followed by summation over num_instances\n",
    "        weighted_sum = (x * weights).sum(dim=1)  \n",
    "        # Shape of weighted_sum: (batch_size, feature_dim)\n",
    "\n",
    "        return weighted_sum, weights.squeeze(-1)  \n",
    "        # Returns weighted sum and attention weights with shape (batch_size, num_instances)"
   ],
   "id": "9ddcf1b2503c8978"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vision Transformer Model",
   "id": "a96bbee7a1ea16a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "\n",
    "        # 3. Make the image size is divisible by the patch size\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "\n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "\n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(img_size=img_size,\n",
    "                                              in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embed_dim=embedding_dim)\n",
    "\n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "\n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "        self.attention = AttentionLayer(input_dim=833)\n",
    "\n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        # 12. Get batch size\n",
    "        batch_size, num_instance = x.shape[0], x.shape[1]\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size * num_instance, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "        # print(f'Shape of x after patch embedding: {x.shape}')\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        # 16. Add position embedding to patch embedding (equation 1)\n",
    "        x = self.position_embedding + x\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "        # print(f'Shape of x after transformer encoder: {x.shape}')\n",
    "        \n",
    "        x = x.view(batch_size, num_instance, -1, embedding_dim)\n",
    "        # print(f'Shape of x after transformer encoder and reshape: {x.shape}')\n",
    "        x = x.max(dim=1)[0]\n",
    "        # print(f'Shape of x after mean pooling: {x.shape}')\n",
    "        \n",
    "        # x = x.view(batch_size, num_instance, -1)\n",
    "        # # print(f'Shape of x after transformer encoder and reshape: {x.shape}')\n",
    "        # attention_features, attention_weights = self.attention(x)\n",
    "        # # print(f'Shape of attention_features: {attention_features.shape}')\n",
    "        # x = attention_features.view(-1, 833)\n",
    "        # # print(f'Shape of x after attention layer: {x.shape}')\n",
    "        # x = x.view(batch_size, 17, 49)\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "        \n",
    "        # print(f'Shape of x after classifier: {x.shape}')\n",
    "        return torch.sigmoid(x)"
   ],
   "id": "afe77e9209ab6107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model & Optimizer Definition",
   "id": "e9b0a52ecb369670"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = ViT(\n",
    "    img_size=img_size,\n",
    "    in_channels=1,\n",
    "    patch_size=patch_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_transformer_layers=num_layers,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "cbb38dd7cf63b4d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Step",
   "id": "6d4f2dc071d08c5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, dataloader, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_images.float())\n",
    "            # batch_labels = batch_labels.view(-1, 1)\n",
    "            loss = criterion(outputs.squeeze(), batch_labels.float())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "            # all_outputs.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            # print(f'Acc {outputs.argmax(dim=1).cpu().numpy()}')\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(all_labels, all_outputs)\n",
    "        recall = recall_score(all_labels, all_outputs)\n",
    "        precision = precision_score(all_labels, all_outputs)\n",
    "        f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f},F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ViT(\n",
    "    img_size=img_size,\n",
    "    in_channels=1,\n",
    "    patch_size=patch_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_transformer_layers=num_layers,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "train(model, train_loader)"
   ],
   "id": "f41247196e92fc74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    all_attended_weights = []  # Store attended weights for visualization\n",
    "    images_to_plot = []  # Store images with label = 1\n",
    "    weights_to_plot = []  # Store attended weights for images with label = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_images.float())\n",
    "\n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "\n",
    "            # # Check for images with label = 1\n",
    "            # for i in range(len(batch_labels)):\n",
    "            #     if batch_labels[i] == 1:\n",
    "            #         images_to_plot.append(batch_images[i].cpu().numpy())\n",
    "            #         weights_to_plot.append(attended_weights[i].squeeze().cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_outputs)\n",
    "    recall = recall_score(all_labels, all_outputs)\n",
    "    precision = precision_score(all_labels, all_outputs)\n",
    "    f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f},F1 Score: {f1:.4f}')\n",
    "\n",
    "    # # Plotting attended weights for images with label = 1\n",
    "    # if images_to_plot:  # Check if there are any images to plot\n",
    "    #     # plot_attended_weights(np.array(images_to_plot), np.array(weights_to_plot))\n",
    "    #     plot_self_attention(np.array(images_to_plot[0]), np.array(weights_to_plot))\n",
    "\n",
    "# Call the test function with your model and test loader\n",
    "test(model, test_loader)"
   ],
   "id": "908b15d95091a810"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "98ccbf72f9bb2576"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "57ccffb72f169234"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References:\n",
    "[1] https://medium.com/@wangdk93/implement-self-attention-and-cross-attention-in-pytorch-1f1a366c9d4b"
   ],
   "id": "b97f550eeddc7f7a"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
