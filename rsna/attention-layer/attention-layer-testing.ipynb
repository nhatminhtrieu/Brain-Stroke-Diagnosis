{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Layer",
   "id": "4a63a61a889046d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Lib",
   "id": "8ef192d740d3f9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:38.839443Z",
     "start_time": "2024-10-14T06:37:38.833791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.xpu import device\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "id": "743be51811c9768d",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:38.858767Z",
     "start_time": "2024-10-14T06:37:38.856197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set a fixed seed value\n",
    "seed_value = 40\n",
    "\n",
    "# Set the random seed for Python's built-in random module\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# If using CUDA, set the seed for GPU as well (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n"
   ],
   "id": "6b8e1153f8ca7340",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:38.902673Z",
     "start_time": "2024-10-14T06:37:38.900665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "590e5508bccbb47b",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "9d7ae223d9dfa27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:38.949063Z",
     "start_time": "2024-10-14T06:37:38.945674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class DatasetGenerator:\n",
    "    def __init__(self, mnist_data, n_bags=1000, min_instances=3, max_instances=5):\n",
    "        self.mnist_data = mnist_data\n",
    "        self.n_bags = n_bags\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.empty_image = torch.zeros(1, 28, 28)  # Create an empty image tensor (1x28x28)\n",
    "\n",
    "    def create_bags(self):\n",
    "        bags = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(self.n_bags):\n",
    "            # Randomly choose a number of instances for the bag\n",
    "            n_instances = np.random.randint(self.min_instances, self.max_instances + 1)\n",
    "            \n",
    "            # Randomly select instances from the dataset\n",
    "            bag_indices = np.random.choice(len(self.mnist_data), n_instances, replace=False)\n",
    "            bag_images = [self.mnist_data[i][0] for i in bag_indices]\n",
    "            \n",
    "            # Determine the label: 1 if any instance is '9', else 0\n",
    "            label = 1 if any(self.mnist_data[i][1] == 9 for i in bag_indices) else 0\n",
    "            \n",
    "            # Convert images to tensors and pad to ensure exactly 7 instances\n",
    "            bag_images_tensors = [ToTensor()(img) for img in bag_images]\n",
    "            while len(bag_images_tensors) < 7:\n",
    "                bag_images_tensors.append(self.empty_image)  # Pad with empty image\n",
    "            \n",
    "            bags.append(torch.stack(bag_images_tensors))\n",
    "            labels.append(label)\n",
    "\n",
    "        return bags, labels\n",
    "\n",
    "class TrainDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=1000):\n",
    "        super().__init__(mnist_data, n_bags)\n",
    "\n",
    "class TestDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=500):  # Example: fewer bags for testing\n",
    "        super().__init__(mnist_data, n_bags)"
   ],
   "id": "b0c74e22e200f87c",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:39.911358Z",
     "start_time": "2024-10-14T06:37:38.992092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "\n",
    "# Create training dataset generator and generate bags\n",
    "train_generator = TrainDatasetGenerator(mnist_dataset)\n",
    "train_bags, train_labels = train_generator.create_bags()\n",
    "train_loader = DataLoader(list(zip(train_bags, train_labels)), batch_size=32, shuffle=True)\n",
    "\n",
    "# Create test dataset generator and generate bags\n",
    "test_generator = TestDatasetGenerator(mnist_dataset)\n",
    "test_bags, test_labels = test_generator.create_bags()\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(list(zip(test_bags, test_labels)), batch_size=16, shuffle=True)"
   ],
   "id": "a04b09c68297d96f",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attention Layer",
   "id": "222827e790c53375"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:39.921398Z",
     "start_time": "2024-10-14T06:37:39.919392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class SelfAttention(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(SelfAttention, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.query = nn.Linear(input_dim, input_dim)\n",
    "#         self.key = nn.Linear(input_dim, input_dim)\n",
    "#         self.value = nn.Linear(input_dim, input_dim)\n",
    "#         self.softmax = nn.Softmax(dim=2)\n",
    "# \n",
    "#     def forward(self, x):  # x.shape (batch_size, seq_length, input_dim)\n",
    "#         queries = self.query(x)\n",
    "#         keys = self.key(x)\n",
    "#         values = self.value(x)\n",
    "# \n",
    "#         score = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "#         attention = self.softmax(score)\n",
    "#         weighted = torch.bmm(attention, values)\n",
    "#         return weighted, attention\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.ReLU(),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "id": "a0c5dbb8cfefc24c",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MIL-CNN Model",
   "id": "f63c6e77d47a7914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:39.962483Z",
     "start_time": "2024-10-14T06:37:39.960695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class MILResNet18(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MILResNet18, self).__init__()\n",
    "#         self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "#         # Modify the first convolutional layer to accept grayscale images\n",
    "#         # Change in_channels from 3 to 1\n",
    "#         self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#         self.resnet.fc = nn.Identity()  # Remove the final classification layer\n",
    "#         # self.resnet.maxpool = nn.AdaptiveAvgPool2d((1, 1))  # Replace max pooling with average pooling\n",
    "#         self.attention = AttentionLayer(input_dim=512)  # Assuming output dim from ResNet is 512\n",
    "#         self.classifier = nn.Linear(512, 1)  # Binary classification\n",
    "#         self.dropout = nn.Dropout(0.4)\n",
    "#       \n",
    "#     def forward(self, bags):\n",
    "#         # bags.shape: (batch_size, num_instances, channels, height, width)\n",
    "#         batch_size, num_instances = bags.size(0), bags.size(1)\n",
    "#         \n",
    "#         # Flatten to (batch_size * num_instances, channels, height, width)\n",
    "#         bags_flattened = bags.view(-1, *bags.shape[2:])\n",
    "#         \n",
    "#         # Get features from ResNet\n",
    "#         features = self.resnet(bags_flattened)  # Shape: (batch_size * num_instances, 512)\n",
    "# \n",
    "#         # Reshape back to (batch_size, num_instances, 512)\n",
    "#         features = features.view(batch_size, num_instances, -1)\n",
    "# \n",
    "#         # Apply attention mechanism\n",
    "#         attended_features, attended_weights = self.attention(features)\n",
    "# \n",
    "#         # Aggregate features (e.g., mean pooling)\n",
    "#         # aggregated_features = attended_features.mean(dim=1)  # Shape: (batch_size, 512)\n",
    "#         \n",
    "#         # dropped_features = self.dropout(aggregated_features)\n",
    "#         dropped_features = self.dropout(attended_features)\n",
    "#         # Classify bag\n",
    "#         outputs = torch.sigmoid(self.classifier(dropped_features))\n",
    "#         \n",
    "#         return outputs, attended_weights"
   ],
   "id": "eeb49b1fb73f9119",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gaussian Process Layer",
   "id": "184e116b03eebef0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:40.013833Z",
     "start_time": "2024-10-14T06:37:40.007340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "\n",
    "class GaussianProcessModel:\n",
    "    def __init__(self, input_dim):\n",
    "        # Khởi tạo Gaussian Process với kernel RBF\n",
    "        self.model = GPy.models.GPRegression(np.zeros((1, input_dim)), np.zeros((1, 1)), kernel=GPy.kern.RBF(input_dim))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Debug: In kích thước của X và y\n",
    "        # print(\"Fitting GPR model...\")\n",
    "        # print(\"Input features shape:\", X.shape)\n",
    "        # print(\"Target labels shape:\", y.shape)\n",
    "\n",
    "        # Đảm bảo y có kích thước đúng\n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)  # Chuyển đổi thành (n_samples, 1)\n",
    "\n",
    "        # Huấn luyện mô hình GPR\n",
    "        self.model.set_XY(X, y)\n",
    "        self.model.optimize()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Dự đoán với mô hình GPR\n",
    "        # print(\"Predicting with GPR model...\")\n",
    "        # print(\"Input features for prediction shape:\", X.shape)\n",
    "        return self.model.predict(X)"
   ],
   "id": "9dabe64878aa072e",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:40.071564Z",
     "start_time": "2024-10-14T06:37:40.059213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.attention = AttentionLayer(input_dim=512)\n",
    "        self.classifier = nn.Linear(512, 1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Khởi tạo mô hình Gaussian Process\n",
    "        self.gpr_model = GaussianProcessModel(input_dim=128)\n",
    "\n",
    "    def forward(self, bags):\n",
    "        batch_size, num_instances = bags.size(0), bags.size(1)\n",
    "        bags_flattened = bags.view(-1, *bags.shape[2:])\n",
    "        \n",
    "        features = self.resnet(bags_flattened)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "\n",
    "        attended_features, attended_weights = self.attention(features)\n",
    "\n",
    "        # Huấn luyện GPR với attended_features và nhãn tương ứng\n",
    "        if hasattr(self, 'gpr_labels'):\n",
    "            if attended_features.shape[0] == self.gpr_labels.shape[0]:\n",
    "                # Chuyển đổi attended_features thành numpy array\n",
    "                self.gpr_model.fit(attended_features.detach().cpu().numpy(), self.gpr_labels.cpu().numpy())\n",
    "            else:\n",
    "                print(f\"Shape mismatch: {attended_features.shape[0]} vs {self.gpr_labels.shape[0]}\")\n",
    "\n",
    "        outputs = torch.sigmoid(self.classifier(attended_features))\n",
    "        \n",
    "        return outputs, attended_weights, attended_features\n",
    "\n",
    "    def set_gpr_labels(self, labels):\n",
    "        # Phương thức để thiết lập nhãn cho GPR\n",
    "        self.gpr_labels = labels.view(-1, 1).float()"
   ],
   "id": "c071f580a946929",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Process",
   "id": "1e9c93e0734c3f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:37:54.393838Z",
     "start_time": "2024-10-14T06:37:40.114635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, dataloader, epochs=20):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            model.set_gpr_labels(batch_labels)  # Set labels for Gaussian Process\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs, attended_weights, attended_features = model(batch_images.float())\n",
    "            loss = criterion(outputs.squeeze(), batch_labels.float())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "            \n",
    "            # Huấn luyện Gaussian Process với attended features và nhãn\n",
    "            # attended_features = attended_weights.detach().cpu().numpy()  # Hoặc sử dụng attended_features nếu cần\n",
    "            model.gpr_model.fit(attended_features.detach().cpu().numpy(), batch_labels.cpu().numpy())\n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(all_labels, all_outputs)\n",
    "        precision = precision_score(all_labels, all_outputs)\n",
    "        f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MILResNet18()\n",
    "model.to(device)\n",
    "\n",
    "train(model, train_loader)"
   ],
   "id": "48899664dd45f891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5799, Accuracy: 0.7500, Precision: 0.6575, F1 Score: 0.4877\n",
      "Epoch [2/20], Loss: 0.3004, Accuracy: 0.8910, Precision: 0.8214, F1 Score: 0.8228\n",
      "Epoch [3/20], Loss: 0.1997, Accuracy: 0.9300, Precision: 0.9100, F1 Score: 0.8826\n",
      "Epoch [4/20], Loss: 0.1195, Accuracy: 0.9610, Precision: 0.9467, F1 Score: 0.9357\n",
      "Epoch [5/20], Loss: 0.0848, Accuracy: 0.9740, Precision: 0.9668, F1 Score: 0.9572\n",
      "Epoch [6/20], Loss: 0.0663, Accuracy: 0.9780, Precision: 0.9582, F1 Score: 0.9644\n",
      "Epoch [7/20], Loss: 0.0690, Accuracy: 0.9800, Precision: 0.9644, F1 Score: 0.9675\n",
      "Epoch [8/20], Loss: 0.0646, Accuracy: 0.9770, Precision: 0.9702, F1 Score: 0.9622\n",
      "Epoch [9/20], Loss: 0.0618, Accuracy: 0.9850, Precision: 0.9740, F1 Score: 0.9756\n",
      "Epoch [10/20], Loss: 0.0474, Accuracy: 0.9880, Precision: 0.9836, F1 Score: 0.9804\n",
      "Epoch [11/20], Loss: 0.0390, Accuracy: 0.9900, Precision: 0.9901, F1 Score: 0.9836\n",
      "Epoch [12/20], Loss: 0.0348, Accuracy: 0.9880, Precision: 0.9836, F1 Score: 0.9804\n",
      "Epoch [13/20], Loss: 0.0659, Accuracy: 0.9780, Precision: 0.9734, F1 Score: 0.9638\n",
      "Epoch [14/20], Loss: 0.0621, Accuracy: 0.9840, Precision: 0.9770, F1 Score: 0.9739\n",
      "Epoch [15/20], Loss: 0.0308, Accuracy: 0.9890, Precision: 0.9837, F1 Score: 0.9821\n",
      "Epoch [16/20], Loss: 0.0236, Accuracy: 0.9950, Precision: 0.9935, F1 Score: 0.9918\n",
      "Epoch [17/20], Loss: 0.0183, Accuracy: 0.9960, Precision: 1.0000, F1 Score: 0.9934\n",
      "Epoch [18/20], Loss: 0.0686, Accuracy: 0.9830, Precision: 0.9770, F1 Score: 0.9722\n",
      "Epoch [19/20], Loss: 0.0396, Accuracy: 0.9930, Precision: 0.9934, F1 Score: 0.9885\n",
      "Epoch [20/20], Loss: 0.0160, Accuracy: 0.9970, Precision: 0.9967, F1 Score: 0.9951\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing Process",
   "id": "cd71439a580ec535"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T06:38:08.414500Z",
     "start_time": "2024-10-14T06:38:08.182142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    all_attended_weights = []  # Store attended weights for visualization\n",
    "    images_to_plot = []  # Store images with label = 1\n",
    "    weights_to_plot = []  # Store attended weights for images with label = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, attended_weights, _ = model(batch_images.float())\n",
    "\n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "\n",
    "            # Check for images with label = 1\n",
    "            for i in range(len(batch_labels)):\n",
    "                if batch_labels[i] == 1:\n",
    "                    images_to_plot.append(batch_images[i].cpu().numpy())\n",
    "                    weights_to_plot.append(attended_weights[i].squeeze().cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_outputs)\n",
    "    precision = precision_score(all_labels, all_outputs)\n",
    "    f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Plotting attended weights for images with label = 1\n",
    "    if images_to_plot:  # Check if there are any images to plot\n",
    "        plot_attended_weights(np.array(images_to_plot), np.array(weights_to_plot))\n",
    "\n",
    "def plot_attended_weights(images, attended_weights):\n",
    "    \"\"\"\n",
    "    Plots the original images and their corresponding attended weights in a single figure.\n",
    "\n",
    "    Args:\n",
    "        images (numpy array): A batch of input image arrays.\n",
    "        attended_weights (numpy array): The attended weights corresponding to the images.\n",
    "    \"\"\"\n",
    "    images = images[0]\n",
    "    attended_weights = attended_weights[0]\n",
    "    num_images = images.shape[0]  # Number of images to plot\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 4, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Display the input image\n",
    "        axes[i].imshow(images[i].transpose(1, 2, 0))  # Change from CHW to HWC format\n",
    "        axes[i].set_title(f'Image {i + 1} - {attended_weights[i]:.4f}')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Plot attention weights as a bar chart\n",
    "        # axes[1, i].bar(range(attended_weights.shape[1]), attended_weights[i], color='orange', alpha=0.7)\n",
    "        # axes[1, i].set_ylim(0, 1)  # Assuming weights are normalized between 0 and 1\n",
    "        # axes[1, i].set_ylabel('Attention Weight')\n",
    "        # axes[1, i].set_xlabel('Feature Index')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Call the test function with your model and test loader\n",
    "test(model, test_loader)"
   ],
   "id": "4f04580fad06751e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 16 vs 8\n",
      "Shape mismatch: 4 vs 8\n",
      "Test Accuracy: 0.9720, Precision: 0.9825, F1 Score: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2800x800 with 7 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References:\n",
    "[1] https://medium.com/@wangdk93/implement-self-attention-and-cross-attention-in-pytorch-1f1a366c9d4b"
   ],
   "id": "77de1738b73d62c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
