{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Layer",
   "id": "4a63a61a889046d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Lib",
   "id": "8ef192d740d3f9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:09.487384Z",
     "start_time": "2024-10-08T09:02:09.484729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.xpu import device\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "id": "743be51811c9768d",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:09.505094Z",
     "start_time": "2024-10-08T09:02:09.492178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set a fixed seed value\n",
    "seed_value = 40\n",
    "\n",
    "# Set the random seed for Python's built-in random module\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# If using CUDA, set the seed for GPU as well (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n"
   ],
   "id": "6b8e1153f8ca7340",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:09.537279Z",
     "start_time": "2024-10-08T09:02:09.535468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "590e5508bccbb47b",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "9d7ae223d9dfa27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:09.595027Z",
     "start_time": "2024-10-08T09:02:09.581205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class DatasetGenerator:\n",
    "    def __init__(self, mnist_data, n_bags=1000, min_instances=3, max_instances=5):\n",
    "        self.mnist_data = mnist_data\n",
    "        self.n_bags = n_bags\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.empty_image = torch.zeros(1, 28, 28)  # Create an empty image tensor (1x28x28)\n",
    "\n",
    "    def create_bags(self):\n",
    "        bags = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(self.n_bags):\n",
    "            # Randomly choose a number of instances for the bag\n",
    "            n_instances = np.random.randint(self.min_instances, self.max_instances + 1)\n",
    "            \n",
    "            # Randomly select instances from the dataset\n",
    "            bag_indices = np.random.choice(len(self.mnist_data), n_instances, replace=False)\n",
    "            bag_images = [self.mnist_data[i][0] for i in bag_indices]\n",
    "            \n",
    "            # Determine the label: 1 if any instance is '9', else 0\n",
    "            label = 1 if any(self.mnist_data[i][1] == 9 for i in bag_indices) else 0\n",
    "            \n",
    "            # Convert images to tensors and pad to ensure exactly 7 instances\n",
    "            bag_images_tensors = [ToTensor()(img) for img in bag_images]\n",
    "            while len(bag_images_tensors) < 7:\n",
    "                bag_images_tensors.append(self.empty_image)  # Pad with empty image\n",
    "            \n",
    "            bags.append(torch.stack(bag_images_tensors))\n",
    "            labels.append(label)\n",
    "\n",
    "        return bags, labels\n",
    "\n",
    "class TrainDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=1000):\n",
    "        super().__init__(mnist_data, n_bags)\n",
    "\n",
    "class TestDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=500):  # Example: fewer bags for testing\n",
    "        super().__init__(mnist_data, n_bags)"
   ],
   "id": "b0c74e22e200f87c",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:10.523678Z",
     "start_time": "2024-10-08T09:02:09.631633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "\n",
    "# Create training dataset generator and generate bags\n",
    "train_generator = TrainDatasetGenerator(mnist_dataset)\n",
    "train_bags, train_labels = train_generator.create_bags()\n",
    "train_loader = DataLoader(list(zip(train_bags, train_labels)), batch_size=32, shuffle=True)\n",
    "\n",
    "# Create test dataset generator and generate bags\n",
    "test_generator = TestDatasetGenerator(mnist_dataset)\n",
    "test_bags, test_labels = test_generator.create_bags()\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(list(zip(test_bags, test_labels)), batch_size=16, shuffle=True)"
   ],
   "id": "a04b09c68297d96f",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attention Layer",
   "id": "222827e790c53375"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:10.534288Z",
     "start_time": "2024-10-08T09:02:10.532290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class SelfAttention(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(SelfAttention, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.query = nn.Linear(input_dim, input_dim)\n",
    "#         self.key = nn.Linear(input_dim, input_dim)\n",
    "#         self.value = nn.Linear(input_dim, input_dim)\n",
    "#         self.softmax = nn.Softmax(dim=2)\n",
    "# \n",
    "#     def forward(self, x):  # x.shape (batch_size, seq_length, input_dim)\n",
    "#         queries = self.query(x)\n",
    "#         keys = self.key(x)\n",
    "#         values = self.value(x)\n",
    "# \n",
    "#         score = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "#         attention = self.softmax(score)\n",
    "#         weighted = torch.bmm(attention, values)\n",
    "#         return weighted, attention\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.ReLU(),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "id": "a0c5dbb8cfefc24c",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MIL-CNN Model",
   "id": "f63c6e77d47a7914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:10.571606Z",
     "start_time": "2024-10-08T09:02:10.568519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # Modify the first convolutional layer to accept grayscale images\n",
    "        # Change in_channels from 3 to 1\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n",
    "        self.attention = AttentionLayer(input_dim=512)  # Assuming output dim from ResNet is 512\n",
    "        self.classifier = nn.Linear(512, 1)  # Binary classification\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "      \n",
    "    def forward(self, bags):\n",
    "        # bags.shape: (batch_size, num_instances, channels, height, width)\n",
    "        batch_size, num_instances = bags.size(0), bags.size(1)\n",
    "        \n",
    "        # Flatten to (batch_size * num_instances, channels, height, width)\n",
    "        bags_flattened = bags.view(-1, *bags.shape[2:])\n",
    "        \n",
    "        # Get features from ResNet\n",
    "        features = self.resnet(bags_flattened)  # Shape: (batch_size * num_instances, 512)\n",
    "\n",
    "        # Reshape back to (batch_size, num_instances, 512)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attended_features, attended_weights = self.attention(features)\n",
    "\n",
    "        # Aggregate features (e.g., mean pooling)\n",
    "        # aggregated_features = attended_features.mean(dim=1)  # Shape: (batch_size, 512)\n",
    "        \n",
    "        # dropped_features = self.dropout(aggregated_features)\n",
    "        dropped_features = self.dropout(attended_features)\n",
    "        # Classify bag\n",
    "        outputs = torch.sigmoid(self.classifier(dropped_features))\n",
    "        \n",
    "        return outputs, attended_weights"
   ],
   "id": "eeb49b1fb73f9119",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Process",
   "id": "1e9c93e0734c3f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:02:12.162686Z",
     "start_time": "2024-10-08T09:02:10.614895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, dataloader, epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, _ = model(batch_images.float())\n",
    "            loss = criterion(outputs.squeeze(), batch_labels.float())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "            \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(all_labels, all_outputs)\n",
    "        precision = precision_score(all_labels, all_outputs)\n",
    "        f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "### Testing Function\n",
    "# def test(model, dataloader):\n",
    "#     model.eval()\n",
    "#     all_labels = []\n",
    "#     all_outputs = []\n",
    "#     \n",
    "#     with torch.no_grad():\n",
    "#         for batch_images, batch_labels in dataloader:\n",
    "#             batch_images = batch_images.to(device)\n",
    "#             batch_labels = batch_labels.to(device)\n",
    "# \n",
    "#             # Forward pass\n",
    "#             outputs, attended_weights = model(batch_images.float())\n",
    "#             \n",
    "#             # Collect outputs and labels for metrics calculation\n",
    "#             all_labels.extend(batch_labels.cpu().numpy())\n",
    "#             all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "# \n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(all_labels, all_outputs)\n",
    "#     precision = precision_score(all_labels, all_outputs)\n",
    "#     f1 = f1_score(all_labels, all_outputs)\n",
    "# \n",
    "#     print(f'Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MILResNet18()\n",
    "model.to(device)\n",
    "\n",
    "train(model, train_loader)"
   ],
   "id": "48899664dd45f891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5038, Accuracy: 0.7720, Precision: 0.6854, F1 Score: 0.5615\n",
      "Epoch [2/5], Loss: 0.2236, Accuracy: 0.9350, Precision: 0.9116, F1 Score: 0.8918\n",
      "Epoch [3/5], Loss: 0.1800, Accuracy: 0.9400, Precision: 0.9186, F1 Score: 0.9003\n",
      "Epoch [4/5], Loss: 0.1356, Accuracy: 0.9550, Precision: 0.9338, F1 Score: 0.9261\n",
      "Epoch [5/5], Loss: 0.1501, Accuracy: 0.9550, Precision: 0.9309, F1 Score: 0.9264\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing Process",
   "id": "cd71439a580ec535"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T09:06:36.372501Z",
     "start_time": "2024-10-08T09:06:36.145625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    all_attended_weights = []  # Store attended weights for visualization\n",
    "    images_to_plot = []  # Store images with label = 1\n",
    "    weights_to_plot = []  # Store attended weights for images with label = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, attended_weights = model(batch_images.float())\n",
    "\n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "\n",
    "            # Check for images with label = 1\n",
    "            for i in range(len(batch_labels)):\n",
    "                if batch_labels[i] == 1:\n",
    "                    images_to_plot.append(batch_images[i].cpu().numpy())\n",
    "                    weights_to_plot.append(attended_weights[i].squeeze().cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_outputs)\n",
    "    precision = precision_score(all_labels, all_outputs)\n",
    "    f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Plotting attended weights for images with label = 1\n",
    "    if images_to_plot:  # Check if there are any images to plot\n",
    "        plot_attended_weights(np.array(images_to_plot), np.array(weights_to_plot))\n",
    "\n",
    "def plot_attended_weights(images, attended_weights):\n",
    "    \"\"\"\n",
    "    Plots the original images and their corresponding attended weights in a single figure.\n",
    "\n",
    "    Args:\n",
    "        images (numpy array): A batch of input image arrays.\n",
    "        attended_weights (numpy array): The attended weights corresponding to the images.\n",
    "    \"\"\"\n",
    "    images = images[0]\n",
    "    attended_weights = attended_weights[0]\n",
    "    num_images = images.shape[0]  # Number of images to plot\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 4, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Display the input image\n",
    "        axes[i].imshow(images[i].transpose(1, 2, 0))  # Change from CHW to HWC format\n",
    "        axes[i].set_title(f'Image {i + 1} - {attended_weights[i]:.4f}')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Plot attention weights as a bar chart\n",
    "        # axes[1, i].bar(range(attended_weights.shape[1]), attended_weights[i], color='orange', alpha=0.7)\n",
    "        # axes[1, i].set_ylim(0, 1)  # Assuming weights are normalized between 0 and 1\n",
    "        # axes[1, i].set_ylabel('Attention Weight')\n",
    "        # axes[1, i].set_xlabel('Feature Index')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# Call the test function with your model and test loader\n",
    "test(model, test_loader)"
   ],
   "id": "4f04580fad06751e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9440, Precision: 0.9314, F1 Score: 0.9209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2800x800 with 7 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACuUAAAGnCAYAAACqgchBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA070lEQVR4nO3deZjWdbk/8HtgBoZVNllcQAXEFMkFUdQA94WTC+6XHZdztFI8WifNNM3qlLac8qiF5klFETMQkVQ0N+SUgqKgibmEinbMFcQAkWX4/v7oMD9Gtnk+nxkGZl6v65rriofv+7k/8+T13Dz6ni9lRVEUAQAAAAAAAAAAAAAka9bQBwAAAAAAAAAAAACAzZ1SLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi7VRo8eHWVlZfHMM8809FHq1W233RYnn3xy9OvXL5o1axbbbbddvc6bOXNmHHzwwdG2bdvo0KFDjBgxIl5//fVa5x955JEYPHhwtG7dOrp06RJnnHFGvP/++2tct3z58vje974X2223XbRs2TJ22mmnuO6669b6nK+//nqMGDEiOnToEG3bto1DDjkkZs6cudZr77zzzthtt92isrIyttpqq/ja174WixYtqvX5gcarKeyNd955Jy677LIYPHhwdOnSJdq3bx977rln3HjjjVFVVVXn8/74xz/GWWedFXvuuWe0bNkyysrKYu7cuSU9h70BbKqawt6IiDjrrLOif//+0aFDh2jVqlXsuOOOcdFFF8WHH35YL/N83gAaq6ayN1b33nvvRefOnaOsrCzuuuuueplhbwCNUVPZGdttt12UlZWt8fXVr361XubZGUBj1VT2RkTEhx9+GBdccEH1e2y3bt3iiCOOiPnz59f5LHsDaKyawt54/PHH1/pZoz4/c9gbNHZKuTQ5Y8aMiRdffDEGDRoUvXv3rtdZL7/8cgwbNiyWLVsW48aNi5tvvjleffXV+MIXvhAffPDBBvNTp06NI444Irp16xaTJk2Ka665Jh555JE46KCDYunSpTWuPffcc+Oqq66KkSNHxu9///s49thj44ILLogrr7yyxnUffPBBfOELX4hXX301br755hg3blx8+umnMWzYsHjllVdqXDt27Ng45ZRTYq+99ooHHnggrrjiihg9enSMGDEi/8UB2Aw8++yzcdttt8VBBx0Ut912W0yYMCGGDh0a55xzTpx99tl1Pu/RRx+NRx55JHr27Bn77rtvyXl7A6DhLV68OL785S/HHXfcEffff3+cddZZceONN8bQoUNj2bJldTrL5w2AxmXkyJFRWVlZb89vbwBs/vbbb7+YNm1aja+LL764zufYGQCbv7/97W+x9957x4MPPhiXX355PPzww3H99ddHnz59/DsqewOghj322GONzxnTpk2L0047LSIijj322DqdZ2/QJBTwf2655ZYiIooZM2Y09FHqVVVVVfX/Hj58eNGrV696m3XCCScUXbp0KT7++OPqx+bOnVtUVFQU3/zmNzeY32uvvYqdd965WL58efVjTzzxRBERxahRo6ofmz17dlFWVlZceeWVNfJnn3120apVq2LevHnVj1100UVFRUVFMXfu3OrHPv7446JLly7FiSeeWP3YihUrih49ehSHHnpojeccO3ZsERHF5MmTa/EKAI1ZU9gb8+fPL5YtW7bG4yNHjiwionjrrbfqdN7qO+qnP/1pERHFG2+8Ueu8vQFsyprC3liXUaNGFRFRPProo3X6vD5vAI1ZU9sbd911V9G2bdvi1ltvLSKiGD9+fJ3PsDeAxqqp7IxevXoVw4cP3yiz7AygMWsqe+Poo48utt5662L+/Pn1PsveABqzprI3PmvlypXFDjvsUPTq1avGf8OuC/YGTYE75bJeZ5xxRrRt2zZefvnlOOyww6JNmzbRo0eP+NGPfhQREdOnT4/9998/2rRpEzvuuGPceuutNfIffPBBnHvuubHzzjtH27Zto2vXrnHggQfGH/7whzVm/e///m8cf/zx0a5du+jQoUOceuqpMWPGjCgrK4vRo0fXuPaZZ56Jo446Kjp16hSVlZWx++67x7hx42r1PTVrtnH+sV+xYkXcd999cdxxx0X79u2rH+/Vq1cccMABMXHixPXm33777ZgxY0b88z//c5SXl1c/vu+++8aOO+5YI3/PPfdEURRx5pln1niOM888M5YsWRIPPvhg9WMTJ06MAw88MHr16lX9WPv27WPEiBFx7733xooVKyLiH//fvvPOO2s85wknnBBt27bd4PmBpqmx7Y2OHTtGRUXFGo8PGjSo+gx1KWdH2RvA5qix7Y112XLLLSMiarw/5/J5A2iKGuvemD9/fowcOTJ++MMfRs+ePUt/YWrB3gCamsa6MzYGOwNoihrb3pg7d2787ne/i7PPPjs6duyY/sLUgr0BNEWNbW+szZQpU+L111+PM888s057VvYGTYVSLhu0fPnyGDFiRAwfPjwmTZoURxxxRFxyySVx6aWXxumnnx7/8i//EhMnTox+/frFGWecEc8++2x1dv78+RERccUVV8T9998ft9xyS+ywww4xbNiwePzxx6uvW7x4cRxwwAExZcqU+PGPfxzjxo2Lbt26xUknnbTGeaZMmRL77bdfLFiwIG644YaYNGlS7LbbbnHSSSetsXAa0muvvRZLliyJAQMGrPF7AwYMiDlz5sSnn366zvzs2bOrr11bftXvr7p2yy23jO7du69x3erPtWTJknjttdfW+ZxLliyJ119/fb3zKyoqYqeddqoxH2B1TWFvPPbYY1FeXh477rhjUr4+2BvA5qqx7o0VK1bE4sWL44knnojLL7889t9//9hvv/2SX6fP8nkDaKoa4944//zzY/vtt4/zzjsv67VZH3sDaIoa4874n//5n2jXrl1UVFTEzjvvHD/72c+iqqoq63X6LDsDaKoa0974wx/+EEVRxFZbbRWnnHJKtG3bNiorK2PYsGExbdq0Onm9VrE3gKaqMe2NtbnpppuiWbNma5RPc9kbNBV1d5seGq1ly5bFD37wgxgxYkRERAwbNizuu+++uOqqq2LmzJmx++67R0TEwIEDo2vXrnHHHXfEnnvuGRER/fr1i1GjRlU/V1VVVRx22GExd+7cuPbaa2PYsGEREXHrrbfGnDlz4oEHHojDDz88IiIOPfTQ+OSTT+JXv/pVjfOce+65scsuu1QXoiIiDjvssPjwww/j0ksvjdNOO22j3Q13febNmxcREZ06dVrj9zp16hRFUcRHH30UPXr0SMqv+v1V167tujZt2kSLFi2qr/3oo4+iKIp1Pufqczc0f+7cuWs9N0Bj3xsPPfRQjBkzJi644ILo3Llz2otUD+wNYHPVGPfG9OnTY/DgwdW/PvLII+POO++M5s2bZ7xSNfm8ATRVjW1v3H///TFu3LiYOXNmvf77LHsDaIoa284YPnx4DBw4MHr37h0fffRRjB8/Pi688MJ47rnnYsyYMXXzooWdATRdjWlvvP322xERceGFF8YBBxwQEyZMiMWLF8f3vve9OPDAA+Opp55aa/Eohb0BNFWNaW981oIFC+Luu++OQw45pM7/Vid7g6ai4ZuLbPLKysriyCOPrP51eXl59OnTJ3r06FG9RCL+8ebStWvXePPNN2vkb7jhhthjjz2isrIyysvLo6KiIh599NF46aWXqq+ZOnVqtGvXrnqJrHLKKafU+PWcOXPi5ZdfjlNPPTUi/nH3qVVfRx55ZLzzzjvxyiuv1Nn3/llVVVU1Zq5cuXKDmbKysqTf29A1n328lDl1cW1tzg40TY15b8ycOTNOPPHE2GeffeKqq67a4PUpeyOXvQFsbhrj3th1111jxowZMXXq1Ljmmmti1qxZccghh8Qnn3yy3pzPG7V7DqBpa0x74+OPP46vfOUrcfHFF0f//v1Lfi3sjdo9B9B0NaadERHxy1/+Ms4888wYMmRIHH300XH77bfHeeedF7fffnvMmjVrvVk7o3bPATRtjWlvrHqf32abbWLChAlx2GGHxYgRI+LBBx+MZs2axU9+8pP1vhb2Ru2eA2jaGtPe+KyxY8fGp59+GmeddVatrrc3avccNC1KuWxQ69ato7KyssZjLVq0WGvrv0WLFjVuI/7zn/88zjnnnNh7771jwoQJMX369JgxY0YcfvjhsWTJkurr5s2bF926dVvj+T772HvvvRcR//ipvoqKihpf5557bkREfPjhh+nf7Ab07t27xszvf//767x21d0TV/8pjFXmz58fZWVl0aFDh+T86q9/586d13rd4sWLY9myZdXXduzYMcrKytb5nBH//6c5SpkPsLrGujdWFar69u0bkydPjpYtW24wU8reyGVvAJurxrg32rRpEwMHDowhQ4bE+eefHxMnToynnnpqjZ9c/yyfN9Y+H2B1jWlvfPvb346Kioo477zzYsGCBbFgwYJYtGhRRER88sknsWDBgiiKYp15e2Pt8wFWaUw7Y12+9KUvRcQ//raO9bEz1j4fYHWNaW+sei88+OCDa/zNTT169IjPf/7zMXPmzHVmI+yNdc0HWF1j2hufddNNN8WWW24ZRx99dK2utzfWPp+mrbyhD0Djdvvtt8ewYcPi+uuvr/H4woULa/y6c+fO8fTTT6+Rf/fdd2v8ukuXLhERcckll1TfAv6z+vXrl3Pk9br33ntj6dKl1b/eaqut1nlt7969o1WrVvHCCy+s8XsvvPBC9OnTZ40FvbpVd0h54YUXavx0zarHVr+Dyq677hp33nlnvPvuu9G9e/ca163+XK1atYo+ffqs80ytWrWKHXbYofo5Vz2+8847V1+3YsWKePnll9f4yRuAurCp7o1Zs2bFwQcfHL169YqHHnootthiiw1mIkrbG7nsDaAp2lT3xmcNHDgwmjVrFq+++up6r/N5w94A6temtjdmz54dc+fOrfHeusrpp58eEf/46/PW9R8i7A17A6g/m9rOWJdVP7yxob+G1s6wM4D6tantjQEDBqzz94qisDfsDaCBbWp7Y3WzZs2KWbNmxTe+8Y2oqKioVcbesDdYkzvlUq/KysrWuJPgn/70p5g2bVqNx4YOHRoLFy6MBx54oMbjd955Z41f9+vXL/r27RvPP/98DBw4cK1f7dq1q59vJv7x5rr6rPUtkvLy8vjiF78Yd999d43F+dZbb8WUKVPWuQhX2XrrrWPQoEFx++23R1VVVfXj06dPj1deeaVG/uijj46ysrK49dZbazzH6NGjo1WrVjVuZX/sscfGY489Fn/961+rH1u4cGHcfffdcdRRR0V5+T+6+nvvvXf06NEjRo8eXeM577rrrli0aNEGzw+QYlPcG88991wcfPDBsc0228TDDz8cHTt2rPX3U8reyGVvAE3Rprg31mbq1KmxcuXK6NOnz3qv83nD3gDq16a2N/7rv/4rpkyZUuPr6quvjoiI7373uzFlypRo27btOvP2hr0B1J9NbWesy2233RYREfvss896r7Mz7Aygfm1qe2PvvfeObbbZJh566KEa78V/+9vf4vnnn7c37A2ggW1qe2N1N910U0RE/Ou//mutvx97w95gLQr4P7fccksREcWMGTOqHzv99NOLNm3arHHt0KFDi1122WWNx3v16lUMHz68+tff+c53irKysuI73/lO8eijjxajRo0qunfvXvTu3bvo1atX9XWLFi0q+vTpU3Tq1KkYNWpU8dBDDxVf//rXi+22266IiOLWW2+tvvaxxx4rWrZsWRx66KHFHXfcUUydOrWYOHFiceWVVxbHH3/8Br/PF198sRg/fnwxfvz4Ys899yy23HLL6l+/+OKLtX25auWll14q2rZtWwwZMqSYPHlycffddxf9+/cvttpqq+L999+vcW3z5s2LAw88sMZjU6ZMKcrLy4tjjz22ePjhh4uxY8cW2267bdG/f//i008/rXHtWWedVbRs2bL46U9/Wjz++OPFpZdeWpSVlRU//OEPa1z3/vvvFz169Ch23XXXYuLEicXkyZOLIUOGFO3atSteeumlGteOGTOmiIjiy1/+cjFlypTixhtvLDp06FAccsghdfgqAZurprA3Xn755aJz585Fp06dinvvvbeYNm1aja/Pvpfnev/996t30mmnnVZERDFq1Khi/PjxxeOPP17jWnsD2Nw0hb1x7733FkcddVTx61//unj44YeLyZMnF9///veLTp06FX369CkWLFhQyku2QT5vAI1ZU9gbazNlypQiIorx48eXnN0QewNorJrCzhg7dmxx3HHHFTfffHPx6KOPFhMmTChOPvnkIiKKM844o5SXq1bsDKAxawp7oyiKYvz48UVZWVkxfPjw4r777it++9vfFv379y+22GKLYs6cObV9uWrF3gAas6ayN4qiKJYsWVJ07Nix2HfffWt1fSp7g6ZAKZdq9bFIli5dWlx44YXF1ltvXVRWVhZ77LFHcc899xSnn356jUVSFEXx1ltvFSNGjCjatm1btGvXrjjuuOOKyZMnFxFRTJo0qca1zz//fHHiiScWXbt2LSoqKoru3bsXBx54YHHDDTds8Pu84ooriohY69cVV1yxwXypnnnmmeKggw4qWrduXbRv37445phj1vpBJyKKoUOHrvH4Qw89VOyzzz5FZWVl0alTp+K0004r3nvvvTWuW7ZsWXHFFVcUPXv2LFq0aFHsuOOOxbXXXrvWM82ZM6c45phjivbt2xetW7cuDjrooOLZZ59d67V33HFHMWDAgKJFixZF9+7di/PPP79YuHBhaS8C0Cg1hb2x6ntc19ctt9xSi1eq9lb9B/i1fX12R9gbwOamKeyNl156qTj++OOLXr16FZWVlUVlZWWx0047FRdddFExb9682rxMJfN5A2ismsLeWJv6LOUWhb0BNE5NYWdMmzatOOigg4ru3bsXFRUVRevWrYu99tqrGDVqVFFVVVWbl6lkdgbQWDWFvbHKPffcU+y1115FZWVlscUWWxRHHXVUnd+kahV7A2ismtLeGDt2bBERxc0331yr63PYGzR2ZUVRFLl324X6cuWVV8Zll10Wb731VmyzzTYNfRwANnH2BgClsDcAKIW9AUBt2RkAlMLeAKAU9gZs+sob+gCwyi9+8YuIiNhpp51i+fLl8dhjj8W1114bX/rSlywRANZgbwBQCnsDgFLYGwDUlp0BQCnsDQBKYW/A5kkpl01G69at4+qrr465c+fG0qVLo2fPnnHxxRfHZZdd1tBHA2ATZG8AUAp7A4BS2BsA1JadAUAp7A0ASmFvwOaprCiKoqEPAQAAAAAAAAAAAACbs2YNfQAAAAAAAAAAAAAA2Nwp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmcpre+EhzU6oz3MAsAl6eOX45Ky9AdD02BsAlMLeAKAUqXvDzgBoenzWAKAU9gYApajN3nCnXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABApvKGPgBQN8oqWpScef17eybNmnna1Um5X3+8U8mZh7+4W9KsFa/PTcoBAAAAAAAAAABACnfKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyFTe0AcAamreZ/uk3I8e+U3JmQEtnk6aFVGZlPpax7klZyb1PThpVovXS58FAAAAAAAAAAAAqdwpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZyhv6ANBYzTtrcFLu6xeNS8oNaFGZlEtx1F8OT8rNnt2r5Ey/J/+cNGtlUgoAgE3WoF1Ljnxw2bKkUTMH/jYptzkY/PxxacExW5YcaX/H9LRZAAAAAAAAsJlyp1wAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBM5Q19ANjUNe/WNSl30MhpSblT281Lyt2zuG3JmX9/8NSkWTt+/dmkXN8V75acWZk0CaBufHDO4JIzE77106RZPctbJ+Wal6X9jFVVUfo7bP//Pi9pVs/vPpmUAzae8u7dSs68/uXeSbP+6Zi0Pyf/W5frS85s3TztvbWqSIptFv44YHxS7q2rPik5c2TfbybN6vk9ewMAAAAAAIDNkzvlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZCpv6APAxrTk6EElZ/pf9qekWT/u9lxSbvTfuyblfnvywSVn+j73VNKsIikF0HA+/afS3/8jIiZ866clZ3qWt06aleqid3dPyv2o27MlZ/509nVJsw6Z8dWkXMv7ZyTloCkrBn8+KXf9b39Zcmbr5hv3/e6qeQNLzvx5YY+kWU+/2Ssp1+6PG/c1SfGf3/hVUm5YZenf22/OuDpp1rd/fWxSbsXbf0vKAQAAAAAAQF1xp1wAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZCpv6ANAimatW6flRr5fcmbU1tOTZt329y5Jud+efHBSbuVzf07KATQFI3/+26Rcz/LS903fu89JmtXvW7OTcsXSpUm5oceUfs6p11yfNKsoK0vKAaVr/smypNyTS7YtObNDi9L/bB0R8a2zv5qUq3z6LyVnqv7+UdKs3pGWS5L4HlkMHpCUa9fs06RcRPOSE2+t6Jg0qVi4KCkHAAAAAAAADc2dcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADKVN/QBIEXFA+2Tcg/0vaduD7Iev7r8uKRc2+eequOTADQef/32vkm5o9vMSMr1vfu80jMXPJM0a+XKqqRcqhZ/L33eRyuXJM1qOX9pUg4o3crnX0rK3dKvV0IqJRNREc8m5Tbuu+TG88mxg5Jyj193feLE5om50n37xjOSclv9/cm6PQgAAAAAAABsJO6UCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkKm8oQ9A07b0yL2Scr/rMyopd+PHvUrO/NftxyTN2nbCU0k5ANbtP06/PSn35oplSbmdrnm/5EzVyqqkWamKwZ9Pyn1z1JiSM/v+8dykWds/+XxSDqChNO/YMSk397zPlZx5+is/T5oV0SIpdc/iDkm56752UsmZrR96OmlWkZQCAAAAAACAhudOuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIVN7QB6DxKGvZsvTM199PmtWyrCIpd/2rQ0rObPvDJ5NmAbB+yw7fq+TMP7WZkTRrz+suSsptPWfT3wGvndQqKXdIqyUlZzpObp00C2B1zTtskZR74797lpz5XLd3k2bt3+m1pNwFHR9NSLVImpXqO6O/lJTbZnLpO7FImgQAAAAAAACbL3fKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyFTe0Aeg8fjLj3YvOfPaLjckzfrd4tZJuS1/UpmUA6DuvTuoouRMeTRPmtVz0gdJuaqk1MbV8cWytOAJpUdGfnt80qixY7ZJygGNVJdOSbGH976+5EyP5mmfGxqzP438RVLugKHHlZxpeWWHpFnNps5KygEAAAAAAEBDc6dcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATOUNfQA2Pc1at07Kdf/c+3V8knX7xWknJOXKpj1XtwcBINnSHZY29BE2Kc3atEnKffG8qXV8knV7b8UWG20W0HhVzXkjKfcvJ40sOfPmkWmfbZp/bmFSLkWzZkVS7vm9x9TxSdZvSv8JJWcWjU3b9btP+lpSru/Ip5JyAAAAAAAAUFfcKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmcob+gBsepbvvVNS7okBN5eceW35oqRZzWa+nJQrklIbV1lFi6Rcs+23reOTrFvVnLlpwZVVdXoOYPPW+qWWpYcOSZu1950vJuXu+dWwkjML+q9ImnXNIbcn5Ya3TtulKSZ99+CkXJt4qo5PAjRFZdOeLzmz3bR6OEhdKytLig0v3ycp997ZA5Ny25/8l5Iz43v/PmnWK8eMSsrt9/R5JWc63ro5/EMCAAAAAADA5sKdcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADKVN/QBqEfNmifFqi6dV8cHWbcjfnNRUm77pdPq+CTr1qyyMin35kV7JOWKAQuTci/tNyYpl2K3q85NynW77sk6PgmwOet5/eySM2PO7J4067Iupc+KiLjs22m5zcHg504qOdP5939OmrUyKQXQRBRFWmz5sqRc11Fpfyb/dFznkjOPP1WRNGtY5fKk3BfOf6rkzOxbk0YBAAAAAADAWrlTLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQKbyhj4A9WfZoXsk5abs8t9JuTeWLyo5s/23piXNSlXst1vJmUvGjE6aNaRyelJuc7Bo8CdJuW7X1fFBgM1a1d//XnLmjjOPSJr1/a+k/ZGny5aln7H8ts5Js35+1S+Tcnu1LEvKtftZ+5IzKxf+JWkWAJu/qg/nlZz5yj1nJ8165eRRSbk92r5ZcmZ2bJs0CwAAAAAAANbGnXIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAylTf0AWg8jrzpmyVnesaTSbNW7r9bUu4/brup5Mw+lc2TZj23dGlS7rTnz0jK/WnQb5JyKdo83XqjzQJYXdm055NyfafV8UHWo3zbJUm5ZZG2bxYVafOaL61KygFArRUbd9yv5g4pOdMq3qiHkwAAAAAAANBUuVMuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADKVN/QBaDyaL9t4s/56flVSbt7KNiVntr/ntKRZn7v6w6TcNmWJL+TjpUduWLB10qitbnohKbcyKQWweXnlx12Scvu1THuXHPxc2p7q+MRzSTkAqK0v7P/iRp337qzuJWe2jzfq4SQAAAAAAAA0Ve6UCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkKm8oQ9A47F4uxUbbdYPd5uUlLvgnjNKzux44fSkWcv33y0p9/bX0l7HqmJlyZlf3nx00qytFj6ZlANoCr65+0NJuSXFsqRcx/9olZQDaCjlO2yXlHvl3O4lZ9q8nfZzqN2vbsR/3m3WPCn2xpWDSs5M3OaapFn//fH2Sbm+1/9vyZmN9ykWAAAAAACApsCdcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADKVN/QBqD/li1ck5d5Yvigp9+I//aLkzMDLv5Y066qfDE7K9XvgzZIzK/YZkDRrl2tmJ+UmdXsyKdd34r+VnvnPtFkATcXHp+5TcuaLbf4zadZ7VUmxePubacFeI7uXnFnxzrtJs4DGqXz7Xkm5IZNeTMpd0Or+kjM/P/mkpFlFUmrz8MYPBiXlXvrnXyak0v6Vw3Vjjk7KbfOmzzcAAAAAAAA0LHfKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAECm8oY+APWn2R9mJeWOmvnlpNwLe99RcubP54xKmpXqsvN2LTnzg66Tk2YtLZYn5T53z78l5fqe91RSDoB1e29IVcmZrs1bJ816Ymnaz0r1On9BUm7FO+8m5YDGqXmHLUrODLt3dtKsf+/4l6TcvpeMLDnT4ZlpSbM2qrKypNg7/z44KTfulKuTchEVJScOf/nopEk9r56ZlFuZlAIAAAAAAIC64065AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmcob+gBserb96odJuW9M3qPkzM96zEyaleoHXV/YaLM+d/d5Sbm+//ZUHZ8EgFQ/GHb3Rpt15qSvJuX6LXm1jk8CNEVl7dqVnPn3jn9JmjV9aVIsujz8RsmZFWmjotmAnZJyc4/pVHJmnyPTPqNM3vYXSbmIiqTUoJknl5zpeknazwGv/PSvSTkAAAAAAABoaO6UCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkKm8oQ/ApqfqvfeTcrP3LD1zWOyWNGtz0DeeaugjAPB/yvbcJSk3tNUfS86siJZJs/r8ZnFSrmre/KQcQEMZ1LJIyt3y9ISSM1VF2qzWzaYl5dqWpe2AFO9UfZKUO3DsRUm5HS6fUXJm5YoVSbMAAAAAAABgc+VOuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIVN7QBwAAqG+vnNsqKdejeeuSM/85v1/SrHj6hbQcQB0oPvmk5Mxxc45ImjWhzwNJuc7N0t7LN6ad/3hGyZmVVWk/K7vDtSuTcttPn5aUK5JSAAAAAAAA0LS4Uy4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAECm8oY+AABAfTt01xeTci8uX1Zy5vdfH5I0qyKeTcoB1IWqefNLziwZmjbryNgjLbgZ2C7+1NBHAAAAAAAAABqQO+UCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkKm/oAwAA1Le5g5Yk5S6KfUrOVMSzSbMAAAAAAAAAANi8uVMuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATEq5AAAAAAAAAAAAAJBJKRcAAAAAAAAAAAAAMinlAgAAAAAAAAAAAEAmpVwAAAAAAAAAAAAAyKSUCwAAAAAAAAAAAACZlHIBAAAAAAAAAAAAIJNSLgAAAAAAAAAAAABkUsoFAAAAAAAAAAAAgExKuQAAAAAAAAAAAACQSSkXAAAAAAAAAAAAADIp5QIAAAAAAAAAAABAJqVcAAAAAAAAAAAAAMiklAsAAAAAAAAAAAAAmZRyAQAAAAAAAAAAACCTUi4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGRSygUAAAAAAAAAAACATGVFURQNfQgAAAAAAAAAAAAA2Jy5Uy4AAAAAAAAAAAAAZFLKBQAAAAAAAAAAAIBMSrkAAAAAAAAAAAAAkEkpFwAAAAAAAAAAAAAyKeUCAAAAAAAAAAAAQCalXAAAAAAAAAAAAADIpJQLAAAAAAAAAAAAAJmUcgEAAAAAAAAAAAAgk1IuAAAAAAAAAAAAAGT6f9deqz2Br+7NAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References:\n",
    "[1] https://medium.com/@wangdk93/implement-self-attention-and-cross-attention-in-pytorch-1f1a366c9d4b"
   ],
   "id": "77de1738b73d62c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
