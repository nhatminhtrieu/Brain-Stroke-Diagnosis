{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Layer",
   "id": "4a63a61a889046d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Lib",
   "id": "8ef192d740d3f9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:46.897762Z",
     "start_time": "2024-11-15T03:23:46.895479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from click.core import batch\n",
    "from torch.xpu import device\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "id": "743be51811c9768d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:46.918276Z",
     "start_time": "2024-11-15T03:23:46.902939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set a fixed seed value\n",
    "seed_value = 42\n",
    "# Set the random seed for Python's built-in random module\n",
    "random.seed(seed_value)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed_value)\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# If using CUDA, set the seed for GPU as well (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ],
   "id": "6b8e1153f8ca7340",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:46.948715Z",
     "start_time": "2024-11-15T03:23:46.946636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "590e5508bccbb47b",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "9d7ae223d9dfa27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:46.997242Z",
     "start_time": "2024-11-15T03:23:46.992239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self, mnist_data, n_bags=1000, min_instances=3, max_instances=5):\n",
    "        self.mnist_data = mnist_data\n",
    "        self.n_bags = n_bags\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.empty_image = torch.zeros(1, 28, 28)  # Create an empty image tensor (1x28x28)\n",
    "\n",
    "    def create_bags(self):\n",
    "        bags = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(self.n_bags):\n",
    "            # Randomly choose a number of instances for the bag\n",
    "            n_instances = np.random.randint(self.min_instances, self.max_instances + 1)\n",
    "            \n",
    "            # Randomly select instances from the dataset\n",
    "            bag_indices = np.random.choice(len(self.mnist_data), n_instances, replace=False)\n",
    "            bag_images = [self.mnist_data[i][0] for i in bag_indices]\n",
    "            \n",
    "            # Determine the label: 1 if any instance is '9', else 0\n",
    "            label = 1 if any(self.mnist_data[i][1] == 9 for i in bag_indices) else 0\n",
    "            \n",
    "            # Convert images to tensors and pad to ensure exactly 7 instances\n",
    "            bag_images_tensors = [ToTensor()(img) for img in bag_images]\n",
    "            while len(bag_images_tensors) < 7:\n",
    "                bag_images_tensors.append(self.empty_image)  # Pad with empty image\n",
    "            \n",
    "            bags.append(torch.stack(bag_images_tensors))\n",
    "            labels.append(label)\n",
    "\n",
    "        return bags, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_bags\n",
    "\n",
    "class TrainDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=1000):\n",
    "        super().__init__(mnist_data, n_bags)\n",
    "\n",
    "class TestDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=500):  # Example: fewer bags for testing\n",
    "        super().__init__(mnist_data, n_bags)"
   ],
   "id": "b0c74e22e200f87c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:47.962788Z",
     "start_time": "2024-11-15T03:23:47.042390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "\n",
    "# Create training dataset generator and generate bags\n",
    "train_generator = TrainDatasetGenerator(mnist_dataset)\n",
    "train_bags, train_labels = train_generator.create_bags()\n",
    "train_loader = DataLoader(list(zip(train_bags, train_labels)), batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# Create test dataset generator and generate bags\n",
    "test_generator = TestDatasetGenerator(mnist_dataset)\n",
    "test_bags, test_labels = test_generator.create_bags()\n",
    "test_loader = DataLoader(list(zip(test_bags, test_labels)), batch_size=16, shuffle=True, drop_last=True)"
   ],
   "id": "a04b09c68297d96f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attention Layer",
   "id": "222827e790c53375"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Self-Attention",
   "id": "a5f1288b6239bb36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:47.973585Z",
     "start_time": "2024-11-15T03:23:47.971144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Linear layers for query, key, and value transformations\n",
    "        self.query = nn.Linear(input_dim, input_dim)  # Shape: (input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)    # Shape: (input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)  # Shape: (input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)              # Softmax along the last dimension\n",
    "\n",
    "    def forward(self, x):  # x.shape: (batch_size, seq_length, input_dim) # seq_length = num_instances\n",
    "        # Transform the input into queries, keys, and values\n",
    "        queries = self.query(x)  # Shape: (batch_size, seq_length, input_dim)\n",
    "        keys = self.key(x)       # Shape: (batch_size, seq_length, input_dim)\n",
    "        values = self.value(x)   # Shape: (batch_size, seq_length, input_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        score = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        # Shape of score: (batch_size, seq_length, seq_length)\n",
    "\n",
    "        attention = self.softmax(score)  \n",
    "        # Shape of attention: (batch_size, seq_length, seq_length)\n",
    "\n",
    "        weighted = torch.bmm(attention, values)  \n",
    "        # Shape of weighted: (batch_size, seq_length, input_dim)\n",
    "\n",
    "        return weighted, attention  # Returns weighted output and attention scores"
   ],
   "id": "5fd7e2264f02a74b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention Layer",
   "id": "478d1bf7862fe438"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:48.021245Z",
     "start_time": "2024-11-15T03:23:48.018476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        # Sequential model for attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # Shape: (input_dim, hidden_dim)\n",
    "            nn.Tanh(),                          # Activation function\n",
    "            nn.Linear(hidden_dim, 1)           # Shape: (hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        \n",
    "        attention_weights = self.attention(x)  \n",
    "        # Shape of attention_weights: (batch_size, num_instances, 1)\n",
    "\n",
    "        weights = F.softmax(attention_weights, dim=1)  \n",
    "        # Shape of weights: (batch_size, num_instances, 1)\n",
    "\n",
    "        # Element-wise multiplication followed by summation over num_instances\n",
    "        weighted_sum = (x * weights).sum(dim=1)  \n",
    "        # Shape of weighted_sum: (batch_size, feature_dim)\n",
    "\n",
    "        return weighted_sum, weights.squeeze(-1)  \n",
    "        # Returns weighted sum and attention weights with shape (batch_size, num_instances)"
   ],
   "id": "a0c5dbb8cfefc24c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gaussian Process Layer",
   "id": "184e116b03eebef0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:48.066432Z",
     "start_time": "2024-11-15T03:23:48.063385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PGLikelihood(gpytorch.likelihoods._OneDimensionalLikelihood):\n",
    "    # this method effectively computes the expected log likelihood\n",
    "    # contribution to Eqn (10) in Reference [1].\n",
    "    def expected_log_prob(self, target, input, *args, **kwargs):\n",
    "        mean, variance = input.mean, input.variance\n",
    "        # Compute the expectation E[f_i^2]\n",
    "        raw_second_moment = variance + mean.pow(2)\n",
    "\n",
    "        # Translate targets to be -1, 1\n",
    "        target = target.to(mean.dtype).mul(2.).sub(1.)\n",
    "\n",
    "        # We detach the following variable since we do not want\n",
    "        # to differentiate through the closed-form PG update.\n",
    "        c = raw_second_moment.detach().sqrt()\n",
    "        # Compute mean of PG auxiliary variable omega: 0.5 * Expectation[omega]\n",
    "        # See Eqn (11) and Appendix A2 and A3 in Reference [1] for details.\n",
    "        half_omega = 0.25 * torch.tanh(0.5 * c) / c\n",
    "\n",
    "        # Expected log likelihood\n",
    "        res = 0.5 * target * mean - half_omega * raw_second_moment\n",
    "        # Sum over data points in mini-batch\n",
    "        res = res.sum(dim=-1)\n",
    "\n",
    "        return res\n",
    "\n",
    "    # define the likelihood\n",
    "    def forward(self, function_samples):\n",
    "        return torch.distributions.Bernoulli(logits=function_samples)\n",
    "\n",
    "    # define the marginal likelihood using Gauss Hermite quadrature\n",
    "    def marginal(self, function_dist):\n",
    "        prob_lambda = lambda function_samples: self.forward(function_samples).probs\n",
    "        probs = self.quadrature(prob_lambda, function_dist)\n",
    "        return torch.distributions.Bernoulli(probs=probs)"
   ],
   "id": "70532c42caedd4b5",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:48.114838Z",
     "start_time": "2024-11-15T03:23:48.110201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "id": "dbf94e502c230128",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MIL Model",
   "id": "60671b87cb13ea1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:48.170031Z",
     "start_time": "2024-11-15T03:23:48.163776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self, attention_method='self-attention'):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.attention_method = attention_method\n",
    "        \n",
    "        self.classifier = nn.Linear(512 + 1, 1)\n",
    "        if attention_method != 'self-attention':\n",
    "            self.attention = AttentionLayer(input_dim=512)\n",
    "        else:\n",
    "            self.attention = SelfAttention(input_dim=512)\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        # Define inducing points for the GP layer\n",
    "        inducing_points = torch.randn(32, 512)\n",
    "        # inducing_points = torch.full((512, 512), 1e-20)  \n",
    "        \n",
    "        # Define inducing points using linspace\n",
    "        # start_value = -1.0  # Example start value\n",
    "        # end_value = 1.0     # Example end value\n",
    "        # steps = 32          # Number of inducing points (should match the first dimension)\n",
    "        self.gp_layer = GPModel(inducing_points)    \n",
    "\n",
    "    def forward(self, bags):\n",
    "        # Input shape: torch.Size([32, 7, 1, 28, 28]) # batch_size, num_instances, channels, height, width\n",
    "        batch_size, num_instances = bags.size(0), bags.size(1)\n",
    "        bags_flattened = bags.view(-1, *bags.shape[2:])\n",
    "\n",
    "        features = self.resnet(bags_flattened)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "        \n",
    "        features_for_gp = features.view(batch_size * num_instances, -1)\n",
    "        # print(f'Shape of Features for GP: {features_for_gp.shape}')\n",
    "        attended_features, attended_weights = self.attention(features)\n",
    "        # (32, 7, 512): attented_features, (32, 7): attended_weights\n",
    "        # print(f'Attended Features Shape: {attended_features.shape}, Attended Weights Shape: {attended_weights.shape}')\n",
    "        if self.attention_method != 'self-attention':\n",
    "            # Shape of attented_features: (batch_size, feature_dim)\n",
    "            attended_features_reshaped = attended_features.view(-1, 512) # Shape: (batch_size, feature_dim)\n",
    "        else:\n",
    "            # Shape of attended_features: (batch_size, num_instances, feature_dim) \n",
    "            attended_features_reshaped = attended_features.mean(dim=1) # Shape: (batch_size, feature_dim)\n",
    "            \n",
    "        # gp_output = self.gp_layer(features_for_gp)\n",
    "        # print(f\"GP Output Mean: {gp_output.mean}\")\n",
    "        \n",
    "        # print(f'Shape of GP Output.Mean: {gp_output.mean.shape}')\n",
    "        # gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "        # print(f'Shape of GP Mean: {gp_mean.shape}')\n",
    "        gp_output = self.gp_layer(attended_features_reshaped)\n",
    "        gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "        if self.attention_method != 'self-attention':\n",
    "            combine_features = torch.cat((attended_features, gp_mean), dim=1)\n",
    "        else: \n",
    "            combine_features = torch.cat((attended_features_reshaped, gp_mean), dim=1)\n",
    "            \n",
    "        combine_features = self.dropout(combine_features)\n",
    "        outputs = torch.sigmoid(self.classifier(combine_features))\n",
    "        \n",
    "        # print(f'Shape of GP Output: {gp_output.mean.shape}')\n",
    "        # print(f'GP Output mean: {gp_output.mean}')\n",
    "        # print(f'GP Output shape mean view -1: {gp_output.mean.view(batch_size, -1)}')\n",
    "        return outputs, attended_weights, gp_output"
   ],
   "id": "68ab01e85b8a8e07",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Process",
   "id": "1e9c93e0734c3f8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss Function",
   "id": "7dca4a53c8cf6042"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:48.223654Z",
     "start_time": "2024-11-15T03:23:48.218015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    # Cross-Entropy Loss for CNN outputs\n",
    "    bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n",
    "    kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "    total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    \n",
    "    return total_loss"
   ],
   "id": "a1fdc800f3638c7b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:48.278785Z",
     "start_time": "2024-11-15T03:23:48.272550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss_v2(outputs, gp_output, mll, target, alpha=0.25):\n",
    "    # Cross-Entropy Loss for CNN outputs\n",
    "    bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n",
    "    gp_loss = -mll(gp_output, target)\n",
    "    total_loss = (1 - alpha) * bce_loss + alpha * gp_loss\n",
    "    \n",
    "    return total_loss"
   ],
   "id": "8c7d6fe383062bcb",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Loop",
   "id": "14baa8180e2ba67d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:48.415922Z",
     "start_time": "2024-11-15T03:23:48.320631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, likelihood, dataloader, epochs=10):\n",
    "    # optimizer = torch.optim.Adam([\n",
    "    #     {'params': model.parameters()},\n",
    "    #     {'params': likelihood.parameters()},\n",
    "    # ], lr=0.001)\n",
    "    \n",
    "    # Collect unique parameters\n",
    "    model_params = list(model.parameters())\n",
    "    gp_layer_params = list(model.gp_layer.hyperparameters())\n",
    "    likelihood_params = list(likelihood.parameters())\n",
    "\n",
    "    # Create a unique set of parameters\n",
    "    unique_params = list(set(model_params + gp_layer_params + likelihood_params))\n",
    "\n",
    "    # Initialize the optimizer with unique parameters\n",
    "    optimizer = torch.optim.Adam(unique_params, lr=0.001)\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.gp_layer.variational_parameters(), num_data=train_generator.__len__(), lr=0.1)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    print(f'Shape of train_generator: {train_generator.__len__()}')\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_generator.__len__())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, attended_weights, gp_output = model(batch_images.float())\n",
    "\n",
    "            # loss = combined_loss(outputs.squeeze(), model.gp_layer, batch_labels)\n",
    "            loss = combined_loss_v2(outputs.squeeze(), gp_output, mll, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward() # Calculate gradients\n",
    "\n",
    "            optimizer.step() # Update model parameters\n",
    "            variational_ngd_optimizer.step() # Update variational parameters\n",
    "            \n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs \n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(all_labels, all_outputs)\n",
    "        recall = recall_score(all_labels, all_outputs)\n",
    "        precision = precision_score(all_labels, all_outputs)\n",
    "        f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f},F1 Score: {f1:.4f}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model = MILResNet18(attention_method='attention').to(device)\n",
    "model.to(device)\n",
    "model.gp_layer.covar_module.base_kernel.initialize(lengthscale=0.2)\n",
    "\n",
    "likelihood = PGLikelihood()\n",
    "# likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "likelihood = likelihood.to(device)"
   ],
   "id": "48899664dd45f891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:52.202011Z",
     "start_time": "2024-11-15T03:23:48.424376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "train(model, likelihood, train_loader)"
   ],
   "id": "c03bd99f40d20057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_generator: 1000\n",
      "Epoch [1/10], Loss: 0.3575, Accuracy: 0.7903, Precision: 0.7065, Recall: 0.6292,F1 Score: 0.6656\n",
      "Epoch [2/10], Loss: 0.1400, Accuracy: 0.9496, Precision: 0.9290, Recall: 0.9177,F1 Score: 0.9233\n",
      "Epoch [3/10], Loss: 0.0942, Accuracy: 0.9637, Precision: 0.9509, Recall: 0.9394,F1 Score: 0.9451\n",
      "Epoch [4/10], Loss: 0.0712, Accuracy: 0.9748, Precision: 0.9660, Recall: 0.9572,F1 Score: 0.9616\n",
      "Epoch [5/10], Loss: 0.0705, Accuracy: 0.9758, Precision: 0.9811, Recall: 0.9455,F1 Score: 0.9630\n",
      "Epoch [6/10], Loss: 0.0349, Accuracy: 0.9909, Precision: 0.9969, Recall: 0.9758,F1 Score: 0.9862\n",
      "Epoch [7/10], Loss: 0.0686, Accuracy: 0.9788, Precision: 0.9642, Recall: 0.9729,F1 Score: 0.9685\n",
      "Epoch [8/10], Loss: 0.0432, Accuracy: 0.9869, Precision: 0.9907, Recall: 0.9695,F1 Score: 0.9800\n",
      "Epoch [9/10], Loss: 0.0433, Accuracy: 0.9829, Precision: 0.9785, Recall: 0.9696,F1 Score: 0.9740\n",
      "Epoch [10/10], Loss: 0.0320, Accuracy: 0.9889, Precision: 0.9818, Recall: 0.9848,F1 Score: 0.9833\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing Process",
   "id": "cd71439a580ec535"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot Attended Weights",
   "id": "a38781df28ad2d0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Attention Weights Visualization",
   "id": "3b480d27569a9b00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:52.212384Z",
     "start_time": "2024-11-15T03:23:52.210335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_attended_weights(images, attended_weights):\n",
    "    \"\"\"\n",
    "    Plots the original images and their corresponding attended weights in a single figure.\n",
    "\n",
    "    Args:\n",
    "        images (numpy array): A batch of input image arrays.\n",
    "        attended_weights (numpy array): The attended weights corresponding to the images.\n",
    "    \"\"\"\n",
    "    images = images[0]\n",
    "    attended_weights = attended_weights[0]\n",
    "    num_images = images.shape[0]  # Number of images to plot\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 4, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i].transpose(1, 2, 0))  # Change from CHW to HWC format\n",
    "        axes[i].set_title(f'Image {i + 1} - {attended_weights[i]:.4f}')\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "eadcdf4947bd4b4e",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Self-Attention Weights Visualization",
   "id": "8a60897b51633be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:52.269376Z",
     "start_time": "2024-11-15T03:23:52.258477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_self_attention(images, attention_maps):\n",
    "    \"\"\"\n",
    "    Plots the original images and their corresponding self-attention maps in a matrix format.\n",
    "\n",
    "    Args:\n",
    "        images (numpy array): A batch of input image arrays with shape (num_instances, channels, height, width).\n",
    "        attention_maps (numpy array): The attention maps corresponding to the images with shape (num_instances, height, width).\n",
    "    \"\"\"\n",
    "    num_instances = images.shape[0]  # Number of instances/images to plot\n",
    "    print(f'Shape of images: {images.shape}, Shape of attention maps: {attention_maps.shape}')\n",
    "    \n",
    "    # Save the attention maps for each instance to txt file \n",
    "    with open('attention_maps.txt', 'w') as f:\n",
    "        for i in range(num_instances):\n",
    "            f.write(f'Instance {i + 1}\\n')\n",
    "            f.write(f'{attention_maps[i]}\\n\\n')\n",
    "\n",
    "    # Create subplots with 2 columns: original images and attention maps\n",
    "    fig, axes = plt.subplots(num_instances, 2, figsize=(10, num_instances * 4))\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(images[i].transpose(1, 2, 0))  # Change from CHW to HWC format\n",
    "        axes[i, 0].set_title(f'Original Image {i + 1}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot attention map\n",
    "        axes[i, 1].imshow(attention_maps[i], cmap='jet', alpha=0.5)  # Example colormap\n",
    "        axes[i, 1].set_title(f'Attention Map {i + 1}')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "e125e9d3ec82cdfb",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test Function",
   "id": "3b145626c8744bb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T03:23:52.638537Z",
     "start_time": "2024-11-15T03:23:52.314247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, dataloader, likelihood):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    all_attended_weights = []  # Store attended weights for visualization\n",
    "    images_to_plot = []  # Store images with label = 1\n",
    "    weights_to_plot = []  # Store attended weights for images with label = 1\n",
    "    \n",
    "    nlls_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, attended_weights, gp_output = model(batch_images.float())\n",
    "            \n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "            \n",
    "            # Calculate NLL for the current batch\n",
    "            assert batch_labels.shape == gp_output.mean.shape, 'Shapes of labels and GP outputs do not match'  \n",
    "            nlls = -likelihood.log_marginal(batch_labels.float(), gp_output)\n",
    "            # print(f'NLLs: {nlls.mean().item()}')\n",
    "            print(f'NLLs: {nlls}')\n",
    "            nlls_list.append(nlls.mean().item())  # Store mean NLL for this batch\n",
    "            \n",
    "            # # Calculate accuracy based on Gaussian likelihood\n",
    "            # # Use log_prob instead of probs\n",
    "            # predicted_distribution = likelihood(gp_output)  # Get MultivariateNormal distribution\n",
    "            # print(f'Predicted distribution: {predicted_distribution.sample()}')\n",
    "            # print(f'Predicted distribution log_prob: {predicted_distribution.log_prob(batch_labels)}')\n",
    "            # predicted_probs = (predicted_distribution.log_prob(batch_labels) > -0.693).float()  # -0.693 is log(0.5)\n",
    "            # print(f'Predicted probs: {predicted_probs}')\n",
    "            # acc = (predicted_probs == batch_labels).float().mean()\n",
    "            # # print(f'Batch label bool shape: {batch_labels.bool().shape}, Predicted probs shape: {predicted_probs}')\n",
    "            # acc_list.append(acc.item())  # Store accuracy for this batch\n",
    "            \n",
    "            print(f'GP output: {gp_output.mean}')\n",
    "            print(f'Likelihood: {likelihood(gp_output.mean).probs}')\n",
    "            \n",
    "            predicted_distribution = likelihood(gp_output).sample()\n",
    "            print(f'Predicted distribution: {predicted_distribution}')\n",
    "            print(f'Batch labels: {batch_labels}')\n",
    "            # acc = (likelihood(gp_output.mean).probs.gt(0.5) == batch_labels.bool()).float().mean()\n",
    "            acc = predicted_distribution.eq(batch_labels).float().mean()\n",
    "            print(f'Accuracy: {acc.item()}')\n",
    "            # print(f'Accuracy: {acc.item()}')\n",
    "            acc_list.append(acc.item())\n",
    "\n",
    "            # Check for images with label = 1\n",
    "            for i in range(len(batch_labels)):\n",
    "                if batch_labels[i] == 1:\n",
    "                    images_to_plot.append(batch_images[i].cpu().numpy())\n",
    "                    weights_to_plot.append(attended_weights[i].squeeze().cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_outputs)\n",
    "    recall = recall_score(all_labels, all_outputs)\n",
    "    precision = precision_score(all_labels, all_outputs)\n",
    "    f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f},F1 Score: {f1:.4f}')\n",
    "    \n",
    "    # Calculate average NLL and accuracy across all batches\n",
    "    avg_nll = np.mean(nlls_list)\n",
    "    avg_acc = np.mean(acc_list)\n",
    "\n",
    "    print(f'Test NLL: {avg_nll:.4f}')\n",
    "    print(f'Test Acc: {avg_acc:.4f}')\n",
    "\n",
    "    # Plotting attended weights for images with label = 1\n",
    "    if images_to_plot:  # Check if there are any images to plot\n",
    "        plot_attended_weights(np.array(images_to_plot), np.array(weights_to_plot))\n",
    "        # plot_self_attention(np.array(images_to_plot[0]), np.array(weights_to_plot))\n",
    "        \n",
    "# Call the test function with your model and test loader\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "test(model, test_loader, likelihood)"
   ],
   "id": "4f04580fad06751e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLLs: tensor([0.8099, 0.8099, 0.5886, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.8099,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.4375\n",
      "NLLs: tensor([0.5886, 0.8099, 0.5886, 0.8099, 0.5886, 0.8099, 0.8099, 0.5886, 0.8099,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.5625\n",
      "NLLs: tensor([0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.5625\n",
      "NLLs: tensor([0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.8099, 0.8099, 0.5886, 0.8099, 0.8099, 0.8099, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.375\n",
      "NLLs: tensor([0.5886, 0.5886, 0.8099, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886,\n",
      "        0.8099, 0.5886, 0.8099, 0.8099, 0.8099, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.125\n",
      "NLLs: tensor([0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.5886,\n",
      "        0.8099, 0.8099, 0.8099, 0.8099, 0.5886, 0.8099, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.4375\n",
      "NLLs: tensor([0.8099, 0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886,\n",
      "        0.8099, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.375\n",
      "NLLs: tensor([0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.8099, 0.8099, 0.8099, 0.5886,\n",
      "        0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.4375\n",
      "NLLs: tensor([0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886,\n",
      "        0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.5\n",
      "NLLs: tensor([0.8099, 0.8099, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.375\n",
      "NLLs: tensor([0.5886, 0.5886, 0.8099, 0.8099, 0.5886, 0.8099, 0.5886, 0.5886, 0.8099,\n",
      "        0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.6875\n",
      "NLLs: tensor([0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.5\n",
      "NLLs: tensor([0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.8099,\n",
      "        0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.5\n",
      "NLLs: tensor([0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.625\n",
      "NLLs: tensor([0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.75\n",
      "NLLs: tensor([0.8099, 0.8099, 0.8099, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.8099,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.4375\n",
      "NLLs: tensor([0.5886, 0.5886, 0.8099, 0.5886, 0.8099, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.5625\n",
      "NLLs: tensor([0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886,\n",
      "        0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.3125\n",
      "NLLs: tensor([0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.8099, 0.8099, 0.8099,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.5\n",
      "NLLs: tensor([0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.8099,\n",
      "        0.5886, 0.8099, 0.8099, 0.8099, 0.5886, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.3125\n",
      "NLLs: tensor([0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.8099, 0.5886, 0.8099, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.6875\n",
      "NLLs: tensor([0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.5\n",
      "NLLs: tensor([0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.8099,\n",
      "        0.5886, 0.5886, 0.8099, 0.8099, 0.5886, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.5\n",
      "NLLs: tensor([0.5886, 0.8099, 0.5886, 0.8099, 0.8099, 0.8099, 0.5886, 0.8099, 0.5886,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.4375\n",
      "NLLs: tensor([0.5886, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886,\n",
      "        0.5886, 0.5886, 0.8099, 0.8099, 0.5886, 0.8099, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "Accuracy: 0.625\n",
      "NLLs: tensor([0.8099, 0.8099, 0.8099, 0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.5886,\n",
      "        0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.5\n",
      "NLLs: tensor([0.5886, 0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.8099, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.6875\n",
      "NLLs: tensor([0.8099, 0.5886, 0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886,\n",
      "        0.8099, 0.5886, 0.8099, 0.8099, 0.8099, 0.5886, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
      "Accuracy: 0.5625\n",
      "NLLs: tensor([0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.625\n",
      "NLLs: tensor([0.8099, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099,\n",
      "        0.5886, 0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.8099],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n",
      "Accuracy: 0.5625\n",
      "NLLs: tensor([0.8099, 0.5886, 0.8099, 0.8099, 0.5886, 0.5886, 0.8099, 0.5886, 0.5886,\n",
      "        0.5886, 0.5886, 0.5886, 0.5886, 0.5886, 0.8099, 0.5886],\n",
      "       device='cuda:0')\n",
      "GP output: tensor([-0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487,\n",
      "        -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487, -0.2487],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382,\n",
      "        0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382, 0.4382],\n",
      "       device='cuda:0')\n",
      "Predicted distribution: tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "Batch labels: tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "Accuracy: 0.5625\n",
      "Accuracy: 0.9496, Precision: 0.9618, Recall: 0.8882,F1 Score: 0.9235\n",
      "Test NLL: 0.6645\n",
      "Test Acc: 0.5040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2800x800 with 7 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References:\n",
    "[1] https://medium.com/@wangdk93/implement-self-attention-and-cross-attention-in-pytorch-1f1a366c9d4b"
   ],
   "id": "77de1738b73d62c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
