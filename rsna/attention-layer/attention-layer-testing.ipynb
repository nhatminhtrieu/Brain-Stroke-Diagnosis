{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Layer",
   "id": "4a63a61a889046d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Lib",
   "id": "8ef192d740d3f9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:30:57.123323Z",
     "start_time": "2024-10-08T03:30:57.117542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.xpu import device\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random"
   ],
   "id": "743be51811c9768d",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:30:57.173966Z",
     "start_time": "2024-10-08T03:30:57.169743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set a fixed seed value\n",
    "seed_value = 42  # You can choose any integer\n",
    "\n",
    "# Set the random seed for Python's built-in random module\n",
    "random.seed(seed_value)\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# If using CUDA, set the seed for GPU as well (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ],
   "id": "6b8e1153f8ca7340",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "9d7ae223d9dfa27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:30:57.225020Z",
     "start_time": "2024-10-08T03:30:57.219269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "class DatasetGenerator:\n",
    "    def __init__(self, mnist_data, n_bags=1000, min_instances=3, max_instances=5):\n",
    "        self.mnist_data = mnist_data\n",
    "        self.n_bags = n_bags\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.empty_image = torch.empty(1, 28, 28)  # Create an empty image tensor (1x28x28)\n",
    "\n",
    "    def create_bags(self):\n",
    "        bags = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(self.n_bags):\n",
    "            # Randomly choose a number of instances for the bag\n",
    "            n_instances = np.random.randint(self.min_instances, self.max_instances + 1)\n",
    "            \n",
    "            # Randomly select instances from the dataset\n",
    "            bag_indices = np.random.choice(len(self.mnist_data), n_instances, replace=False)\n",
    "            bag_images = [self.mnist_data[i][0] for i in bag_indices]\n",
    "            \n",
    "            # Determine the label: 1 if any instance is '9', else 0\n",
    "            label = 1 if any(self.mnist_data[i][1] == 9 for i in bag_indices) else 0\n",
    "            \n",
    "            # Convert images to tensors and pad to ensure exactly 7 instances\n",
    "            bag_images_tensors = [ToTensor()(img) for img in bag_images]\n",
    "            while len(bag_images_tensors) < 7:\n",
    "                bag_images_tensors.append(self.empty_image)  # Pad with empty image\n",
    "            \n",
    "            bags.append(torch.stack(bag_images_tensors))\n",
    "            labels.append(label)\n",
    "\n",
    "        return bags, labels\n",
    "\n",
    "class TrainDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=1000):\n",
    "        super().__init__(mnist_data, n_bags)\n",
    "\n",
    "class TestDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=500):  # Example: fewer bags for testing\n",
    "        super().__init__(mnist_data, n_bags)"
   ],
   "id": "b0c74e22e200f87c",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:30:58.114229Z",
     "start_time": "2024-10-08T03:30:57.269262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "\n",
    "# Create training dataset generator and generate bags\n",
    "train_generator = TrainDatasetGenerator(mnist_dataset)\n",
    "train_bags, train_labels = train_generator.create_bags()\n",
    "\n",
    "# Create DataLoader for training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(list(zip(train_bags, train_labels)), batch_size=32, shuffle=True)\n",
    "\n",
    "# Create test dataset generator and generate bags\n",
    "test_generator = TestDatasetGenerator(mnist_dataset)\n",
    "test_bags, test_labels = test_generator.create_bags()\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(list(zip(test_bags, test_labels)), batch_size=32, shuffle=False)"
   ],
   "id": "a04b09c68297d96f",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attention Layer",
   "id": "222827e790c53375"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:30:58.125634Z",
     "start_time": "2024-10-08T03:30:58.123485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):  # x.shape (batch_size, seq_length, input_dim)\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        score = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        attention = self.softmax(score)\n",
    "        weighted = torch.bmm(attention, values)\n",
    "        return weighted\n"
   ],
   "id": "a0c5dbb8cfefc24c",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MIL-CNN Model",
   "id": "f63c6e77d47a7914"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:30:58.172621Z",
     "start_time": "2024-10-08T03:30:58.165719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # Modify the first convolutional layer to accept grayscale images\n",
    "        # Change in_channels from 3 to 1\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet.fc = nn.Identity()  # Remove the final classification layer\n",
    "        self.attention = SelfAttention(input_dim=512)  # Assuming output dim from ResNet is 512\n",
    "        self.classifier = nn.Linear(512, 1)  # Binary classification\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    def forward(self, bags):\n",
    "        # bags.shape: (batch_size, num_instances, channels, height, width)\n",
    "        batch_size, num_instances = bags.size(0), bags.size(1)\n",
    "        \n",
    "        # Flatten to (batch_size * num_instances, channels, height, width)\n",
    "        bags_flattened = bags.view(-1, *bags.shape[2:])\n",
    "        \n",
    "        # Get features from ResNet\n",
    "        features = self.resnet(bags_flattened)  # Shape: (batch_size * num_instances, 512)\n",
    "\n",
    "        # Reshape back to (batch_size, num_instances, 512)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        attended_features = self.attention(features)\n",
    "\n",
    "        # Aggregate features (e.g., mean pooling)\n",
    "        aggregated_features = attended_features.mean(dim=1)  # Shape: (batch_size, 512)\n",
    "        \n",
    "        dropped_features = self.dropout(aggregated_features)\n",
    "        # Classify bag\n",
    "        outputs = torch.sigmoid(self.classifier(dropped_features))\n",
    "        \n",
    "        return outputs\n"
   ],
   "id": "eeb49b1fb73f9119",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Process",
   "id": "1e9c93e0734c3f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T03:31:00.790929Z",
     "start_time": "2024-10-08T03:30:58.222443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "def train(model, dataloader, epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_images.float())\n",
    "            loss = criterion(outputs.squeeze(), batch_labels.float())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "            \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(all_labels, all_outputs)\n",
    "        precision = precision_score(all_labels, all_outputs)\n",
    "        f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "### Testing Function\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_images.float())\n",
    "            \n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_outputs)\n",
    "    precision = precision_score(all_labels, all_outputs)\n",
    "    f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MILResNet18()\n",
    "model.to(device)\n",
    "# Train the model\n",
    "train(model, train_loader)\n",
    "# Test the model\n",
    "test(model, test_loader)"
   ],
   "id": "48899664dd45f891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8980, Accuracy: 0.6170, Precision: 0.3906, F1 Score: 0.3221\n",
      "Epoch [2/10], Loss: 0.4352, Accuracy: 0.8310, Precision: 0.7523, F1 Score: 0.7420\n",
      "Epoch [3/10], Loss: 0.2775, Accuracy: 0.8950, Precision: 0.8558, F1 Score: 0.8387\n",
      "Epoch [4/10], Loss: 0.1714, Accuracy: 0.9380, Precision: 0.9245, F1 Score: 0.9046\n",
      "Epoch [5/10], Loss: 0.0996, Accuracy: 0.9640, Precision: 0.9540, F1 Score: 0.9453\n",
      "Epoch [6/10], Loss: 0.1066, Accuracy: 0.9590, Precision: 0.9561, F1 Score: 0.9370\n",
      "Epoch [7/10], Loss: 0.0357, Accuracy: 0.9900, Precision: 0.9909, F1 Score: 0.9848\n",
      "Epoch [8/10], Loss: 0.0413, Accuracy: 0.9900, Precision: 0.9909, F1 Score: 0.9848\n",
      "Epoch [9/10], Loss: 0.0355, Accuracy: 0.9830, Precision: 0.9758, F1 Score: 0.9744\n",
      "Epoch [10/10], Loss: 0.0528, Accuracy: 0.9820, Precision: 0.9729, F1 Score: 0.9729\n",
      "Test Accuracy: 0.8840, Precision: 0.9913, F1 Score: 0.7972\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References:\n",
    "[1] https://medium.com/@wangdk93/implement-self-attention-and-cross-attention-in-pytorch-1f1a366c9d4b"
   ],
   "id": "77de1738b73d62c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
