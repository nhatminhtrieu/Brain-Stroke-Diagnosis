{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention Layer",
   "id": "4a63a61a889046d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Lib",
   "id": "8ef192d740d3f9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:42.341967Z",
     "start_time": "2024-11-10T14:31:42.338920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from click.core import batch\n",
    "from torch.xpu import device\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "id": "743be51811c9768d",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:42.387395Z",
     "start_time": "2024-11-10T14:31:42.384598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set a fixed seed value\n",
    "seed_value = 42\n",
    "# Set the random seed for Python's built-in random module\n",
    "random.seed(seed_value)\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(seed_value)\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# If using CUDA, set the seed for GPU as well (if applicable)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ],
   "id": "6b8e1153f8ca7340",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:42.431372Z",
     "start_time": "2024-11-10T14:31:42.429663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "id": "590e5508bccbb47b",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preparation",
   "id": "9d7ae223d9dfa27d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:42.481170Z",
     "start_time": "2024-11-10T14:31:42.474320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DatasetGenerator:\n",
    "    def __init__(self, mnist_data, n_bags=1000, min_instances=3, max_instances=5):\n",
    "        self.mnist_data = mnist_data\n",
    "        self.n_bags = n_bags\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        self.empty_image = torch.zeros(1, 28, 28)  # Create an empty image tensor (1x28x28)\n",
    "\n",
    "    def create_bags(self):\n",
    "        bags = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(self.n_bags):\n",
    "            # Randomly choose a number of instances for the bag\n",
    "            n_instances = np.random.randint(self.min_instances, self.max_instances + 1)\n",
    "            \n",
    "            # Randomly select instances from the dataset\n",
    "            bag_indices = np.random.choice(len(self.mnist_data), n_instances, replace=False)\n",
    "            bag_images = [self.mnist_data[i][0] for i in bag_indices]\n",
    "            \n",
    "            # Determine the label: 1 if any instance is '9', else 0\n",
    "            label = 1 if any(self.mnist_data[i][1] == 9 for i in bag_indices) else 0\n",
    "            \n",
    "            # Convert images to tensors and pad to ensure exactly 7 instances\n",
    "            bag_images_tensors = [ToTensor()(img) for img in bag_images]\n",
    "            while len(bag_images_tensors) < 7:\n",
    "                bag_images_tensors.append(self.empty_image)  # Pad with empty image\n",
    "            \n",
    "            bags.append(torch.stack(bag_images_tensors))\n",
    "            labels.append(label)\n",
    "\n",
    "        return bags, labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_bags\n",
    "\n",
    "class TrainDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=1000):\n",
    "        super().__init__(mnist_data, n_bags)\n",
    "\n",
    "class TestDatasetGenerator(DatasetGenerator):\n",
    "    def __init__(self, mnist_data, n_bags=500):  # Example: fewer bags for testing\n",
    "        super().__init__(mnist_data, n_bags)"
   ],
   "id": "b0c74e22e200f87c",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.381154Z",
     "start_time": "2024-11-10T14:31:42.528144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True)\n",
    "\n",
    "# Create training dataset generator and generate bags\n",
    "train_generator = TrainDatasetGenerator(mnist_dataset)\n",
    "train_bags, train_labels = train_generator.create_bags()\n",
    "train_loader = DataLoader(list(zip(train_bags, train_labels)), batch_size=32, shuffle=True)\n",
    "\n",
    "# Create test dataset generator and generate bags\n",
    "test_generator = TestDatasetGenerator(mnist_dataset)\n",
    "test_bags, test_labels = test_generator.create_bags()\n",
    "test_loader = DataLoader(list(zip(test_bags, test_labels)), batch_size=16, shuffle=True)"
   ],
   "id": "a04b09c68297d96f",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Attention Layer",
   "id": "222827e790c53375"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Self-Attention",
   "id": "a5f1288b6239bb36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.392359Z",
     "start_time": "2024-11-10T14:31:43.390146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Linear layers for query, key, and value transformations\n",
    "        self.query = nn.Linear(input_dim, input_dim)  # Shape: (input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)    # Shape: (input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)  # Shape: (input_dim, input_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)              # Softmax along the last dimension\n",
    "\n",
    "    def forward(self, x):  # x.shape: (batch_size, seq_length, input_dim) # seq_length = num_instances\n",
    "        # Transform the input into queries, keys, and values\n",
    "        queries = self.query(x)  # Shape: (batch_size, seq_length, input_dim)\n",
    "        keys = self.key(x)       # Shape: (batch_size, seq_length, input_dim)\n",
    "        values = self.value(x)   # Shape: (batch_size, seq_length, input_dim)\n",
    "\n",
    "        # Compute attention scores\n",
    "        score = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)\n",
    "        # Shape of score: (batch_size, seq_length, seq_length)\n",
    "\n",
    "        attention = self.softmax(score)  \n",
    "        # Shape of attention: (batch_size, seq_length, seq_length)\n",
    "\n",
    "        weighted = torch.bmm(attention, values)  \n",
    "        # Shape of weighted: (batch_size, seq_length, input_dim)\n",
    "\n",
    "        return weighted, attention  # Returns weighted output and attention scores"
   ],
   "id": "5fd7e2264f02a74b",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention Layer",
   "id": "478d1bf7862fe438"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.448103Z",
     "start_time": "2024-11-10T14:31:43.445768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        # Sequential model for attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # Shape: (input_dim, hidden_dim)\n",
    "            nn.Tanh(),                          # Activation function\n",
    "            nn.Linear(hidden_dim, 1)           # Shape: (hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        \n",
    "        attention_weights = self.attention(x)  \n",
    "        # Shape of attention_weights: (batch_size, num_instances, 1)\n",
    "\n",
    "        weights = F.softmax(attention_weights, dim=1)  \n",
    "        # Shape of weights: (batch_size, num_instances, 1)\n",
    "\n",
    "        # Element-wise multiplication followed by summation over num_instances\n",
    "        weighted_sum = (x * weights).sum(dim=1)  \n",
    "        # Shape of weighted_sum: (batch_size, feature_dim)\n",
    "\n",
    "        return weighted_sum, weights.squeeze(-1)  \n",
    "        # Returns weighted sum and attention weights with shape (batch_size, num_instances)"
   ],
   "id": "a0c5dbb8cfefc24c",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gaussian Process Layer",
   "id": "184e116b03eebef0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.492809Z",
     "start_time": "2024-11-10T14:31:43.488383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PGLikelihood(gpytorch.likelihoods._OneDimensionalLikelihood):\n",
    "    # this method effectively computes the expected log likelihood\n",
    "    # contribution to Eqn (10) in Reference [1].\n",
    "    # def expected_log_prob(self, target, input, *args, **kwargs):\n",
    "    #     mean, variance = input.mean, input.variance\n",
    "    #     # Compute the expectation E[f_i^2]\n",
    "    #     raw_second_moment = variance + mean.pow(2)\n",
    "    # \n",
    "    #     # Translate targets to be -1, 1\n",
    "    #     target = target.to(mean.dtype).mul(2.).sub(1.)\n",
    "    # \n",
    "    #     # We detach the following variable since we do not want\n",
    "    #     # to differentiate through the closed-form PG update.\n",
    "    #     c = raw_second_moment.detach().sqrt()\n",
    "    #     # Compute mean of PG auxiliary variable omega: 0.5 * Expectation[omega]\n",
    "    #     # See Eqn (11) and Appendix A2 and A3 in Reference [1] for details.\n",
    "    #     half_omega = 0.25 * torch.tanh(0.5 * c) / c\n",
    "    # \n",
    "    #     # Expected log likelihood\n",
    "    #     res = 0.5 * target * mean - half_omega * raw_second_moment\n",
    "    #     # Sum over data points in mini-batch\n",
    "    #     res = res.sum(dim=-1)\n",
    "    # \n",
    "    #     return res\n",
    "    def expected_log_prob(self, target, input, *args, **kwargs):\n",
    "        # Calculate expected log probability for each instance in a bag\n",
    "        probs = torch.sigmoid(input.mean)  # Use mean of GP predictions for simplicity\n",
    "        \n",
    "        # Check dimensions of probs\n",
    "        if probs.dim() == 1:\n",
    "            # If there's only one dimension, reshape it\n",
    "            probs = probs.view(-1, 1)\n",
    "\n",
    "        # Bag-level log probability: positive if any instance is positive\n",
    "        bag_probs = 1 - (1 - probs).prod(dim=1)  # P(bag positive)\n",
    "        \n",
    "        return bag_probs.log() * target + (1 - target) * (1 - bag_probs).log()\n",
    "\n",
    "    # define the likelihood\n",
    "    def forward(self, function_samples):\n",
    "        return torch.distributions.Bernoulli(logits=function_samples)\n",
    "\n",
    "    # define the marginal likelihood using Gauss Hermite quadrature\n",
    "    def marginal(self, function_dist):\n",
    "        prob_lambda = lambda function_samples: self.forward(function_samples).probs\n",
    "        probs = self.quadrature(prob_lambda, function_dist)\n",
    "        return torch.distributions.Bernoulli(probs=probs)"
   ],
   "id": "70532c42caedd4b5",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.545515Z",
     "start_time": "2024-11-10T14:31:43.536455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        # variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n",
    "        # variational_distribution = gpytorch.variational.TrilNaturalVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ],
   "id": "dbf94e502c230128",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MIL Model",
   "id": "60671b87cb13ea1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.605790Z",
     "start_time": "2024-11-10T14:31:43.590056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MILResNet18(nn.Module):\n",
    "    def __init__(self, attention_method='self-attention'):\n",
    "        super(MILResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.attention_method = attention_method\n",
    "        \n",
    "        self.classifier = nn.Linear(512 + 1, 1)\n",
    "        if attention_method != 'self-attention':\n",
    "            self.attention = AttentionLayer(input_dim=512)\n",
    "        else:\n",
    "            self.attention = SelfAttention(input_dim=512)\n",
    "            \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        # Define inducing points for the GP layer\n",
    "        inducing_points = torch.randn(32, 512)\n",
    "        # inducing_points = torch.full((512, 512), 1e-20)  \n",
    "        \n",
    "        # Define inducing points using linspace\n",
    "        # start_value = -1.0  # Example start value\n",
    "        # end_value = 1.0     # Example end value\n",
    "        # steps = 32          # Number of inducing points (should match the first dimension)\n",
    "        # \n",
    "        # inducing_points = torch.linspace(start_value, end_value, steps).unsqueeze(1).expand(-1, 512)\n",
    "        \n",
    "        self.gp_layer = GPModel(inducing_points)    \n",
    "\n",
    "    def forward(self, bags):\n",
    "        # Input shape: torch.Size([32, 7, 1, 28, 28]) # batch_size, num_instances, channels, height, width\n",
    "        batch_size, num_instances = bags.size(0), bags.size(1)\n",
    "        bags_flattened = bags.view(-1, *bags.shape[2:])\n",
    "\n",
    "        features = self.resnet(bags_flattened)\n",
    "        features = features.view(batch_size, num_instances, -1)\n",
    "        attended_features, attended_weights = self.attention(features)\n",
    "        # (32, 7, 512): attented_features, (32, 7): attended_weights\n",
    "        # print(f'Attended Features Shape: {attended_features.shape}, Attended Weights Shape: {attended_weights.shape}')\n",
    "        if self.attention_method != 'self-attention':\n",
    "            attended_features_reshaped = attended_features.view(-1, 512) # Shape: (batch_size * num_instances, feature_dim)\n",
    "        else:\n",
    "            # Shape of attended_features: (batch_size, num_instances, feature_dim) \n",
    "            attended_features_reshaped = attended_features.mean(dim=1) # Shape: (batch_size, feature_dim)\n",
    "            \n",
    "        gp_output = self.gp_layer(attended_features_reshaped)\n",
    "        # gp_output = self.gp_layer(features)\n",
    "        print(f\"GP Output: {gp_output.mean}\")\n",
    "        # print(f'Shape of GP Output: {gp_output.mean.shape}')\n",
    "        gp_mean = gp_output.mean.view(batch_size, -1)\n",
    "        if self.attention_method != 'self-attention':\n",
    "            combine_features = torch.cat((attended_features, gp_mean), dim=1)\n",
    "        else: \n",
    "            combine_features = torch.cat((attended_features_reshaped, gp_mean), dim=1)\n",
    "            \n",
    "        combine_features = self.dropout(combine_features)\n",
    "        outputs = torch.sigmoid(self.classifier(combine_features))\n",
    "        \n",
    "        # print(f'Shape of GP Output: {gp_output.mean.shape}')\n",
    "        # print(f'GP Output mean: {gp_output.mean}')\n",
    "        # print(f'GP Output shape mean view -1: {gp_output.mean.view(batch_size, -1)}')\n",
    "        return outputs, attended_weights, gp_output"
   ],
   "id": "68ab01e85b8a8e07",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Process",
   "id": "1e9c93e0734c3f8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss Function",
   "id": "7dca4a53c8cf6042"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.656128Z",
     "start_time": "2024-11-10T14:31:43.649192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss(outputs, gp_distribution, target, alpha=0.5):\n",
    "    # Cross-Entropy Loss for CNN outputs\n",
    "    bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n",
    "    kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n",
    "    total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n",
    "    \n",
    "    return total_loss"
   ],
   "id": "a1fdc800f3638c7b",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.702600Z",
     "start_time": "2024-11-10T14:31:43.698677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_loss_v2(outputs, gp_output, mll, target, alpha=0.25):\n",
    "    # Cross-Entropy Loss for CNN outputs\n",
    "    bce_loss = nn.BCELoss()(outputs.squeeze(), target.float())\n",
    "    gp_loss = -mll(gp_output, target)\n",
    "\n",
    "    total_loss = (1 - alpha) * bce_loss + alpha * gp_loss\n",
    "    return total_loss"
   ],
   "id": "8c7d6fe383062bcb",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training Loop",
   "id": "14baa8180e2ba67d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:43.857017Z",
     "start_time": "2024-11-10T14:31:43.750251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, likelihood, dataloader, epochs=10):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},\n",
    "        {'params': likelihood.parameters()},\n",
    "    ], lr=0.001)\n",
    "    \n",
    "    variational_ngd_optimizer = gpytorch.optim.NGD(model.gp_layer.variational_parameters(), num_data=train_generator.__len__(), lr=0.1)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=train_generator.__len__())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            variational_ngd_optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, attended_weights, gp_output = model(batch_images.float())\n",
    "\n",
    "            # loss = combined_loss(outputs.squeeze(), model.gp_layer, batch_labels)\n",
    "            loss = combined_loss_v2(outputs.squeeze(), gp_output, mll, batch_labels)\n",
    "            # loss = -mll(gp_output, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            variational_ngd_optimizer.step()\n",
    "            \n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs \n",
    "\n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = accuracy_score(all_labels, all_outputs)\n",
    "        recall = recall_score(all_labels, all_outputs)\n",
    "        precision = precision_score(all_labels, all_outputs)\n",
    "        f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, '\n",
    "              f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f},F1 Score: {f1:.4f}')\n",
    "    \n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(f\"{name} grad: {param.grad}\")\n",
    "    #     else:\n",
    "    #         print(f\"{name} grad: None\")\n",
    "    #         \n",
    "    #     print(f'{name} requires grad: {param.requires_grad}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "model = MILResNet18(attention_method='attention').to(device)\n",
    "model.to(device)\n",
    "model.gp_layer.covar_module.base_kernel.initialize(lengthscale=0.2)\n",
    "\n",
    "# likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "likelihood = PGLikelihood()\n",
    "likelihood = likelihood.to(device)"
   ],
   "id": "48899664dd45f891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:48.287599Z",
     "start_time": "2024-11-10T14:31:43.869754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "train(model, likelihood, train_loader)"
   ],
   "id": "c03bd99f40d20057",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Output: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,\n",
      "        -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,\n",
      "        -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010,\n",
      "        -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010, -0.0010],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020,\n",
      "        -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020,\n",
      "        -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020,\n",
      "        -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020, -0.0020],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "        -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "        -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
      "        -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039,\n",
      "        -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039,\n",
      "        -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039,\n",
      "        -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039, -0.0039],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047,\n",
      "        -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047,\n",
      "        -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047,\n",
      "        -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047, -0.0047],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056,\n",
      "        -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056,\n",
      "        -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056,\n",
      "        -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056, -0.0056],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064,\n",
      "        -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064,\n",
      "        -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064,\n",
      "        -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064, -0.0064],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071,\n",
      "        -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071,\n",
      "        -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071,\n",
      "        -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071, -0.0071],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079,\n",
      "        -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079,\n",
      "        -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079,\n",
      "        -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079, -0.0079],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087,\n",
      "        -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087,\n",
      "        -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087,\n",
      "        -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087, -0.0087],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093,\n",
      "        -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093,\n",
      "        -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093,\n",
      "        -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093, -0.0093],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100,\n",
      "        -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100,\n",
      "        -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100,\n",
      "        -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107,\n",
      "        -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107,\n",
      "        -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107,\n",
      "        -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107, -0.0107],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115,\n",
      "        -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115,\n",
      "        -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115,\n",
      "        -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115, -0.0115],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123,\n",
      "        -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123,\n",
      "        -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123,\n",
      "        -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123, -0.0123],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131,\n",
      "        -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131,\n",
      "        -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131,\n",
      "        -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131, -0.0131],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139,\n",
      "        -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139,\n",
      "        -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139,\n",
      "        -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139, -0.0139],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147,\n",
      "        -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147,\n",
      "        -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147,\n",
      "        -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147, -0.0147],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156,\n",
      "        -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156, -0.0156],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164,\n",
      "        -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164,\n",
      "        -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164,\n",
      "        -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164, -0.0164],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174,\n",
      "        -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174,\n",
      "        -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174,\n",
      "        -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174, -0.0174],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183,\n",
      "        -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183,\n",
      "        -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183,\n",
      "        -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193,\n",
      "        -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193,\n",
      "        -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193,\n",
      "        -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193, -0.0193],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203,\n",
      "        -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203,\n",
      "        -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203,\n",
      "        -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203, -0.0203],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212,\n",
      "        -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212,\n",
      "        -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212,\n",
      "        -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212, -0.0212],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222,\n",
      "        -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222,\n",
      "        -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222,\n",
      "        -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222, -0.0222],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232,\n",
      "        -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232,\n",
      "        -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232,\n",
      "        -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232, -0.0232],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242,\n",
      "        -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242,\n",
      "        -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242,\n",
      "        -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242, -0.0242],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251,\n",
      "        -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251,\n",
      "        -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251,\n",
      "        -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251, -0.0251],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261,\n",
      "        -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261,\n",
      "        -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261,\n",
      "        -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261, -0.0261],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0270, -0.0270, -0.0270, -0.0270, -0.0270, -0.0270, -0.0270, -0.0270],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [1/10], Loss: 0.4851, Accuracy: 0.8080, Precision: 0.7229, Recall: 0.6837,F1 Score: 0.7028\n",
      "GP Output: tensor([-0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280,\n",
      "        -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280,\n",
      "        -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280,\n",
      "        -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280, -0.0280],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288,\n",
      "        -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288,\n",
      "        -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288,\n",
      "        -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288, -0.0288],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297,\n",
      "        -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297,\n",
      "        -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297,\n",
      "        -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297, -0.0297],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305,\n",
      "        -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305,\n",
      "        -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305,\n",
      "        -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305, -0.0305],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314,\n",
      "        -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314,\n",
      "        -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314,\n",
      "        -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314, -0.0314],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323,\n",
      "        -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323,\n",
      "        -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323,\n",
      "        -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323, -0.0323],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332,\n",
      "        -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332,\n",
      "        -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332,\n",
      "        -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332, -0.0332],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340,\n",
      "        -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340,\n",
      "        -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340,\n",
      "        -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340, -0.0340],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348,\n",
      "        -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348,\n",
      "        -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348,\n",
      "        -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348, -0.0348],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356,\n",
      "        -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356,\n",
      "        -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356,\n",
      "        -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356, -0.0356],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365,\n",
      "        -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365,\n",
      "        -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365,\n",
      "        -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365, -0.0365],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374,\n",
      "        -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374,\n",
      "        -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374,\n",
      "        -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374, -0.0374],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383,\n",
      "        -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383,\n",
      "        -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383,\n",
      "        -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383, -0.0383],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392,\n",
      "        -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392,\n",
      "        -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392,\n",
      "        -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392, -0.0392],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401,\n",
      "        -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401,\n",
      "        -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401,\n",
      "        -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401, -0.0401],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411,\n",
      "        -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411,\n",
      "        -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411,\n",
      "        -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411, -0.0411],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420,\n",
      "        -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420,\n",
      "        -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420,\n",
      "        -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420, -0.0420],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428,\n",
      "        -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428,\n",
      "        -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428,\n",
      "        -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428, -0.0428],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437,\n",
      "        -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437,\n",
      "        -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437,\n",
      "        -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437, -0.0437],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445,\n",
      "        -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445,\n",
      "        -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445,\n",
      "        -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445, -0.0445],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453,\n",
      "        -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453,\n",
      "        -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453,\n",
      "        -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453, -0.0453],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462,\n",
      "        -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462,\n",
      "        -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462,\n",
      "        -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462, -0.0462],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470,\n",
      "        -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470,\n",
      "        -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470,\n",
      "        -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470, -0.0470],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479,\n",
      "        -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479,\n",
      "        -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479,\n",
      "        -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479, -0.0479],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487,\n",
      "        -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487,\n",
      "        -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487,\n",
      "        -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487, -0.0487],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496,\n",
      "        -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496,\n",
      "        -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496,\n",
      "        -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496, -0.0496],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505,\n",
      "        -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505,\n",
      "        -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505,\n",
      "        -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505, -0.0505],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514,\n",
      "        -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514,\n",
      "        -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514,\n",
      "        -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514, -0.0514],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524,\n",
      "        -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524,\n",
      "        -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524,\n",
      "        -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524, -0.0524],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533,\n",
      "        -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533,\n",
      "        -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533,\n",
      "        -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533, -0.0533],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543,\n",
      "        -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543,\n",
      "        -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543,\n",
      "        -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543, -0.0543],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0553, -0.0553, -0.0553, -0.0553, -0.0553, -0.0553, -0.0553, -0.0553],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [2/10], Loss: 0.2959, Accuracy: 0.9410, Precision: 0.9075, Recall: 0.9157,F1 Score: 0.9115\n",
      "GP Output: tensor([-0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562,\n",
      "        -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562,\n",
      "        -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562,\n",
      "        -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562, -0.0562],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572,\n",
      "        -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572,\n",
      "        -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572,\n",
      "        -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572, -0.0572],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581,\n",
      "        -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581,\n",
      "        -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581,\n",
      "        -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581, -0.0581],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589,\n",
      "        -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589,\n",
      "        -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589,\n",
      "        -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589, -0.0589],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598,\n",
      "        -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598,\n",
      "        -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598,\n",
      "        -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598, -0.0598],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606,\n",
      "        -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606,\n",
      "        -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606,\n",
      "        -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606, -0.0606],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615,\n",
      "        -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615,\n",
      "        -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615,\n",
      "        -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615, -0.0615],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623,\n",
      "        -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623,\n",
      "        -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623,\n",
      "        -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623, -0.0623],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631,\n",
      "        -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631,\n",
      "        -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631,\n",
      "        -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631, -0.0631],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639,\n",
      "        -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639,\n",
      "        -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639,\n",
      "        -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639, -0.0639],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647,\n",
      "        -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647,\n",
      "        -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647,\n",
      "        -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647, -0.0647],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655,\n",
      "        -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655,\n",
      "        -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655,\n",
      "        -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655, -0.0655],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663,\n",
      "        -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663,\n",
      "        -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663,\n",
      "        -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663, -0.0663],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672,\n",
      "        -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672,\n",
      "        -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672,\n",
      "        -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672, -0.0672],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679,\n",
      "        -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679,\n",
      "        -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679,\n",
      "        -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679, -0.0679],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687,\n",
      "        -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687,\n",
      "        -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687,\n",
      "        -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687, -0.0687],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695,\n",
      "        -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695,\n",
      "        -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695,\n",
      "        -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695, -0.0695],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703,\n",
      "        -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703,\n",
      "        -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703,\n",
      "        -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703, -0.0703],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711,\n",
      "        -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711,\n",
      "        -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711,\n",
      "        -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711, -0.0711],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
      "        -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
      "        -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
      "        -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725,\n",
      "        -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725,\n",
      "        -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725,\n",
      "        -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725, -0.0725],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733,\n",
      "        -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733,\n",
      "        -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733,\n",
      "        -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733, -0.0733],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742,\n",
      "        -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742,\n",
      "        -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742,\n",
      "        -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742, -0.0742],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751,\n",
      "        -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751,\n",
      "        -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751,\n",
      "        -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751, -0.0751],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760,\n",
      "        -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760,\n",
      "        -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760,\n",
      "        -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760, -0.0760],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769,\n",
      "        -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769,\n",
      "        -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769,\n",
      "        -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769, -0.0769],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779,\n",
      "        -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779,\n",
      "        -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779,\n",
      "        -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779, -0.0779],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789,\n",
      "        -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789,\n",
      "        -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789,\n",
      "        -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789, -0.0789],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798,\n",
      "        -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798,\n",
      "        -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798,\n",
      "        -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798, -0.0798],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808,\n",
      "        -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808,\n",
      "        -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808,\n",
      "        -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808, -0.0808],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817,\n",
      "        -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817,\n",
      "        -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817,\n",
      "        -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817, -0.0817],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0826, -0.0826, -0.0826, -0.0826, -0.0826, -0.0826, -0.0826, -0.0826],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [3/10], Loss: 0.2568, Accuracy: 0.9630, Precision: 0.9538, Recall: 0.9337,F1 Score: 0.9437\n",
      "GP Output: tensor([-0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836,\n",
      "        -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836,\n",
      "        -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836,\n",
      "        -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836, -0.0836],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844,\n",
      "        -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844,\n",
      "        -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844,\n",
      "        -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844, -0.0844],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854,\n",
      "        -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854,\n",
      "        -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854,\n",
      "        -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854, -0.0854],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862,\n",
      "        -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862,\n",
      "        -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862,\n",
      "        -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862, -0.0862],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871,\n",
      "        -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871,\n",
      "        -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871,\n",
      "        -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871, -0.0871],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879,\n",
      "        -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879,\n",
      "        -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879,\n",
      "        -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879, -0.0879],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889,\n",
      "        -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889,\n",
      "        -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889,\n",
      "        -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889, -0.0889],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898,\n",
      "        -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898,\n",
      "        -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898,\n",
      "        -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898, -0.0898],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908,\n",
      "        -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908,\n",
      "        -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908,\n",
      "        -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908, -0.0908],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918,\n",
      "        -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918,\n",
      "        -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918,\n",
      "        -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918, -0.0918],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928,\n",
      "        -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928,\n",
      "        -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928,\n",
      "        -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928, -0.0928],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939,\n",
      "        -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939,\n",
      "        -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939,\n",
      "        -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939, -0.0939],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949,\n",
      "        -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949,\n",
      "        -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949,\n",
      "        -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949, -0.0949],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959,\n",
      "        -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959,\n",
      "        -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959,\n",
      "        -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959, -0.0959],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969,\n",
      "        -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969,\n",
      "        -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969,\n",
      "        -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969, -0.0969],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979,\n",
      "        -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979,\n",
      "        -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979,\n",
      "        -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979, -0.0979],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988,\n",
      "        -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988,\n",
      "        -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988,\n",
      "        -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988, -0.0988],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996,\n",
      "        -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996,\n",
      "        -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996,\n",
      "        -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0996],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005,\n",
      "        -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005,\n",
      "        -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005,\n",
      "        -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005, -0.1005],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013,\n",
      "        -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013,\n",
      "        -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013,\n",
      "        -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013, -0.1013],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021,\n",
      "        -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021,\n",
      "        -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021,\n",
      "        -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021, -0.1021],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030,\n",
      "        -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030,\n",
      "        -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030,\n",
      "        -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030, -0.1030],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038,\n",
      "        -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038,\n",
      "        -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038,\n",
      "        -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038, -0.1038],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046,\n",
      "        -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046,\n",
      "        -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046,\n",
      "        -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046, -0.1046],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053,\n",
      "        -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053,\n",
      "        -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053,\n",
      "        -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053, -0.1053],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061,\n",
      "        -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061,\n",
      "        -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061,\n",
      "        -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061, -0.1061],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069,\n",
      "        -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069,\n",
      "        -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069,\n",
      "        -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069, -0.1069],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077,\n",
      "        -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077,\n",
      "        -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077,\n",
      "        -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077, -0.1077],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083,\n",
      "        -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083,\n",
      "        -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083,\n",
      "        -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083, -0.1083],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090,\n",
      "        -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090,\n",
      "        -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090,\n",
      "        -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090, -0.1090],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097,\n",
      "        -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097,\n",
      "        -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097,\n",
      "        -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097, -0.1097],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1105, -0.1105, -0.1105, -0.1105, -0.1105, -0.1105, -0.1105, -0.1105],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [4/10], Loss: 0.2386, Accuracy: 0.9750, Precision: 0.9637, Recall: 0.9608,F1 Score: 0.9623\n",
      "GP Output: tensor([-0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111,\n",
      "        -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111,\n",
      "        -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111,\n",
      "        -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111, -0.1111],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118,\n",
      "        -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118,\n",
      "        -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118,\n",
      "        -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118, -0.1118],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124,\n",
      "        -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124,\n",
      "        -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124,\n",
      "        -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124, -0.1124],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130,\n",
      "        -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130,\n",
      "        -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130,\n",
      "        -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130, -0.1130],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136,\n",
      "        -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136,\n",
      "        -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136,\n",
      "        -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136, -0.1136],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143,\n",
      "        -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143,\n",
      "        -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143,\n",
      "        -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143, -0.1143],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149,\n",
      "        -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149,\n",
      "        -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149,\n",
      "        -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149, -0.1149],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157,\n",
      "        -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157,\n",
      "        -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157,\n",
      "        -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157, -0.1157],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162,\n",
      "        -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162,\n",
      "        -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162,\n",
      "        -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162, -0.1162],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168,\n",
      "        -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168,\n",
      "        -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168,\n",
      "        -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168, -0.1168],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174,\n",
      "        -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174,\n",
      "        -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174,\n",
      "        -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174, -0.1174],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180,\n",
      "        -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180,\n",
      "        -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180,\n",
      "        -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180, -0.1180],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186,\n",
      "        -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186,\n",
      "        -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186,\n",
      "        -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186, -0.1186],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193,\n",
      "        -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193,\n",
      "        -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193,\n",
      "        -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193, -0.1193],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199,\n",
      "        -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199,\n",
      "        -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199,\n",
      "        -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199, -0.1199],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205,\n",
      "        -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205,\n",
      "        -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205,\n",
      "        -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205, -0.1205],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212,\n",
      "        -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212,\n",
      "        -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212,\n",
      "        -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212, -0.1212],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219,\n",
      "        -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219,\n",
      "        -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219,\n",
      "        -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219, -0.1219],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226,\n",
      "        -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226,\n",
      "        -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226,\n",
      "        -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226, -0.1226],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234,\n",
      "        -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234,\n",
      "        -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234,\n",
      "        -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234, -0.1234],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243,\n",
      "        -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243,\n",
      "        -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243,\n",
      "        -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243, -0.1243],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251,\n",
      "        -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251,\n",
      "        -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251,\n",
      "        -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251, -0.1251],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259,\n",
      "        -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259,\n",
      "        -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259,\n",
      "        -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259, -0.1259],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268,\n",
      "        -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268,\n",
      "        -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268,\n",
      "        -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268, -0.1268],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276,\n",
      "        -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276,\n",
      "        -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276,\n",
      "        -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276, -0.1276],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285,\n",
      "        -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285,\n",
      "        -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285,\n",
      "        -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285, -0.1285],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293,\n",
      "        -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293,\n",
      "        -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293,\n",
      "        -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293, -0.1293],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301,\n",
      "        -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301,\n",
      "        -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301,\n",
      "        -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301, -0.1301],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310,\n",
      "        -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310,\n",
      "        -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310,\n",
      "        -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310, -0.1310],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318,\n",
      "        -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318,\n",
      "        -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318,\n",
      "        -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318, -0.1318],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326,\n",
      "        -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326,\n",
      "        -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326,\n",
      "        -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326, -0.1326],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1335, -0.1335, -0.1335, -0.1335, -0.1335, -0.1335, -0.1335, -0.1335],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [5/10], Loss: 0.2041, Accuracy: 0.9880, Precision: 0.9908, Recall: 0.9729,F1 Score: 0.9818\n",
      "GP Output: tensor([-0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344,\n",
      "        -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344,\n",
      "        -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344,\n",
      "        -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344, -0.1344],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353,\n",
      "        -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353,\n",
      "        -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353,\n",
      "        -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353, -0.1353],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362,\n",
      "        -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362,\n",
      "        -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362,\n",
      "        -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362, -0.1362],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371,\n",
      "        -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371,\n",
      "        -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371,\n",
      "        -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371, -0.1371],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380,\n",
      "        -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380,\n",
      "        -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380,\n",
      "        -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380, -0.1380],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389,\n",
      "        -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389,\n",
      "        -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389,\n",
      "        -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389, -0.1389],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398,\n",
      "        -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398,\n",
      "        -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398,\n",
      "        -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398, -0.1398],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407,\n",
      "        -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407,\n",
      "        -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407,\n",
      "        -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407, -0.1407],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416,\n",
      "        -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416,\n",
      "        -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416,\n",
      "        -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416, -0.1416],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424,\n",
      "        -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424,\n",
      "        -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424,\n",
      "        -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424, -0.1424],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433,\n",
      "        -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433,\n",
      "        -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433,\n",
      "        -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433, -0.1433],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442,\n",
      "        -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442,\n",
      "        -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442,\n",
      "        -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442, -0.1442],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450,\n",
      "        -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450,\n",
      "        -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450,\n",
      "        -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450, -0.1450],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458,\n",
      "        -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458,\n",
      "        -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458,\n",
      "        -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458, -0.1458],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465,\n",
      "        -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465,\n",
      "        -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465,\n",
      "        -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465, -0.1465],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473,\n",
      "        -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473,\n",
      "        -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473,\n",
      "        -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473, -0.1473],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481,\n",
      "        -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481,\n",
      "        -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481,\n",
      "        -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481, -0.1481],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490,\n",
      "        -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490,\n",
      "        -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490,\n",
      "        -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490, -0.1490],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498,\n",
      "        -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498,\n",
      "        -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498,\n",
      "        -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498, -0.1498],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507,\n",
      "        -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507,\n",
      "        -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507,\n",
      "        -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507, -0.1507],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516,\n",
      "        -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516,\n",
      "        -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516,\n",
      "        -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516, -0.1516],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524,\n",
      "        -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524,\n",
      "        -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524,\n",
      "        -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524, -0.1524],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533,\n",
      "        -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533,\n",
      "        -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533,\n",
      "        -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533, -0.1533],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542,\n",
      "        -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542,\n",
      "        -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542,\n",
      "        -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542, -0.1542],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550,\n",
      "        -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550,\n",
      "        -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550,\n",
      "        -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550, -0.1550],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559,\n",
      "        -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559,\n",
      "        -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559,\n",
      "        -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559, -0.1559],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568,\n",
      "        -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568,\n",
      "        -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568,\n",
      "        -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568, -0.1568],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576,\n",
      "        -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576,\n",
      "        -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576,\n",
      "        -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576, -0.1576],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585,\n",
      "        -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585,\n",
      "        -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585,\n",
      "        -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585, -0.1585],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592,\n",
      "        -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592,\n",
      "        -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592,\n",
      "        -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592, -0.1592],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599,\n",
      "        -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599,\n",
      "        -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599,\n",
      "        -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599, -0.1599],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1606, -0.1606, -0.1606, -0.1606, -0.1606, -0.1606, -0.1606, -0.1606],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [6/10], Loss: 0.1985, Accuracy: 0.9890, Precision: 0.9908, Recall: 0.9759,F1 Score: 0.9833\n",
      "GP Output: tensor([-0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613,\n",
      "        -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613,\n",
      "        -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613,\n",
      "        -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613, -0.1613],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621,\n",
      "        -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621,\n",
      "        -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621,\n",
      "        -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621, -0.1621],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629,\n",
      "        -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629,\n",
      "        -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629,\n",
      "        -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629, -0.1629],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638,\n",
      "        -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638,\n",
      "        -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638,\n",
      "        -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638, -0.1638],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646,\n",
      "        -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646,\n",
      "        -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646,\n",
      "        -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646, -0.1646],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655,\n",
      "        -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655,\n",
      "        -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655,\n",
      "        -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655, -0.1655],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663,\n",
      "        -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663,\n",
      "        -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663,\n",
      "        -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663, -0.1663],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671,\n",
      "        -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671,\n",
      "        -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671,\n",
      "        -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671, -0.1671],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678,\n",
      "        -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678,\n",
      "        -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678,\n",
      "        -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678, -0.1678],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685,\n",
      "        -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685,\n",
      "        -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685,\n",
      "        -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685, -0.1685],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693,\n",
      "        -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693,\n",
      "        -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693,\n",
      "        -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693, -0.1693],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700,\n",
      "        -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700,\n",
      "        -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700,\n",
      "        -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700, -0.1700],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706,\n",
      "        -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706,\n",
      "        -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706,\n",
      "        -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706, -0.1706],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713,\n",
      "        -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713,\n",
      "        -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713,\n",
      "        -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713, -0.1713],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720,\n",
      "        -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720,\n",
      "        -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720,\n",
      "        -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720, -0.1720],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728,\n",
      "        -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728,\n",
      "        -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728,\n",
      "        -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728, -0.1728],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735,\n",
      "        -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735,\n",
      "        -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735,\n",
      "        -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735, -0.1735],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742,\n",
      "        -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742,\n",
      "        -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742,\n",
      "        -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742, -0.1742],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750,\n",
      "        -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750,\n",
      "        -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750,\n",
      "        -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750, -0.1750],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757,\n",
      "        -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757,\n",
      "        -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757,\n",
      "        -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757, -0.1757],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763,\n",
      "        -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763,\n",
      "        -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763,\n",
      "        -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763, -0.1763],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
      "        -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
      "        -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770,\n",
      "        -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770, -0.1770],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777,\n",
      "        -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777,\n",
      "        -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777,\n",
      "        -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777, -0.1777],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784,\n",
      "        -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784,\n",
      "        -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784,\n",
      "        -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784, -0.1784],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791,\n",
      "        -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791,\n",
      "        -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791,\n",
      "        -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791, -0.1791],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798,\n",
      "        -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798,\n",
      "        -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798,\n",
      "        -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798, -0.1798],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805,\n",
      "        -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805,\n",
      "        -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805,\n",
      "        -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805, -0.1805],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813,\n",
      "        -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813,\n",
      "        -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813,\n",
      "        -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813, -0.1813],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821,\n",
      "        -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821,\n",
      "        -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821,\n",
      "        -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821, -0.1821],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829,\n",
      "        -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829,\n",
      "        -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829,\n",
      "        -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829, -0.1829],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836,\n",
      "        -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836,\n",
      "        -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836,\n",
      "        -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836, -0.1836],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1843, -0.1843, -0.1843, -0.1843, -0.1843, -0.1843, -0.1843, -0.1843],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [7/10], Loss: 0.2555, Accuracy: 0.9790, Precision: 0.9698, Recall: 0.9669,F1 Score: 0.9683\n",
      "GP Output: tensor([-0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852,\n",
      "        -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852,\n",
      "        -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852,\n",
      "        -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852, -0.1852],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861,\n",
      "        -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861,\n",
      "        -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861,\n",
      "        -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861, -0.1861],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869,\n",
      "        -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869,\n",
      "        -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869,\n",
      "        -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869, -0.1869],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877,\n",
      "        -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877,\n",
      "        -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877,\n",
      "        -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877, -0.1877],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886,\n",
      "        -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886,\n",
      "        -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886,\n",
      "        -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886, -0.1886],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894,\n",
      "        -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894,\n",
      "        -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894,\n",
      "        -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894, -0.1894],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903,\n",
      "        -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903,\n",
      "        -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903,\n",
      "        -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903, -0.1903],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910,\n",
      "        -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910,\n",
      "        -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910,\n",
      "        -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910, -0.1910],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919,\n",
      "        -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919,\n",
      "        -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919,\n",
      "        -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919, -0.1919],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927,\n",
      "        -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927,\n",
      "        -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927,\n",
      "        -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927, -0.1927],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935,\n",
      "        -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935,\n",
      "        -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935,\n",
      "        -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935, -0.1935],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943,\n",
      "        -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943,\n",
      "        -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943,\n",
      "        -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943, -0.1943],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950,\n",
      "        -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950,\n",
      "        -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950,\n",
      "        -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950, -0.1950],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956,\n",
      "        -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956,\n",
      "        -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956,\n",
      "        -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956, -0.1956],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962,\n",
      "        -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962,\n",
      "        -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962,\n",
      "        -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962, -0.1962],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969,\n",
      "        -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969,\n",
      "        -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969,\n",
      "        -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969, -0.1969],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976,\n",
      "        -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976,\n",
      "        -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976,\n",
      "        -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976, -0.1976],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984,\n",
      "        -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984,\n",
      "        -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984,\n",
      "        -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984, -0.1984],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991,\n",
      "        -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991,\n",
      "        -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991,\n",
      "        -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991, -0.1991],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999,\n",
      "        -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999,\n",
      "        -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999,\n",
      "        -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999, -0.1999],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007,\n",
      "        -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007,\n",
      "        -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007,\n",
      "        -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007, -0.2007],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015,\n",
      "        -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015,\n",
      "        -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015,\n",
      "        -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015, -0.2015],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023,\n",
      "        -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023,\n",
      "        -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023,\n",
      "        -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023, -0.2023],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032,\n",
      "        -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032,\n",
      "        -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032,\n",
      "        -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032, -0.2032],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040,\n",
      "        -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040,\n",
      "        -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040,\n",
      "        -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040, -0.2040],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048,\n",
      "        -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048,\n",
      "        -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048,\n",
      "        -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048, -0.2048],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056,\n",
      "        -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056,\n",
      "        -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056,\n",
      "        -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056, -0.2056],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064,\n",
      "        -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064,\n",
      "        -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064,\n",
      "        -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064, -0.2064],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071,\n",
      "        -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071,\n",
      "        -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071,\n",
      "        -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071, -0.2071],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078,\n",
      "        -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078,\n",
      "        -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078,\n",
      "        -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078, -0.2078],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086,\n",
      "        -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086,\n",
      "        -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086,\n",
      "        -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086, -0.2086],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2093, -0.2093, -0.2093, -0.2093, -0.2093, -0.2093, -0.2093, -0.2093],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [8/10], Loss: 0.2504, Accuracy: 0.9680, Precision: 0.9630, Recall: 0.9398,F1 Score: 0.9512\n",
      "GP Output: tensor([-0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099,\n",
      "        -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099,\n",
      "        -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099,\n",
      "        -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099, -0.2099],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105,\n",
      "        -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105,\n",
      "        -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105,\n",
      "        -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105, -0.2105],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112,\n",
      "        -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112,\n",
      "        -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112,\n",
      "        -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112, -0.2112],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120,\n",
      "        -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120,\n",
      "        -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120,\n",
      "        -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120, -0.2120],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127,\n",
      "        -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127,\n",
      "        -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127,\n",
      "        -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127, -0.2127],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135,\n",
      "        -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135,\n",
      "        -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135,\n",
      "        -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135, -0.2135],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142,\n",
      "        -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142,\n",
      "        -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142,\n",
      "        -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142, -0.2142],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149,\n",
      "        -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149,\n",
      "        -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149,\n",
      "        -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149, -0.2149],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157,\n",
      "        -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157,\n",
      "        -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157,\n",
      "        -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157, -0.2157],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165,\n",
      "        -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165,\n",
      "        -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165,\n",
      "        -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165, -0.2165],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173,\n",
      "        -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173,\n",
      "        -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173,\n",
      "        -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173, -0.2173],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181,\n",
      "        -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181,\n",
      "        -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181,\n",
      "        -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181, -0.2181],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187,\n",
      "        -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187,\n",
      "        -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187,\n",
      "        -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187, -0.2187],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194,\n",
      "        -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194,\n",
      "        -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194,\n",
      "        -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194, -0.2194],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201,\n",
      "        -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201,\n",
      "        -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201,\n",
      "        -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201, -0.2201],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208,\n",
      "        -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208,\n",
      "        -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208,\n",
      "        -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208, -0.2208],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215,\n",
      "        -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215,\n",
      "        -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215,\n",
      "        -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215, -0.2215],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223,\n",
      "        -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223,\n",
      "        -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223,\n",
      "        -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223, -0.2223],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231,\n",
      "        -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231,\n",
      "        -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231,\n",
      "        -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231, -0.2231],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239,\n",
      "        -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239,\n",
      "        -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239,\n",
      "        -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239, -0.2239],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246,\n",
      "        -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246,\n",
      "        -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246,\n",
      "        -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246, -0.2246],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252,\n",
      "        -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252,\n",
      "        -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252,\n",
      "        -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252, -0.2252],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257,\n",
      "        -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257,\n",
      "        -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257,\n",
      "        -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257, -0.2257],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263,\n",
      "        -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263,\n",
      "        -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263,\n",
      "        -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263, -0.2263],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268,\n",
      "        -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268,\n",
      "        -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268,\n",
      "        -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268, -0.2268],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274,\n",
      "        -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274,\n",
      "        -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274,\n",
      "        -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274, -0.2274],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281,\n",
      "        -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281,\n",
      "        -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281,\n",
      "        -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281, -0.2281],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287,\n",
      "        -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287,\n",
      "        -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287,\n",
      "        -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287, -0.2287],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294,\n",
      "        -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294,\n",
      "        -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294,\n",
      "        -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294, -0.2294],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300,\n",
      "        -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300,\n",
      "        -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300,\n",
      "        -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300, -0.2300],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307,\n",
      "        -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307,\n",
      "        -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307,\n",
      "        -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307, -0.2307],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313, -0.2313],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [9/10], Loss: 0.2719, Accuracy: 0.9520, Precision: 0.9251, Recall: 0.9307,F1 Score: 0.9279\n",
      "GP Output: tensor([-0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319,\n",
      "        -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319,\n",
      "        -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319,\n",
      "        -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319, -0.2319],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326,\n",
      "        -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326,\n",
      "        -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326,\n",
      "        -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326, -0.2326],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332,\n",
      "        -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332,\n",
      "        -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332,\n",
      "        -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332, -0.2332],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338,\n",
      "        -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338,\n",
      "        -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338,\n",
      "        -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338, -0.2338],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344,\n",
      "        -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344,\n",
      "        -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344,\n",
      "        -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344, -0.2344],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351,\n",
      "        -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351,\n",
      "        -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351,\n",
      "        -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351, -0.2351],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358,\n",
      "        -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358,\n",
      "        -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358,\n",
      "        -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358, -0.2358],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366,\n",
      "        -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366,\n",
      "        -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366,\n",
      "        -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366, -0.2366],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374,\n",
      "        -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374,\n",
      "        -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374,\n",
      "        -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374, -0.2374],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381,\n",
      "        -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381,\n",
      "        -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381,\n",
      "        -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381, -0.2381],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387,\n",
      "        -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387,\n",
      "        -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387,\n",
      "        -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387, -0.2387],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393,\n",
      "        -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393,\n",
      "        -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393,\n",
      "        -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393, -0.2393],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399,\n",
      "        -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399,\n",
      "        -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399,\n",
      "        -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399, -0.2399],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405,\n",
      "        -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405,\n",
      "        -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405,\n",
      "        -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405, -0.2405],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411,\n",
      "        -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411,\n",
      "        -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411,\n",
      "        -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411, -0.2411],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418,\n",
      "        -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418,\n",
      "        -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418,\n",
      "        -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418, -0.2418],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425,\n",
      "        -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425,\n",
      "        -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425,\n",
      "        -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425, -0.2425],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431,\n",
      "        -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431,\n",
      "        -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431,\n",
      "        -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431, -0.2431],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438,\n",
      "        -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438,\n",
      "        -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438,\n",
      "        -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438, -0.2438],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446,\n",
      "        -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446,\n",
      "        -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446,\n",
      "        -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446, -0.2446],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452,\n",
      "        -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452,\n",
      "        -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452,\n",
      "        -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452, -0.2452],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458,\n",
      "        -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458,\n",
      "        -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458,\n",
      "        -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458, -0.2458],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465,\n",
      "        -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465,\n",
      "        -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465,\n",
      "        -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465, -0.2465],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472,\n",
      "        -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472,\n",
      "        -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472,\n",
      "        -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472, -0.2472],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478,\n",
      "        -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478,\n",
      "        -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478,\n",
      "        -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478, -0.2478],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484,\n",
      "        -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484,\n",
      "        -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484,\n",
      "        -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484, -0.2484],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490,\n",
      "        -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490,\n",
      "        -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490,\n",
      "        -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490, -0.2490],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496,\n",
      "        -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496,\n",
      "        -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496,\n",
      "        -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496, -0.2496],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502,\n",
      "        -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502,\n",
      "        -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502,\n",
      "        -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502, -0.2502],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509,\n",
      "        -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509,\n",
      "        -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509,\n",
      "        -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509, -0.2509],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516,\n",
      "        -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516,\n",
      "        -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516,\n",
      "        -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516, -0.2516],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "GP Output: tensor([-0.2523, -0.2523, -0.2523, -0.2523, -0.2523, -0.2523, -0.2523, -0.2523],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Epoch [10/10], Loss: 0.2076, Accuracy: 0.9820, Precision: 0.9701, Recall: 0.9759,F1 Score: 0.9730\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Testing Process",
   "id": "cd71439a580ec535"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot Attended Weights",
   "id": "a38781df28ad2d0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Attention Weights Visualization",
   "id": "3b480d27569a9b00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:48.298524Z",
     "start_time": "2024-11-10T14:31:48.296593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_attended_weights(images, attended_weights):\n",
    "    \"\"\"\n",
    "    Plots the original images and their corresponding attended weights in a single figure.\n",
    "\n",
    "    Args:\n",
    "        images (numpy array): A batch of input image arrays.\n",
    "        attended_weights (numpy array): The attended weights corresponding to the images.\n",
    "    \"\"\"\n",
    "    images = images[0]\n",
    "    attended_weights = attended_weights[0]\n",
    "    num_images = images.shape[0]  # Number of images to plot\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 4, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i].transpose(1, 2, 0))  # Change from CHW to HWC format\n",
    "        axes[i].set_title(f'Image {i + 1} - {attended_weights[i]:.4f}')\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "eadcdf4947bd4b4e",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Self-Attention Weights Visualization",
   "id": "8a60897b51633be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:48.338136Z",
     "start_time": "2024-11-10T14:31:48.335272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_self_attention(images, attention_maps):\n",
    "    \"\"\"\n",
    "    Plots the original images and their corresponding self-attention maps in a matrix format.\n",
    "\n",
    "    Args:\n",
    "        images (numpy array): A batch of input image arrays with shape (num_instances, channels, height, width).\n",
    "        attention_maps (numpy array): The attention maps corresponding to the images with shape (num_instances, height, width).\n",
    "    \"\"\"\n",
    "    num_instances = images.shape[0]  # Number of instances/images to plot\n",
    "    print(f'Shape of images: {images.shape}, Shape of attention maps: {attention_maps.shape}')\n",
    "    \n",
    "    # Save the attention maps for each instance to txt file \n",
    "    with open('attention_maps.txt', 'w') as f:\n",
    "        for i in range(num_instances):\n",
    "            f.write(f'Instance {i + 1}\\n')\n",
    "            f.write(f'{attention_maps[i]}\\n\\n')\n",
    "\n",
    "    # Create subplots with 2 columns: original images and attention maps\n",
    "    fig, axes = plt.subplots(num_instances, 2, figsize=(10, num_instances * 4))\n",
    "\n",
    "    for i in range(num_instances):\n",
    "        # Plot original image\n",
    "        axes[i, 0].imshow(images[i].transpose(1, 2, 0))  # Change from CHW to HWC format\n",
    "        axes[i, 0].set_title(f'Original Image {i + 1}')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot attention map\n",
    "        axes[i, 1].imshow(attention_maps[i], cmap='jet', alpha=0.5)  # Example colormap\n",
    "        axes[i, 1].set_title(f'Attention Map {i + 1}')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "e125e9d3ec82cdfb",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test Function",
   "id": "3b145626c8744bb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T14:31:48.854050Z",
     "start_time": "2024-11-10T14:31:48.381349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(model, dataloader, likelihood):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    all_attended_weights = []  # Store attended weights for visualization\n",
    "    images_to_plot = []  # Store images with label = 1\n",
    "    weights_to_plot = []  # Store attended weights for images with label = 1\n",
    "    \n",
    "    nlls_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, attended_weights, gp_output = model(batch_images.float())\n",
    "            \n",
    "            # Collect outputs and labels for metrics calculation\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "            all_outputs.extend((outputs.squeeze().cpu().detach().numpy() > 0.5).astype(int))  # Binarize outputs\n",
    "            \n",
    "            # Calculate NLL for the current batch\n",
    "            assert batch_labels.shape == gp_output.mean.shape, 'Shapes of labels and GP outputs do not match'\n",
    "            nlls = -likelihood.log_marginal(batch_labels.float(), gp_output)\n",
    "            print(f'NLLs: {nlls.mean().item()}')\n",
    "            nlls_list.append(nlls.mean().item())  # Store mean NLL for this batch\n",
    "            \n",
    "            # # Calculate accuracy based on likelihood\n",
    "            # # Use log_prob instead of probs\n",
    "            # predicted_distribution = likelihood(gp_output)  # Get MultivariateNormal distribution\n",
    "            # \n",
    "            # predicted_probs = (predicted_distribution.log_prob(batch_labels) > -0.693).float()  # -0.693 is log(0.5)\n",
    "            # \n",
    "            # acc = (predicted_probs == batch_labels).float().mean()\n",
    "            # # print(f'Batch label bool shape: {batch_labels.bool().shape}, Predicted probs shape: {predicted_probs}')\n",
    "            # acc_list.append(acc.item())  # Store accuracy for this batch\n",
    "            \n",
    "            # acc = likelihood(gp_output).probs.gt(0.5).eq(batch_labels.float()).float().mean()\n",
    "            print(f'GP output: {gp_output.mean}')\n",
    "            print(f'Likelihood: {likelihood(gp_output.mean).probs}')\n",
    "            acc = (likelihood(gp_output.mean).probs.gt(0.5) == batch_labels.bool()).float().mean()\n",
    "            # print(f'Accuracy: {acc.item()}')\n",
    "            acc_list.append(acc.item())\n",
    "\n",
    "            # Check for images with label = 1\n",
    "            for i in range(len(batch_labels)):\n",
    "                if batch_labels[i] == 1:\n",
    "                    images_to_plot.append(batch_images[i].cpu().numpy())\n",
    "                    weights_to_plot.append(attended_weights[i].squeeze().cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_outputs)\n",
    "    recall = recall_score(all_labels, all_outputs)\n",
    "    precision = precision_score(all_labels, all_outputs)\n",
    "    f1 = f1_score(all_labels, all_outputs)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f},F1 Score: {f1:.4f}')\n",
    "    \n",
    "    # Calculate average NLL and accuracy across all batches\n",
    "    avg_nll = np.mean(nlls_list)\n",
    "    avg_acc = np.mean(acc_list)\n",
    "\n",
    "    print(f'Test NLL: {avg_nll:.4f}')\n",
    "    print(f'Test Acc: {avg_acc:.4f}')\n",
    "\n",
    "    # Plotting attended weights for images with label = 1\n",
    "    if images_to_plot:  # Check if there are any images to plot\n",
    "        plot_attended_weights(np.array(images_to_plot), np.array(weights_to_plot))\n",
    "        # plot_self_attention(np.array(images_to_plot[0]), np.array(weights_to_plot))\n",
    "        \n",
    "# Call the test function with your model and test loader\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "test(model, test_loader, likelihood)"
   ],
   "id": "4f04580fad06751e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6717120409011841\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.616808295249939\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.7128898501396179\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6991639137268066\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6717120409011841\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6717120409011841\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6305342316627502\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6579861044883728\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6717120409011841\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6442601680755615\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6442601680755615\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6442601680755615\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6305342316627502\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6579861044883728\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6442601680755615\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6717120409011841\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6579861044883728\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.616808295249939\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6717120409011841\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6579861044883728\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6854379773139954\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6579861044883728\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6579861044883728\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "NLLs: 0.6579861044883728\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529,\n",
      "        -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529, -0.2529],\n",
      "       device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371,\n",
      "        0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371, 0.4371],\n",
      "       device='cuda:0')\n",
      "GP Output: tensor([-0.2529, -0.2529, -0.2529, -0.2529], device='cuda:0')\n",
      "NLLs: 0.6442601680755615\n",
      "GP output: tensor([-0.2529, -0.2529, -0.2529, -0.2529], device='cuda:0')\n",
      "Likelihood: tensor([0.4371, 0.4371, 0.4371, 0.4371], device='cuda:0')\n",
      "Accuracy: 0.9520, Precision: 0.9742, Recall: 0.8830,F1 Score: 0.9264\n",
      "Test NLL: 0.6640\n",
      "Test Acc: 0.6602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2800x800 with 7 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## References:\n",
    "[1] https://medium.com/@wangdk93/implement-self-attention-and-cross-attention-in-pytorch-1f1a366c9d4b"
   ],
   "id": "77de1738b73d62c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
