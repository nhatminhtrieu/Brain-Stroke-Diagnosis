{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:52:52.931381Z",
     "start_time": "2024-10-07T14:52:51.718123Z"
    }
   },
   "source": [
    "import os\n",
    "import datetime\n",
    "import copy\n",
    "import re\n",
    "import yaml\n",
    "import uuid\n",
    "import warnings\n",
    "import time\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial, reduce\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from numpy.ma.extras import average\n",
    "from torch import nn, optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.datasets import MNIST\n",
    "# import tensorflow as tf\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn import metrics as mtx\n",
    "from sklearn import model_selection as ms"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_216647/2100086295.py:29: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 1. Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:52:52.950550Z",
     "start_time": "2024-10-07T14:52:52.947961Z"
    }
   },
   "source": [
    "def get_data_loaders(train_batch_size, val_batch_size):\n",
    "    mnist = MNIST(download=True, train=True, root=\".\").train_data.float()\n",
    "    \n",
    "    data_transform = Compose([ Resize((224, 224)),ToTensor(), Normalize((mnist.mean()/255,), (mnist.std()/255,))])\n",
    "\n",
    "    train_loader = DataLoader(MNIST(download=True, root=\".\", transform=data_transform, train=True),\n",
    "                              batch_size=train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(MNIST(download=False, root=\".\", transform=data_transform, train=False),\n",
    "                            batch_size=val_batch_size, shuffle=False)\n",
    "    return train_loader, val_loader"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:52:53.089242Z",
     "start_time": "2024-10-07T14:52:53.004588Z"
    }
   },
   "source": [
    "train_batch_size = 256\n",
    "val_batch_size = 256\n",
    "\n",
    "train_loader, valid_loader = get_data_loaders(train_batch_size, val_batch_size)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1.1 define model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:52:53.097428Z",
     "start_time": "2024-10-07T14:52:53.095550Z"
    }
   },
   "source": [
    "class MnistResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super(MnistResNet, self).__init__(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(super(MnistResNet, self).forward(x), dim=-1)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1.2 helper function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:52:53.147472Z",
     "start_time": "2024-10-07T14:52:53.141326Z"
    }
   },
   "source": [
    "def calculate_metric(metric_fn, true_y, pred_y):\n",
    "    # multi class problems need to have averaging method\n",
    "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
    "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
    "    else:\n",
    "        return metric_fn(true_y, pred_y)\n",
    "    \n",
    "def print_scores(p, r, f1, a, batch_size):\n",
    "    # just utility printing function\n",
    "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
    "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1.3 train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:43.442108Z",
     "start_time": "2024-10-07T14:52:53.197206Z"
    }
   },
   "source": [
    "start_ts = time.time()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model:\n",
    "model = MnistResNet().to(device)\n",
    "\n",
    "# params you need to specify:\n",
    "epochs = 5\n",
    "train_loader, val_loader = get_data_loaders(train_batch_size, val_batch_size)\n",
    "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
    "\n",
    "# optimizer, I've used Adadelta, as it wokrs well without any magic numbers\n",
    "optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "\n",
    "losses = []\n",
    "batches = len(train_loader)\n",
    "val_batches = len(val_loader)\n",
    "\n",
    "# loop for every epoch (training + evaluation)\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    # progress bar (works in Jupyter notebook too!)\n",
    "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
    "\n",
    "    # ----------------- TRAINING  -------------------- \n",
    "    # set model to training\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in progress:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        # training step for single batch\n",
    "        model.zero_grad() # to make sure that all the grads are 0 \n",
    "        \"\"\"\n",
    "        model.zero_grad() and optimizer.zero_grad() are the same \n",
    "        IF all your model parameters are in that optimizer. \n",
    "        I found it is safer to call model.zero_grad() to make sure all grads are zero, \n",
    "        e.g. if you have two or more optimizers for one model.\n",
    "\n",
    "        \"\"\"\n",
    "        outputs = model(X) # forward\n",
    "        loss = loss_function(outputs, y) # get loss\n",
    "        loss.backward() # accumulates the gradient (by addition) for each parameter.\n",
    "        optimizer.step() # performs a parameter update based on the current gradient \n",
    "\n",
    "        # getting training quality data\n",
    "        current_loss = loss.item()\n",
    "        total_loss += current_loss\n",
    "\n",
    "        # updating progress bar\n",
    "        progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "        \n",
    "    # releasing unceseccary memory in GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # ----------------- VALIDATION  ----------------- \n",
    "    val_losses = 0\n",
    "    precision, recall, f1, accuracy = [], [], [], []\n",
    "    \n",
    "    # set model to evaluating (testing)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            outputs = model(X) # this gets the prediction from the network\n",
    "\n",
    "            val_losses += loss_function(outputs, y)\n",
    "\n",
    "            predicted_classes = torch.max(outputs, 1)[1] # get class from network's prediction\n",
    "            \n",
    "            # calculate P/R/F1/A metrics for batch\n",
    "            # for acc, metric in zip((precision, recall, f1, accuracy), \n",
    "            #                        (precision_score, recall_score, f1_score, accuracy_score)):\n",
    "            #     acc.append(\n",
    "            #         calculate_metric(metric, y.cpu(), predicted_classes.cpu())\n",
    "            #     )\n",
    "            precision.append(precision_score(y.cpu(), predicted_classes.cpu(), average='macro'))\n",
    "            recall.append(recall_score(y.cpu(), predicted_classes.cpu(), average='macro'))\n",
    "            f1.append(f1_score(y.cpu(), predicted_classes.cpu(), average='macro'))\n",
    "            accuracy.append(accuracy_score(y.cpu(), predicted_classes.cpu()))\n",
    "          \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
    "    print_scores(precision, recall, f1, accuracy, val_batches)\n",
    "    losses.append(total_loss/batches) # for plotting learning curve\n",
    "print(f\"Training time: {time.time()-start_ts}s\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6672: 100%|██████████| 235/235 [01:15<00:00,  3.11it/s]\n",
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, training loss: 1.6672336679823856, validation loss: 1.5713893175125122\n",
      "\t     precision: 0.9151\n",
      "\t        recall: 0.8959\n",
      "\t            F1: 0.8795\n",
      "\t      accuracy: 0.8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4785: 100%|██████████| 235/235 [01:15<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, training loss: 1.4785164376522633, validation loss: 1.4768619537353516\n",
      "\t     precision: 0.9877\n",
      "\t        recall: 0.9869\n",
      "\t            F1: 0.9868\n",
      "\t      accuracy: 0.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4724: 100%|██████████| 235/235 [01:16<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, training loss: 1.4724035293497937, validation loss: 1.4933072328567505\n",
      "\t     precision: 0.9747\n",
      "\t        recall: 0.9712\n",
      "\t            F1: 0.9711\n",
      "\t      accuracy: 0.9716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4695: 100%|██████████| 235/235 [01:16<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, training loss: 1.4695461729739574, validation loss: 1.4758371114730835\n",
      "\t     precision: 0.9869\n",
      "\t        recall: 0.9865\n",
      "\t            F1: 0.9861\n",
      "\t      accuracy: 0.9867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4680: 100%|██████████| 235/235 [01:17<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, training loss: 1.467963846186374, validation loss: 1.5489450693130493\n",
      "\t     precision: 0.9342\n",
      "\t        recall: 0.9153\n",
      "\t            F1: 0.9122\n",
      "\t      accuracy: 0.9180\n",
      "Training time: 410.2328402996063s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> ### 1.4 save model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:43.609565Z",
     "start_time": "2024-10-07T14:59:43.579402Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'mnist_state.pt')",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 2. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:43.674732Z",
     "start_time": "2024-10-07T14:59:43.620059Z"
    }
   },
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_set = dset.MNIST(root='./data', train=True, download=True)\n",
    "test_set = dset.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Extracting x_train, y_train, x_test, y_test from the datasets\n",
    "x_train = train_set.data.unsqueeze(1).float() / 255.0  # Normalize to [0, 1] range\n",
    "y_train = train_set.targets\n",
    "\n",
    "x_test = test_set.data.unsqueeze(1).float() / 255.0  # Normalize to [0, 1] range\n",
    "y_test = test_set.targets\n",
    "\n",
    "# Convert to numpy arrays if needed (optional)\n",
    "x_train_np = x_train.numpy()\n",
    "y_train_np = y_train.numpy()\n",
    "x_test_np = x_test.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "# Print shapes to verify\n",
    "print('x_train shape:', x_train.shape)  # Should be [60000, 1, 28, 28]\n",
    "print('y_train shape:', y_train.shape)  # Should be [60000]\n",
    "print('x_test shape:', x_test.shape)    # Should be [10000, 1, 28, 28]\n",
    "print('y_test shape:', y_test.shape)    # Should be [10000]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([60000, 1, 28, 28])\n",
      "y_train shape: torch.Size([60000])\n",
      "x_test shape: torch.Size([10000, 1, 28, 28])\n",
      "y_test shape: torch.Size([10000])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:43.689978Z",
     "start_time": "2024-10-07T14:59:43.687431Z"
    }
   },
   "source": [
    "x_train.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:43.740724Z",
     "start_time": "2024-10-07T14:59:43.737801Z"
    }
   },
   "source": [
    "x_train = x_train[:30001]\n",
    "y_train = y_train[:30001]\n",
    "x_test = x_test[:9000]\n",
    "y_test = y_test[:9000]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:43.791972Z",
     "start_time": "2024-10-07T14:59:43.787237Z"
    }
   },
   "source": [
    "# # Making sure that the values are float so that we can get decimal points after division\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# \n",
    "# # Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([30001, 1, 28, 28])\n",
      "Number of images in x_train 30001\n",
      "Number of images in x_test 9000\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.2 Create tuple (index, label) for train and test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:43.930524Z",
     "start_time": "2024-10-07T14:59:43.845260Z"
    }
   },
   "source": [
    "instance_index_label = [(i, y_train[i]) for i in range(x_train.shape[0])]\n",
    "instance_index_label_test = [(i, y_test[i]) for i in range(x_test.shape[0])]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:44.000833Z",
     "start_time": "2024-10-07T14:59:43.947134Z"
    }
   },
   "source": [
    "# find the index if label is 1\n",
    "find_index = [instance_index_label[i][0] for i in range(len(instance_index_label)) if instance_index_label[i][1]==1]\n",
    "# find the index if label is 1\n",
    "find_index_test = [instance_index_label_test[i][0] for i in range(len(instance_index_label_test))\n",
    "                   if instance_index_label_test[i][1]==1]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:44.019218Z",
     "start_time": "2024-10-07T14:59:44.017599Z"
    }
   },
   "source": [
    "print('index:', instance_index_label[0][0]) #index\n",
    "print('label:', instance_index_label[0][1]) #label"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "label: tensor(5)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.3 load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:44.058487Z",
     "start_time": "2024-10-07T14:59:44.056295Z"
    }
   },
   "source": [
    "import torch\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "class MnistResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super(MnistResNet, self).__init__(BasicBlock, [2, 2, 2, 2], num_classes=10)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.softmax(super(MnistResNet, self).forward(x), dim=-1)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:44.195286Z",
     "start_time": "2024-10-07T14:59:44.103172Z"
    }
   },
   "source": [
    "model = MnistResNet()\n",
    "model.load_state_dict(torch.load('mnist_state.pt'))\n",
    "body = nn.Sequential(*list(model.children()))\n",
    "# extract the last layer\n",
    "model = body[:9]\n",
    "# the model we will use\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_216647/2671729891.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('mnist_state.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.4 get features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:44.286791Z",
     "start_time": "2024-10-07T14:59:44.211539Z"
    }
   },
   "source": [
    "train_batch_size = 1\n",
    "val_batch_size = 1\n",
    "train_loader, val_loader = get_data_loaders(train_batch_size, val_batch_size)\n",
    "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adadelta(model.parameters())"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T14:59:44.305302Z",
     "start_time": "2024-10-07T14:59:44.303772Z"
    }
   },
   "source": [
    "losses = []\n",
    "batches = len(train_loader)\n",
    "val_batches = len(val_loader)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2.4.1 get features for train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:04:55.147875Z",
     "start_time": "2024-10-07T14:59:44.348332Z"
    }
   },
   "source": [
    "# loop for every epoch (training + evaluation)\n",
    "meta_table = dict()\n",
    "feature_result = []\n",
    "\n",
    "# progress bar\n",
    "progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, data in progress:\n",
    "    if i==30001:\n",
    "        break\n",
    "    X, y = data[0], data[1]\n",
    "    # training step for single batch\n",
    "    model.zero_grad()\n",
    "    outputs = model(X)\n",
    "    feature_result.append(outputs.reshape(-1).tolist())\n",
    "    meta_table[i] = outputs.reshape(-1).tolist()\n",
    "    \n",
    "feature_array = np.array(feature_result)\n",
    "np.save('feature_array_full',feature_array )\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  50%|█████     | 30001/60000 [05:10<05:10, 96.63it/s] \n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:05:15.104879Z",
     "start_time": "2024-10-07T15:05:15.071220Z"
    }
   },
   "source": [
    "# load\n",
    "feature_array = np.load('feature_array_full.npy', allow_pickle=True)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2.4.2 get features for test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:03.417179Z",
     "start_time": "2024-10-07T15:05:27.549056Z"
    }
   },
   "source": [
    "# loop for every epoch (training + evaluation)\n",
    "meta_t_table = dict()\n",
    "feature_t_result = []\n",
    "\n",
    "# progress bar\n",
    "progress = tqdm(enumerate(val_loader), desc=\"Loss: \", total=batches)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, data in progress:\n",
    "    if i==9000:\n",
    "        break\n",
    "    X, y = data[0], data[1]\n",
    "    # training step for single batch\n",
    "    model.zero_grad()\n",
    "    outputs_t = model(X)\n",
    "    feature_t_result.append(outputs_t.reshape(-1).tolist())\n",
    "    meta_t_table[i] = outputs_t.reshape(-1).tolist()\n",
    "\n",
    "feature_test_array = np.array(feature_t_result)\n",
    "# save \n",
    "np.save('feature_test_array_full',feature_test_array )\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:  15%|█▌        | 9000/60000 [01:35<09:02, 93.99it/s] \n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:14.317205Z",
     "start_time": "2024-10-07T15:07:14.305463Z"
    }
   },
   "source": [
    "#load\n",
    "feature_test_array = np.load('feature_test_array_full.npy', allow_pickle=True)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.5 generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2.5.1 generate data for train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:17.503348Z",
     "start_time": "2024-10-07T15:07:17.499135Z"
    }
   },
   "source": [
    "from typing import List, Dict, Tuple\n",
    "def data_generation(instance_index_label: List[Tuple]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    bags: {key1: [ind1, ind2, ind3],\n",
    "           key2: [ind1, ind2, ind3, ind4, ind5],\n",
    "           ... }\n",
    "    bag_lbls:\n",
    "        {key1: 0,\n",
    "         key2: 1,\n",
    "         ... }\n",
    "    \"\"\"\n",
    "    bag_size = np.random.randint(3,7,size=len(instance_index_label)//5)\n",
    "    data_cp = copy.copy(instance_index_label)\n",
    "    np.random.shuffle(data_cp)\n",
    "    bags = {}\n",
    "    bags_per_instance_labels = {}\n",
    "    bags_labels = {}\n",
    "    for bag_ind, size in enumerate(bag_size):\n",
    "        bags[bag_ind] = []\n",
    "        bags_per_instance_labels[bag_ind] = []\n",
    "        try:\n",
    "            for _ in range(size):\n",
    "                inst_ind, lbl = data_cp.pop()\n",
    "                bags[bag_ind].append(inst_ind)\n",
    "                # simplfy, just use a temporary variable instead of bags_per_instance_labels\n",
    "                bags_per_instance_labels[bag_ind].append(lbl)\n",
    "            bags_labels[bag_ind] = bag_label_from_instance_labels(bags_per_instance_labels[bag_ind])\n",
    "        except:\n",
    "            break\n",
    "    return bags, bags_labels\n",
    "\n",
    "def bag_label_from_instance_labels(instance_labels):\n",
    "    return int(any(((x==1) for x in instance_labels)))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:20.398325Z",
     "start_time": "2024-10-07T15:07:20.311428Z"
    }
   },
   "source": [
    "bag_indices, bag_labels = data_generation(instance_index_label)\n",
    "bag_features = {kk: torch.Tensor(feature_array[inds]) for kk, inds in bag_indices.items()}"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:22.343401Z",
     "start_time": "2024-10-07T15:07:22.203053Z"
    }
   },
   "source": [
    "# save\n",
    "import pickle\n",
    "pickle.dump(bag_indices, open( \"bag_indices\", \"wb\" ) )\n",
    "pickle.dump(bag_labels, open( \"bag_labels\", \"wb\" ) )\n",
    "pickle.dump(bag_features, open( \"bag_features\", \"wb\" ) )"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:24.100235Z",
     "start_time": "2024-10-07T15:07:23.917265Z"
    }
   },
   "source": [
    "import pickle\n",
    "bag_indices = pickle.load( open( \"bag_indices\", \"rb\" ) )\n",
    "bag_labels = pickle.load( open( \"bag_labels\", \"rb\" ) )\n",
    "bag_features = pickle.load( open( \"bag_features\", \"rb\" ) )"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 2.5.2 generate data for test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:25.801483Z",
     "start_time": "2024-10-07T15:07:25.779943Z"
    }
   },
   "source": [
    "bag_t_indices, bag_t_labels = data_generation(instance_index_label_test)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:27.217375Z",
     "start_time": "2024-10-07T15:07:27.197584Z"
    }
   },
   "source": [
    "bag_t_features = {kk: torch.Tensor(feature_test_array[inds]) for kk, inds in bag_t_indices.items()}"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:28.692408Z",
     "start_time": "2024-10-07T15:07:28.414646Z"
    }
   },
   "source": [
    "pickle.dump(bag_t_indices, open( \"bag_t_indices\", \"wb\" ) )\n",
    "pickle.dump(bag_t_labels, open( \"bag_t_labels\", \"wb\" ) )\n",
    "pickle.dump(bag_t_features, open( \"bag_t_features\", \"wb\" ) )"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:30.056691Z",
     "start_time": "2024-10-07T15:07:29.992350Z"
    }
   },
   "source": [
    "bag_t_indices = pickle.load( open( \"bag_t_indices\", \"rb\" ) )\n",
    "bag_t_labels = pickle.load( open( \"bag_t_labels\", \"rb\" ) )\n",
    "bag_t_features = pickle.load( open( \"bag_t_features\", \"rb\" ) )"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 3. Multiple Instance Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.1 Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:32.735180Z",
     "start_time": "2024-10-07T15:07:32.731616Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "class Transform_data(Dataset):\n",
    "    \"\"\"\n",
    "    We want to 1. pad tensor 2. transform the data to the size that fits in the input size.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        tensor = self.data[index][0]\n",
    "        if self.transform is not None:\n",
    "            tensor = self.transform(tensor)\n",
    "        return (tensor, self.data[index][1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:34.550657Z",
     "start_time": "2024-10-07T15:07:34.547494Z"
    }
   },
   "source": [
    "train_data = [(bag_features[i],bag_labels[i]) for i in range(len(bag_features))]"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:36.397169Z",
     "start_time": "2024-10-07T15:07:36.387588Z"
    }
   },
   "source": [
    "bag_features[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2113, 0.1604, 0.3617,  ..., 0.4698, 1.0727, 0.2668],\n",
       "        [2.1209, 1.0954, 0.6056,  ..., 0.8730, 1.1708, 1.2247],\n",
       "        [0.3593, 0.8008, 1.1112,  ..., 0.8670, 0.8385, 0.1196],\n",
       "        [0.4573, 0.4233, 0.4793,  ..., 0.6923, 0.9962, 0.9839],\n",
       "        [0.0913, 1.6382, 0.6698,  ..., 0.9004, 0.6057, 0.2232]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:44.046330Z",
     "start_time": "2024-10-07T15:07:44.039480Z"
    }
   },
   "source": [
    "def pad_tensor(data:list, max_number_instance) -> list:\n",
    "    \"\"\"\n",
    "    Since our bag has different sizes, we need to pad each tensor to have the same shape (max: 7).\n",
    "    We will look through each one instance and look at the shape of the tensor, and then we will pad 7-n \n",
    "    to the existing tensor where n is the number of instances in the bag.\n",
    "    The function will return a padded data set.\"\"\"\n",
    "    new_data = []\n",
    "    for bag_index in range(len(data)):\n",
    "        tensor_size = len(data[bag_index][0])\n",
    "        pad_size = max_number_instance - tensor_size\n",
    "        p2d = (0,0, 0, pad_size)\n",
    "        padded = nn.functional.pad(data[bag_index][0], p2d, 'constant', 0)\n",
    "        new_data.append((padded, data[bag_index][1]))\n",
    "    return new_data"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:46.228700Z",
     "start_time": "2024-10-07T15:07:46.193949Z"
    }
   },
   "source": [
    "max_number_instance = 7\n",
    "padded_train = pad_tensor(train_data, max_number_instance)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:48.680461Z",
     "start_time": "2024-10-07T15:07:48.664470Z"
    }
   },
   "source": [
    "test_data = [(bag_t_features[i],bag_t_labels[i]) for i in range(len(bag_t_features))]\n",
    "padded_test = pad_tensor(test_data, max_number_instance)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:50.008224Z",
     "start_time": "2024-10-07T15:07:50.003417Z"
    }
   },
   "source": [
    "def get_data_loaders(train_data, test_data, train_batch_size, val_batch_size):\n",
    "    train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_data, batch_size=val_batch_size, shuffle=False)\n",
    "    return train_loader, val_loader"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:51.327750Z",
     "start_time": "2024-10-07T15:07:51.320251Z"
    }
   },
   "source": [
    "train_loader,valid_loader = get_data_loaders(padded_train, padded_test, 1, 1)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:52.566158Z",
     "start_time": "2024-10-07T15:07:52.562729Z"
    }
   },
   "source": [
    "train_batch_size = 1\n",
    "val_batch_size = 1"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.2 Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.2.1 Aggregation Funtion model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:07:55.288339Z",
     "start_time": "2024-10-07T15:07:55.283009Z"
    }
   },
   "source": [
    "# aggregation functions\n",
    "class SoftMaxMeanSimple(torch.nn.Module):\n",
    "    def __init__(self, n, n_inst, dim=0):\n",
    "        \"\"\"\n",
    "        if dim==1:\n",
    "            given a tensor `x` with dimensions [N * M],\n",
    "            where M -- dimensionality of the featur vector\n",
    "                       (number of features per instance)\n",
    "                  N -- number of instances\n",
    "            initialize with `AggModule(M)`\n",
    "            returns:\n",
    "            - weighted result: [M]\n",
    "            - gate: [N]\n",
    "        if dim==0:\n",
    "            ...\n",
    "        \"\"\"\n",
    "        super(SoftMaxMeanSimple, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.gate = torch.nn.Softmax(dim=self.dim)      \n",
    "        self.mdl_instance_transform = nn.Sequential(\n",
    "                            nn.Linear(n, n_inst),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Linear(n_inst, n),\n",
    "                            nn.LeakyReLU(),\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        z = self.mdl_instance_transform(x)\n",
    "        if self.dim==0:\n",
    "            z = z.view((z.shape[0],1)).sum(1)\n",
    "        elif self.dim==1:\n",
    "            z = z.view((1, z.shape[1])).sum(0)\n",
    "        gate_ = self.gate(z)\n",
    "        res = torch.sum(x* gate_, self.dim)\n",
    "        return res, gate_\n",
    "\n",
    "    \n",
    "class AttentionSoftMax(torch.nn.Module):\n",
    "    def __init__(self, in_features = 3, out_features = None):\n",
    "        \"\"\"\n",
    "        given a tensor `x` with dimensions [N * M],\n",
    "        where M -- dimensionality of the feature vector\n",
    "                   (number of features per instance)\n",
    "              N -- number of instances\n",
    "        initialize with `AggModule(M)`\n",
    "        returns:\n",
    "        - weighted result: [M]\n",
    "        - gate: [N]\n",
    "        \"\"\"\n",
    "        super(AttentionSoftMax, self).__init__()\n",
    "        self.otherdim = ''\n",
    "        if out_features is None:\n",
    "            out_features = in_features\n",
    "        self.layer_linear_tr = nn.Linear(in_features, out_features)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.layer_linear_query = nn.Linear(out_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.layer_linear_tr(x)\n",
    "        keys = self.activation(keys)\n",
    "        attention_map_raw = self.layer_linear_query(keys)[...,0]\n",
    "        attention_map = nn.Softmax(dim=-1)(attention_map_raw)\n",
    "        result = torch.einsum(f'{self.otherdim}i,{self.otherdim}ij->{self.otherdim}j', attention_map, x)\n",
    "        return result, attention_map\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.2 MIL_NN model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:28:39.264553Z",
     "start_time": "2024-10-07T15:28:39.254422Z"
    }
   },
   "source": [
    "class NoisyAnd(torch.nn.Module):\n",
    "    def __init__(self, a=10, dims=[1,2]):\n",
    "        super(NoisyAnd, self).__init__()\n",
    "#         self.output_dim = output_dim\n",
    "        self.a = a\n",
    "        self.b = torch.nn.Parameter(torch.tensor(0.01))\n",
    "        self.dims =dims\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "#         h_relu = self.linear1(x).clamp(min=0)\n",
    "        mean = torch.mean(x, self.dims, True)\n",
    "        res = (self.sigmoid(self.a * (mean - self.b)) - self.sigmoid(-self.a * self.b)) / (\n",
    "              self.sigmoid(self.a * (1 - self.b)) - self.sigmoid(-self.a * self.b))\n",
    "        return res\n",
    "    \n",
    "\n",
    "\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, n=512, n_mid = 1024,\n",
    "                 n_out=1, dropout=0.2,\n",
    "                 scoring = None,\n",
    "                ):\n",
    "        super(NN, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(n, n_mid)\n",
    "        self.non_linearity = torch.nn.LeakyReLU()\n",
    "        self.linear2 = torch.nn.Linear(n_mid, n_out)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        if scoring:\n",
    "            self.scoring = scoring\n",
    "        else:\n",
    "            self.scoring = torch.nn.Softmax() if n_out>1 else torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.linear1(x)\n",
    "        z = self.non_linearity(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.linear2(z)\n",
    "        y_pred = self.scoring(z)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, n=512, n_out=1):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n, n_out)\n",
    "        self.scoring = torch.nn.Softmax() if n_out>1 else torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.linear(x)\n",
    "        y_pred = self.scoring(z)\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "def regularization_loss(params,\n",
    "                        reg_factor = 0.005,\n",
    "                        reg_alpha = 0.5):\n",
    "    params = [pp for pp in params if len(pp.shape)>1]\n",
    "    l1_reg = nn.L1Loss()\n",
    "    l2_reg = nn.MSELoss()\n",
    "    loss_reg =0\n",
    "    for pp in params:\n",
    "        loss_reg+=reg_factor*((1-reg_alpha)*l1_reg(pp, target=torch.zeros_like(pp)) +\\\n",
    "                           reg_alpha*l2_reg(pp, target=torch.zeros_like(pp)))\n",
    "    return loss_reg\n",
    "\n",
    "class MIL_NN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n=512,  \n",
    "                 n_mid=1024, \n",
    "                 n_classes=1, \n",
    "                 dropout=0.1,\n",
    "                 agg = None,\n",
    "                 scoring=None,\n",
    "                ):\n",
    "        super(MIL_NN, self).__init__()\n",
    "        self.agg = agg if agg is not None else AttentionSoftMax(n)\n",
    "        \n",
    "        if n_mid == 0:\n",
    "            self.bag_model = LogisticRegression(n, n_classes)\n",
    "        else:\n",
    "            self.bag_model = NN(n, n_mid, n_classes, dropout=dropout, scoring=scoring)\n",
    "        \n",
    "    def forward(self, bag_features, bag_lbls=None):\n",
    "        \"\"\"\n",
    "        bag_feature is an aggregated vector of 512 features\n",
    "        bag_att is a gate vector of n_inst instances\n",
    "        bag_lbl is a vector a labels\n",
    "        figure out batches\n",
    "        \"\"\"\n",
    "        bag_feature, bag_att, bag_keys = list(zip(*[list(self.agg(ff.float())) + [idx]\n",
    "                                                    for idx, ff in (bag_features.items())]))\n",
    "        bag_att = dict(zip(bag_keys, [a.detach().cpu() for a  in bag_att]))\n",
    "        bag_feature_stacked = torch.stack(bag_feature)\n",
    "        y_pred = self.bag_model(bag_feature_stacked)\n",
    "        return y_pred, bag_att, bag_keys"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 3.3.3 helper function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:28:41.661675Z",
     "start_time": "2024-10-07T15:28:41.655341Z"
    }
   },
   "source": [
    "def calculate_metric(metric_fn, true_y, pred_y):\n",
    "    # multi class problems need to have averaging method\n",
    "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
    "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
    "    else:\n",
    "        return metric_fn(true_y, pred_y)\n",
    "    \n",
    "def print_scores(p, r, f1, a, batch_size):\n",
    "    # just utility printing function\n",
    "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
    "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 4. Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T15:28:43.569693Z",
     "start_time": "2024-10-07T15:28:43.527185Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "start_ts = time.time()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr0 = 1e-4\n",
    "\n",
    "# model:\n",
    "model = MIL_NN().to(device)\n",
    "\n",
    "# params you need to specify:\n",
    "epochs = 10\n",
    "train_loader, val_loader = get_data_loaders(padded_train, padded_test, 1, 1)\n",
    "loss_function = torch.nn.BCELoss(reduction='mean') # your loss function, cross entropy works well for multi-class problems\n",
    "\n",
    "\n",
    "#optimizer = optim.Adadelta(model.parameters())\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr0, momentum=0.9)\n",
    "\n",
    "losses = []\n",
    "batches = len(train_loader)\n",
    "val_batches = len(val_loader)\n",
    "\n",
    "# loop for every epoch (training + evaluation)\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    # progress bar (works in Jupyter notebook too!)\n",
    "    progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
    "\n",
    "    # ----------------- TRAINING  -------------------- \n",
    "    # set model to training\n",
    "    model.train()\n",
    "    for i, data in progress:\n",
    "        X, y = data[0].to(device), data[1].to(device)\n",
    "        X = X.reshape([1,7*512])\n",
    "        # X = X.reshape(-1, 512)\n",
    "        print(f'X shape: {X.shape}')\n",
    "        y = y.type(torch.cuda.FloatTensor)\n",
    "        # training step for single batch\n",
    "        model.zero_grad() # to make sure that all the grads are 0 \n",
    "        \"\"\"\n",
    "        model.zero_grad() and optimizer.zero_grad() are the same \n",
    "        IF all your model parameters are in that optimizer. \n",
    "        I found it is safer to call model.zero_grad() to make sure all grads are zero, \n",
    "        e.g. if you have two or more optimizers for one model.\n",
    "\n",
    "        \"\"\"\n",
    "        outputs = model(X) # forward\n",
    "        loss = loss_function(outputs, y) # get loss\n",
    "        loss.backward() # accumulates the gradient (by addition) for each parameter.\n",
    "        optimizer.step() # performs a parameter update based on the current gradient \n",
    "\n",
    "        # getting training quality data\n",
    "        current_loss = loss.item()\n",
    "        total_loss += current_loss\n",
    "\n",
    "        # updating progress bar\n",
    "        progress.set_description(\"Loss: {:.4f}\".format(total_loss/(i+1)))\n",
    "        \n",
    "    # releasing unceseccary memory in GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # ----------------- VALIDATION  ----------------- \n",
    "    val_losses = 0\n",
    "    precision, recall, f1, accuracy = [], [], [], []\n",
    "    \n",
    "    # set model to evaluating (testing)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            X, y = data[0].to(device), data[1].to(device)\n",
    "            X = X.reshape([1,7*512])\n",
    "            y = y.type(torch.cuda.FloatTensor)\n",
    "            outputs = model(X) # this gets the prediction from the network\n",
    "            prediced_classes =outputs.detach().round()\n",
    "            #y_pred.extend(prediced_classes.tolist())\n",
    "            val_losses += loss_function(outputs, y)\n",
    "            \n",
    "            # # calculate P/R/F1/A metrics for batch\n",
    "            # for acc, metric in zip((precision, recall, f1, accuracy), \n",
    "            #                        (precision_score, recall_score, f1_score, accuracy_score)):\n",
    "            #     acc.append(\n",
    "            #         calculate_metric(metric, y.cpu(), prediced_classes.cpu())\n",
    "            #     )\n",
    "            precision.append(precision_score(y.cpu(), predicted_classes.cpu(), average='macro'))\n",
    "            recall.append(recall_score(y.cpu(), predicted_classes.cpu(), average='macro'))\n",
    "            f1.append(f1_score(y.cpu(), predicted_classes.cpu(), average='macro'))\n",
    "            accuracy.append(accuracy_score(y.cpu(), predicted_classes.cpu()))\n",
    "          \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
    "    print_scores(precision, recall, f1, accuracy, val_batches)\n",
    "    losses.append(total_loss/batches) # for plotting learning curve\n",
    "print(f\"Training time: {time.time()-start_ts}s\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:   0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1, 3584])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[62], line 48\u001B[0m\n\u001B[1;32m     40\u001B[0m model\u001B[38;5;241m.\u001B[39mzero_grad() \u001B[38;5;66;03m# to make sure that all the grads are 0 \u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03mmodel.zero_grad() and optimizer.zero_grad() are the same \u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;03mIF all your model parameters are in that optimizer. \u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     46\u001B[0m \n\u001B[1;32m     47\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 48\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# forward\u001B[39;00m\n\u001B[1;32m     49\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(outputs, y) \u001B[38;5;66;03m# get loss\u001B[39;00m\n\u001B[1;32m     50\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward() \u001B[38;5;66;03m# accumulates the gradient (by addition) for each parameter.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[60], line 92\u001B[0m, in \u001B[0;36mMIL_NN.forward\u001B[0;34m(self, bag_features, bag_lbls)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, bag_features, bag_lbls\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     85\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;124;03m    bag_feature is an aggregated vector of 512 features\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;124;03m    bag_att is a gate vector of n_inst instances\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;124;03m    bag_lbl is a vector a labels\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;124;03m    figure out batches\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     91\u001B[0m     bag_feature, bag_att, bag_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m[\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magg(ff\u001B[38;5;241m.\u001B[39mfloat())) \u001B[38;5;241m+\u001B[39m [idx]\n\u001B[0;32m---> 92\u001B[0m                                                 \u001B[38;5;28;01mfor\u001B[39;00m idx, ff \u001B[38;5;129;01min\u001B[39;00m (\u001B[43mbag_features\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m())]))\n\u001B[1;32m     93\u001B[0m     bag_att \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(bag_keys, [a\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m a  \u001B[38;5;129;01min\u001B[39;00m bag_att]))\n\u001B[1;32m     94\u001B[0m     bag_feature_stacked \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(bag_feature)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'items'"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
