{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10551862,"sourceType":"datasetVersion","datasetId":6395200}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Lib","metadata":{}},{"cell_type":"code","source":"!pip install gpytorch\n!pip install torchsummary\n!pip install iterative-stratification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:12.034806Z","iopub.execute_input":"2025-01-22T19:12:12.035216Z","iopub.status.idle":"2025-01-22T19:12:23.761437Z","shell.execute_reply.started":"2025-01-22T19:12:12.035178Z","shell.execute_reply":"2025-01-22T19:12:23.760554Z"}},"outputs":[{"name":"stdout","text":"Collecting gpytorch\n  Downloading gpytorch-1.13-py3-none-any.whl.metadata (8.0 kB)\nCollecting jaxtyping==0.2.19 (from gpytorch)\n  Downloading jaxtyping-0.2.19-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.2.2)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.13.1)\nCollecting linear-operator>=0.5.3 (from gpytorch)\n  Downloading linear_operator-0.5.3-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\nRequirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping==0.2.19->gpytorch) (4.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\nRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.3->gpytorch) (2.5.1+cu121)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.16.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->jaxtyping==0.2.19->gpytorch) (2024.2.0)\nDownloading gpytorch-1.13-py3-none-any.whl (277 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.8/277.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jaxtyping-0.2.19-py3-none-any.whl (24 kB)\nDownloading linear_operator-0.5.3-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jaxtyping, linear-operator, gpytorch\nSuccessfully installed gpytorch-1.13 jaxtyping-0.2.19 linear-operator-0.5.3\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\nCollecting iterative-stratification\n  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from iterative-stratification) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->iterative-stratification) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->iterative-stratification) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->iterative-stratification) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->iterative-stratification) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->iterative-stratification) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->iterative-stratification) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->iterative-stratification) (2024.2.0)\nDownloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\nInstalling collected packages: iterative-stratification\nSuccessfully installed iterative-stratification-0.1.9\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os.path\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport pandas as pd\nimport random\nimport numpy as np\nimport cv2\nimport pydicom\nfrom skimage.transform import resize\nfrom sklearn.model_selection import train_test_split\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom torch.nn import functional as F\nimport gpytorch\nfrom gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel\nfrom torchsummary import summary\n\n# Device configuration\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f'Device: {device}')\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\nelse:\n    print(\"No GPU available. Training will run on CPU.\")\n\ndef seed_everything(seed=39):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()\n\nimport warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:23.762791Z","iopub.execute_input":"2025-01-22T19:12:23.763010Z","iopub.status.idle":"2025-01-22T19:12:32.877031Z","shell.execute_reply.started":"2025-01-22T19:12:23.762990Z","shell.execute_reply":"2025-01-22T19:12:32.876164Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nGPU: Tesla P100-PCIE-16GB is available.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Kaggle and local switch\nKAGGLE = os.path.exists('/kaggle')\nHPC = os.path.exists('/media02/tdhoang01')\n\nif KAGGLE:\n    print(\"Running on Kaggle\")\n    ROOT_DIR = '/kaggle/input/'\n    DATA_DIR = ROOT_DIR + 'rsna-ich-mil/'\n    DICOM_DIR = DATA_DIR + 'rsna-ich-mil/'\n    CSV_PATH = DICOM_DIR + 'training_dataset_2_redundancy_1150_for_kaggle.csv'\nelif HPC:\n    print(\"Running on HPC\")\n    DATA_DIR = '/media02/tdhoang01/21127112-21127734/data/'\n    DICOM_DIR = DATA_DIR + 'rsna-ich-mil/'\n    CSV_PATH = DATA_DIR + 'training_dataset_1150_redundancy.csv'\nelse: # Local\n    print(\"Running locally\")\n    DATA_DIR = '../rsna-mil-training/'\n    DICOM_DIR = DATA_DIR + 'rsna-mil-training/'\n    CSV_PATH = DATA_DIR + 'training_1000_scan_subset.csv'\n\ndicom_dir = DICOM_DIR if KAGGLE or HPC else DATA_DIR\n# Load patient scan labels\npatient_scan_labels = pd.read_csv(CSV_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:32.878452Z","iopub.execute_input":"2025-01-22T19:12:32.878860Z","iopub.status.idle":"2025-01-22T19:12:32.930202Z","shell.execute_reply.started":"2025-01-22T19:12:32.878837Z","shell.execute_reply":"2025-01-22T19:12:32.929380Z"}},"outputs":[{"name":"stdout","text":"Running on Kaggle\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Accessing constants from config\nHEIGHT = 224\nWIDTH = 224\nCHANNELS = 1\n\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 4\nTEST_BATCH_SIZE = 2\nTEST_SIZE = 0.15\nVALID_SIZE = 0.25\n\nTRAINING_TYPE = 'end_to_end'\nGP_MODEL = 'multi_task' # or 'single_task'\nGP_KERNEL = 'rbf'\n\nMAX_SLICES = 28\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nNUM_EPOCHS = 50\nLEARNING_RATE = 1e-4\nINDUCING_POINTS = 50\nTHRESHOLD = 0.5\n\nNUM_CLASSES = 6\n\nTARGET_LABELS = 'intraparenchymal'\n\nMODEL_PATH = 'results/trained_model.pth'\nDEVICE = 'cuda'\n\nPROJECTION_LOCATION = 'after_gp'\nPROJECTION_HIDDEN_DIM = 256\nPROJECTION_OUTPUT_DIM = 128\n\nATTENTION_HIDDEN_DIM = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:32.931504Z","iopub.execute_input":"2025-01-22T19:12:32.931746Z","iopub.status.idle":"2025-01-22T19:12:32.936478Z","shell.execute_reply.started":"2025-01-22T19:12:32.931725Z","shell.execute_reply":"2025-01-22T19:12:32.935674Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Hyperparameters definition","metadata":{}},{"cell_type":"code","source":"# # Hyperparameters\nbatch_size = 64\nlearning_rate = 1e-4\nnum_epochs = 10\nimg_size = 224  # MNIST images are 28x28 pixels\nnum_classes = 6  # Digits from 0 to 9\npatch_size = 14  # Size of each patch (7x7)\nembedding_dim = 64  # Dimensionality of the embeddings\nnum_heads = 4  # Number of attention heads\nnum_layers = 6  # Number of transformer layers\ndropout_rate = 0.5  # Dropout rate for regularization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:32.937275Z","iopub.execute_input":"2025-01-22T19:12:32.937550Z","iopub.status.idle":"2025-01-22T19:12:32.953887Z","shell.execute_reply.started":"2025-01-22T19:12:32.937520Z","shell.execute_reply":"2025-01-22T19:12:32.953166Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Prepare the Dataset","metadata":{}},{"cell_type":"code","source":"def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n    # Extract the labels from the DataFrame\n    labels = patient_scan_labels['patient_label']\n    if test_size > 0:\n        # First, split off the test set\n        train_val_labels, test_labels = train_test_split(\n            patient_scan_labels,\n            test_size=test_size,\n            stratify=labels,\n            random_state=random_state\n        )\n\n        # Calculate the validation size relative to the train_val set\n        val_size_adjusted = val_size / (1 - test_size)\n\n        # Split the train_val set into train and validation sets\n        train_labels, val_labels = train_test_split(\n            train_val_labels,\n            test_size=val_size_adjusted,\n            stratify=train_val_labels['patient_label'],\n            random_state=random_state\n        )\n    else:\n        train_labels, val_labels = train_test_split(\n            patient_scan_labels,\n            test_size=val_size,\n            stratify=labels,\n            random_state=random_state\n        )\n        test_labels = None\n\n    return train_labels, val_labels, test_labels\n\ndef split_dataset_for_multilabel(patient_scan_labels, test_size=0.15, val_size=0.25, random_state=42):\n    # Extract the labels from the DataFrame\n    labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal',\n                                  'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']].values\n\n    if test_size != 0:\n        # First split: train + test\n        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n        train_idx, test_idx = next(msss.split(patient_scan_labels, labels))\n\n        train_labels = patient_scan_labels.iloc[train_idx]\n        test_labels = patient_scan_labels.iloc[test_idx]\n\n        # Second split: train + validation\n        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n        train_idx, val_idx = next(msss_val.split(train_labels, labels[train_idx]))\n\n        train_labels_final = train_labels.iloc[train_idx]\n        val_labels = train_labels.iloc[val_idx]\n\n    else:\n        # Only split into train and validation if test_size is 0\n        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n        train_idx, val_idx = next(msss_val.split(patient_scan_labels, labels))\n\n        train_labels_final = patient_scan_labels.iloc[train_idx]\n        val_labels = patient_scan_labels.iloc[val_idx]\n        test_labels = patient_scan_labels_test\n\n    return train_labels_final, val_labels, test_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:32.954724Z","iopub.execute_input":"2025-01-22T19:12:32.954925Z","iopub.status.idle":"2025-01-22T19:12:32.970051Z","shell.execute_reply.started":"2025-01-22T19:12:32.954908Z","shell.execute_reply":"2025-01-22T19:12:32.969389Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def correct_dcm(dcm):\n    x = dcm.pixel_array + 1000\n    px_mode = 4096\n    x[x>=px_mode] = x[x>=px_mode] - px_mode\n    dcm.PixelData = x.tobytes()\n    dcm.RescaleIntercept = -1000\n\ndef create_bone_mask(dcm):\n    # Assuming dcm.pixel_array contains the HU values\n    hu_values = dcm.pixel_array\n\n    # Create a mask for bone regions\n    # bone_mask = (hu_values >= 1000) & (hu_values <= 1200)\n    bone_mask = (hu_values >= 1000) & (hu_values <= 1200)\n    return bone_mask\n\ndef extract_bone_mask(dcm):\n    # Create the bone mask\n    bone_mask = create_bone_mask(dcm)\n\n    # Extract the bone mask from the image\n    hu_values = dcm.pixel_array.copy()\n    # hu_values[bone_mask] = 0\n    hu_values[~bone_mask] = 0\n\n    # Update the DICOM pixel data\n    dcm.PixelData = hu_values.tobytes()\n\ndef window_image(dcm, window_center, window_width):\n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n        correct_dcm(dcm)\n    # extract_bone_mask(dcm)\n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n\n    # Resize\n    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n    return img\n\ndef bsb_window(dcm):\n    brain_img = window_image(dcm, 40, 80)\n    subdural_img = window_image(dcm, 80, 200)\n    soft_img = window_image(dcm, 40, 380)\n\n    brain_img = (brain_img - 0) / 80\n    subdural_img = (subdural_img - (-20)) / 200\n    soft_img = (soft_img - (-150)) / 380\n\n    if CHANNELS == 3:\n        bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=-1)\n    else:\n        bsb_img = brain_img\n    return bsb_img.astype(np.float16)\n\ndef preprocess_slice(slice, target_size=(HEIGHT, WIDTH)):\n    # Check if type of slice is dicom or an empty numpy array\n    if type(slice) == np.ndarray:\n        slice = resize(slice, target_size, anti_aliasing=True)\n        multichannel_slice = np.stack([slice, slice, slice], axis=-1)\n        if CHANNELS == 3:\n            return multichannel_slice.astype(np.float16)\n        else:\n            return slice.astype(np.float16)\n    else:\n        slice = bsb_window(slice)\n        return slice.astype(np.float16)\n\ndef read_dicom_folder(folder_path, max_slices=MAX_SLICES):\n    # Filter and sort DICOM files directly based on ImagePositionPatient\n    dicom_files = sorted(\n        [f for f in os.listdir(folder_path) if f.endswith(\".dcm\")],\n        key=lambda f: float(pydicom.dcmread(os.path.join(folder_path, f)).ImagePositionPatient[2])\n    )[:max_slices]\n\n    # Read and store slices\n    slices = [pydicom.dcmread(os.path.join(folder_path, f)) for f in dicom_files]\n\n    # Pad with black images if necessary\n    if len(slices) < max_slices:\n        black_image = np.zeros_like(slices[0].pixel_array)\n        slices += [black_image] * (max_slices - len(slices))\n\n    return slices[:max_slices]\n\ndef process_patient_data(dicom_dir, row):\n    patient_id = row['patient_id'].replace('ID_', '')\n    study_instance_uid = row['study_instance_uid'].replace('ID_', '')\n\n    folder_name = f\"{patient_id}_{study_instance_uid}\"\n    folder_path = os.path.join(dicom_dir, folder_name)\n\n    if os.path.exists(folder_path):\n        try:\n            slices = read_dicom_folder(folder_path)\n\n            preprocessed_slices = [torch.tensor(preprocess_slice(slice), dtype=torch.float32) for slice in slices]  # Convert to tensor\n\n            # Stack preprocessed slices into an array\n            preprocessed_slices = torch.stack(preprocessed_slices, dim=0)  # (num_slices, height, width, channels)\n\n            # Labels are already in list form, so just convert them to a tensor\n            labels = torch.tensor(row['labels'], dtype=torch.long)\n\n            # Fill labels with 0s if necessary\n            if len(preprocessed_slices) > len(labels):\n                padded_labels = torch.zeros(len(preprocessed_slices), dtype=torch.long)\n                padded_labels[:len(labels)] = labels\n            else:\n                padded_labels = labels[:len(preprocessed_slices)]\n\n            return preprocessed_slices, padded_labels\n        except Exception as e:\n            print(f\"Error processing patient data: {e} ; dicom_dir: {dicom_dir}, folder_name: {folder_name}\")\n            return None, None\n\n\n    else:\n        print(f\"Folder not found: {folder_name}\")\n        return None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:32.970735Z","iopub.execute_input":"2025-01-22T19:12:32.970928Z","iopub.status.idle":"2025-01-22T19:12:32.989721Z","shell.execute_reply.started":"2025-01-22T19:12:32.970911Z","shell.execute_reply":"2025-01-22T19:12:32.988915Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Full Dataset\nclass MedicalScanDataset:\n    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n        self.data_dir = data_dir\n        self.dataset = self._parse_patient_scan_labels(patient_scan_labels)\n        self.augmentor = augmentor\n\n    def _parse_patient_scan_labels(self, patient_scan_labels):\n        \"\"\"Parse and validate patient scan labels.\"\"\"\n        patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n            lambda x: eval(x) if isinstance(x, str) else x\n        )\n        patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n            lambda x: eval(x) if isinstance(x, str) else x\n        )\n        # Convert multi-label columns from string representation to actual lists\n        multi_label_columns = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n\n        for column in multi_label_columns:\n            patient_scan_labels[column] = patient_scan_labels[column].apply(\n                lambda x: eval(x) if isinstance(x, str) else x\n            )\n\n        patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n        return patient_scan_labels\n\n    def _process_patient_data(self, row):\n        \"\"\"Process patient data to get preprocessed slices and labels.\"\"\"\n        return process_patient_data(self.data_dir, row)\n\n    def __len__(self):\n        return len(self.dataset) * (self.augmentor.levels if self.augmentor else 1)\n\n    def __getitem__(self, idx):\n        patient_idx = idx // (self.augmentor.levels if self.augmentor else 1)\n        aug_level = idx % (self.augmentor.levels if self.augmentor else 1)\n\n        row = self.dataset.iloc[patient_idx]\n        preprocessed_slices, labels = self._process_patient_data(row)\n\n        preprocessed_slices = self._prepare_tensor(preprocessed_slices, aug_level if self.augmentor else None)\n        patient_label = torch.tensor(bool(row['patient_label']), dtype=torch.uint8)\n\n        multi_class_labels = torch.tensor([0, 0, 0, 0, 0, 0], dtype=torch.uint8)\n        if any(row['any']):\n            multi_class_labels[0] = 1\n        if any(row['epidural']):\n            multi_class_labels[1] = 1\n        if any(row['intraparenchymal']):\n            multi_class_labels[2] = 1\n        if any(row['intraventricular']):\n            multi_class_labels[3] = 1\n        if any(row['subarachnoid']):\n            multi_class_labels[4] = 1\n        if any(row['subdural']):\n            multi_class_labels[5] = 1\n\n        return preprocessed_slices, labels, patient_label, multi_class_labels\n\n    def _prepare_tensor(self, preprocessed_slices, aug_level):\n        # Convert to numpy array and then to torch tensor\n        preprocessed_slices = np.asarray(preprocessed_slices, dtype=np.float32)\n        preprocessed_slices = torch.tensor(preprocessed_slices, dtype=torch.float32)\n\n        # Add an additional dimension for channel if it's missing (no augmentor)\n        if preprocessed_slices.ndim == 3:\n            preprocessed_slices = preprocessed_slices.unsqueeze(1)  # shape: [slices, 1, H, W]\n\n        # Apply augmentation if augmentor is specified\n        if self.augmentor and aug_level is not None:\n            if preprocessed_slices.ndim == 4:  # Ensure it has the [slices, channels, H, W] format\n                return torch.stack([self.augmentor.apply_transform(img, aug_level) for img in preprocessed_slices])\n\n        return preprocessed_slices  # Return without augmentation if augmentor is None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:32.992493Z","iopub.execute_input":"2025-01-22T19:12:32.992736Z","iopub.status.idle":"2025-01-22T19:12:33.008187Z","shell.execute_reply.started":"2025-01-22T19:12:32.992715Z","shell.execute_reply":"2025-01-22T19:12:33.007553Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class TrainDatasetGenerator(MedicalScanDataset):\n    \"\"\"Dataset class for training medical scan data.\"\"\"\n    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n        super().__init__(data_dir, patient_scan_labels, augmentor)\n\nclass TestDatasetGenerator(MedicalScanDataset):\n    \"\"\"Dataset class for testing medical scan data.\"\"\"\n    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n        super().__init__(data_dir, patient_scan_labels, augmentor)\n\ndef get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n\ndef get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.009562Z","iopub.execute_input":"2025-01-22T19:12:33.009786Z","iopub.status.idle":"2025-01-22T19:12:33.026188Z","shell.execute_reply.started":"2025-01-22T19:12:33.009768Z","shell.execute_reply":"2025-01-22T19:12:33.025416Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def calculate_metrics(predictions, labels):\n    \"\"\"Calculate and return performance metrics.\"\"\"\n    return {\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"precision\": precision_score(labels, predictions, average='macro'),\n        \"recall\": recall_score(labels, predictions, average='macro'),\n        \"f1\": f1_score(labels, predictions, average='macro')\n    }\n\ndef print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n    \"\"\"Print statistics for an epoch.\"\"\"\n    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n          f\"F1: {metrics['f1']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.026879Z","iopub.execute_input":"2025-01-22T19:12:33.027061Z","iopub.status.idle":"2025-01-22T19:12:33.046502Z","shell.execute_reply.started":"2025-01-22T19:12:33.027046Z","shell.execute_reply":"2025-01-22T19:12:33.045767Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"weights = torchvision.models.ViT_B_16_Weights.DEFAULT\ntransform = weights.transforms()\n\n# train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\ntrain_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n\ntrain_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\nval_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\ntest_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.047218Z","iopub.execute_input":"2025-01-22T19:12:33.047407Z","iopub.status.idle":"2025-01-22T19:12:33.200935Z","shell.execute_reply.started":"2025-01-22T19:12:33.047390Z","shell.execute_reply":"2025-01-22T19:12:33.200179Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-5ef67021d1ba>:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n<ipython-input-8-5ef67021d1ba>:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n<ipython-input-8-5ef67021d1ba>:20: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  patient_scan_labels[column] = patient_scan_labels[column].apply(\n<ipython-input-8-5ef67021d1ba>:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Patch Embedding Layer\n* A module that converts input images into patch embeddings suitable for a Vision Transformer.\n\n* This module takes an input image, divides it into patches, and then embeds each patch into a vector of \nspecified dimensionality using a convolutional layer.\n\n* Attributes:\n    patch_size (int): The size of each square patch.\n    embed_dim (int): The dimensionality of the output embedding vector for each patch.\n    conv (nn.Conv2d): A convolutional layer that extracts patches from the input image.\n\n* Input Shape:\n    - Input tensor `x`: Shape (N, C, H, W)\n        - N: Batch size\n        - C: Number of channels (1 for grayscale images like MNIST)\n        - H: Height of the input image\n        - W: Width of the input image\n\n* Output Shape:\n    - Output tensor: Shape (N, num_patches, embed_dim)\n        - N: Batch size\n        - num_patches: The number of patches extracted from the image, calculated as:\n          $$ \\text{num\\_patches} = \\left(\\frac{H}{\\text{patch\\_size}}\\right) \\times \\left(\\frac{W}{\\text{patch\\_size}}\\right) $$\n        - embed_dim: The dimensionality of the embedding for each patch.\n\n* Methods:\n    forward(x): Forward pass to compute the patch embeddings from input images.\n","metadata":{}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, in_channels, patch_size, embed_dim):\n        super(PatchEmbedding, self).__init__()\n        self.patch_size = patch_size\n        self.embed_dim = embed_dim\n        \n        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=embed_dim,\n                              kernel_size=patch_size, stride=patch_size, padding=0)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass to create patch embeddings.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (N, C, H, W).\n\n        Returns:\n            torch.Tensor: Output tensor of shape (N, num_patches, embed_dim).\n        \"\"\"\n        x = self.conv(x)  # Apply convolution to create patches \n        # After convolution: shape (N, embed_dim, H/patch_size, W/patch_size)\n        \n        x = x.flatten(2)  # Flatten patches into a sequence \n        # After flattening: shape (N, embed_dim, num_patches)\n\n        return x.transpose(1, 2)  # Rearrange dimensions for transformer input \n        # Final output shape: (N, num_patches, embed_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.201603Z","iopub.execute_input":"2025-01-22T19:12:33.201827Z","iopub.status.idle":"2025-01-22T19:12:33.206605Z","shell.execute_reply.started":"2025-01-22T19:12:33.201808Z","shell.execute_reply":"2025-01-22T19:12:33.205823Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Multihead Self-Attention Layer","metadata":{}},{"cell_type":"code","source":"# 1. Create a class that inherits from nn.Module\nclass MultiheadSelfAttentionBlock(nn.Module):\n    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n    \"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # does our batch dimension come first?\n\n    # 5. Create a forward() method to pass the data through the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, attn_weights = self.multihead_attn(query=x, # query embeddings\n                                             key=x, # key embeddings\n                                             value=x, # value embeddings\n                                             need_weights=True) # do we need the weights or just the layer outputs?\n        return attn_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.207510Z","iopub.execute_input":"2025-01-22T19:12:33.207843Z","iopub.status.idle":"2025-01-22T19:12:33.221347Z","shell.execute_reply.started":"2025-01-22T19:12:33.207813Z","shell.execute_reply":"2025-01-22T19:12:33.220746Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## MLP Block Layer","metadata":{}},{"cell_type":"code","source":"# 1. Create a class that inherits from nn.Module\nclass MLPBlock(nn.Module):\n    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multilayer perceptron (MLP) layer(s)\n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n                      out_features=embedding_dim), # take back to embedding_dim\n            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n        )\n\n    # 5. Create a forward() method to pass the data through the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.222094Z","iopub.execute_input":"2025-01-22T19:12:33.222326Z","iopub.status.idle":"2025-01-22T19:12:33.239261Z","shell.execute_reply.started":"2025-01-22T19:12:33.222308Z","shell.execute_reply":"2025-01-22T19:12:33.238480Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Transformer Encoder Block","metadata":{}},{"cell_type":"code","source":"# 1. Create a class that inherits from nn.Module\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"Creates a Transformer Encoder block.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                 attn_dropout:float=0): # Amount of dropout for attention layers\n        super().__init__()\n\n        # 3. Create MSA block (equation 2)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n\n        # 4. Create MLP block (equation 3)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n\n    # 5. Create a forward() method\n    def forward(self, x):\n\n        # 6. Create residual connection for MSA block (add the input to the output)\n        # x =  self.msa_block(x) + x\n        attn_output = self.msa_block(x)\n        \n        assert type(attn_output) == torch.Tensor, \"The MSA block output should be a PyTorch tensor.\"\n        x = attn_output + x\n\n        # 7. Create residual connection for MLP block (add the input to the output)\n        x = self.mlp_block(x) + x\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.239918Z","iopub.execute_input":"2025-01-22T19:12:33.240172Z","iopub.status.idle":"2025-01-22T19:12:33.255914Z","shell.execute_reply.started":"2025-01-22T19:12:33.240142Z","shell.execute_reply":"2025-01-22T19:12:33.255118Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Vision Transformer Model","metadata":{}},{"cell_type":"code","source":"class AttentionLayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        # x shape: (batch_size, num_instances, feature_dim)\n        attention_weights = self.attention(x)\n        weights = F.softmax(attention_weights, dim=1)\n\n        return (x * weights).sum(dim=1), weights.squeeze(-1)\n\n# 1. Create a ViT class that inherits from nn.Module\nclass ViT(nn.Module):\n    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n                 in_channels:int=3, # Number of channels in input image\n                 patch_size:int=16, # Patch size\n                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0, # Dropout for attention projection\n                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n                 num_classes:int=1000): # Default for ImageNet but can customize this\n        super().__init__() # don't forget the super().__init__()!\n\n        # 3. Make the image size is divisible by the patch size\n        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n\n        # 4. Calculate number of patches (height * width/patch^2)\n        self.num_patches = (img_size * img_size) // patch_size**2\n\n        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n\n        # 6. Create learnable position embedding\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n\n        # 7. Create embedding dropout value\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n\n        # 8. Create patch embedding layer\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embed_dim=embedding_dim)\n\n        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n        # Note: The \"*\" means \"all\"\n        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n\n        # 10. Create classifier head\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n\n        # Attention layer\n        self.attention_layer = AttentionLayer(embedding_dim * (self.num_patches + 1),\n                                                hidden_dim=128)\n        self.cls_attention = nn.Linear(embedding_dim * (self.num_patches + 1), num_classes)\n\n    # 11. Create a forward() method\n    def forward(self, x):\n        batch_size, num_instances, channels, height, width = x.size()\n        class_token = self.class_embedding.expand(batch_size * num_instances, -1, -1)\n        x = x.view(batch_size * num_instances, channels, height, width)\n        x = self.patch_embedding(x)\n        x = torch.cat((class_token, x), dim=1)\n        x = self.position_embedding + x\n        x = self.embedding_dropout(x)\n        x = self.transformer_encoder(x) # Shape: (batch_size * num_instances, num_patches + 1, hidden_size)\n        x = x.view(batch_size, num_instances, -1)\n        x = x.max(dim=1).values\n        x = self.cls_attention(x)\n        return x.squeeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.256546Z","iopub.execute_input":"2025-01-22T19:12:33.256797Z","iopub.status.idle":"2025-01-22T19:12:33.440482Z","shell.execute_reply.started":"2025-01-22T19:12:33.256778Z","shell.execute_reply":"2025-01-22T19:12:33.439751Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class PGLikelihood(gpytorch.likelihoods._OneDimensionalLikelihood):\n    # contribution to Eqn (10) in Reference [1].\n    def expected_log_prob(self, target, input, *args, **kwargs):\n        mean, variance = input.mean, input.variance\n        # Compute the expectation E[f_i^2]\n        raw_second_moment = variance + mean.pow(2)\n\n        # Translate targets to be -1, 1\n        target = target.to(mean.dtype).mul(2.).sub(1.)\n\n        # We detach the following variable since we do not want\n        # to differentiate through the closed-form PG update.\n        c = raw_second_moment.detach().sqrt()\n        half_omega = 0.25 * torch.tanh(0.5 * c) / c\n        res = 0.5 * target * mean - half_omega * raw_second_moment\n        res = res.sum(dim=-1)\n\n        return res\n\n    # define the likelihood\n    def forward(self, function_samples):\n        return torch.distributions.Bernoulli(logits=function_samples)\n\n    # define the marginal likelihood using Gauss Hermite quadrature\n    def marginal(self, function_dist):\n        prob_lambda = lambda function_samples: self.forward(function_samples).probs\n        probs = self.quadrature(prob_lambda, function_dist)\n        return torch.distributions.Bernoulli(probs=probs)\n\n\nclass SingletaskGPModel(gpytorch.models.ApproximateGP):\n    def __init__(self, inducing_points, kernel_type='rbf', nu=2.5):\n        \"\"\"\n        Args:\n            inducing_points (torch.Tensor):\n            kernel_type (str):\n            nu (float):\n        \"\"\"\n        variational_distribution = gpytorch.variational.NaturalVariationalDistribution(inducing_points.size(0))\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(SingletaskGPModel, self).__init__(variational_strategy)\n\n        self.mean_module = gpytorch.means.ConstantMean()\n\n        if kernel_type == 'rbf':\n            self.covar_module = ScaleKernel(RBFKernel())\n        elif kernel_type == 'matern_kernel':\n            self.covar_module = ScaleKernel(MaternKernel(nu=nu))\n        else:\n            raise ValueError(\"kernel_type must be either 'rbf' or 'matern_kernel'\")\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\nclass MultitaskGPModel(gpytorch.models.ApproximateGP):\n    def __init__(self, num_latents, num_tasks, hidden_dim=512):\n        # Let's use a different set of inducing points for each latent function\n        inducing_points = torch.rand(num_latents, hidden_dim, 1)\n\n        # We have to mark the CholeskyVariationalDistribution as batch\n        # so that we learn a variational distribution for each task\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n        )\n\n        # We have to wrap the VariationalStrategy in a LMCVariationalStrategy\n        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n        variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n            gpytorch.variational.VariationalStrategy(\n                self, inducing_points, variational_distribution, learn_inducing_locations=True\n            ),\n            num_tasks=num_tasks,\n            num_latents=num_latents,\n            latent_dim=-1\n        )\n\n        super().__init__(variational_strategy)\n\n        # The mean and covariance modules should be marked as batch\n        # so we learn a different set of hyperparameters\n        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents])),\n            batch_shape=torch.Size([num_latents])\n        )\n\n    def forward(self, x):\n        # The forward function should be written as if we were dealing with each output\n        # dimension in batch\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.441569Z","iopub.execute_input":"2025-01-22T19:12:33.441934Z","iopub.status.idle":"2025-01-22T19:12:33.462501Z","shell.execute_reply.started":"2025-01-22T19:12:33.441904Z","shell.execute_reply":"2025-01-22T19:12:33.461751Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class BiGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, dropout=0.6):\n        super(BiGRU, self).__init__()\n        self.bigru = nn.GRU(input_size, hidden_size, num_layers,\n                             batch_first=True, bidirectional=bidirectional, dropout=dropout)\n        self.num_layers = num_layers\n\n    def forward(self, x, hidden_states=None):\n        if hidden_states is None:\n            num_directions = 2 if self.bigru.bidirectional else 1\n            hidden_states = torch.zeros(num_directions * self.bigru.num_layers,\n                                        x.size(0), self.bigru.hidden_size,\n                                        device=x.device)\n        gru_out, _ = self.bigru(x, hidden_states)\n        return gru_out\n\nclass InstanceAttention(nn.Module):\n    def __init__(self, input_dim, num_classes=1):\n        super(InstanceAttention, self).__init__()\n        # Feature-level attention\n        self.feature_attention = nn.Linear(input_dim, input_dim)\n        # Slice-level attention\n        self.slice_attention = nn.Linear(input_dim, num_classes)\n\n    def forward(self, x):\n        # x shape: (batch_size, num_instances, input_dim)\n\n        # Feature-level attention\n        feature_scores = self.feature_attention(x)  # (batch_size, num_instances, input_dim)\n        feature_attention_weights = torch.nn.functional.softmax(feature_scores, dim=2)\n        # Apply feature-level attention\n        x_weighted_features = x * feature_attention_weights  # (batch_size, num_instances, input_dim)\n\n        # Slice-level attention\n        slice_scores = self.slice_attention(x_weighted_features).squeeze(-1)  # (batch_size, num_instances)\n        slice_attention_weights = torch.nn.functional.softmax(slice_scores, dim=1)\n        slice_attention_weights = slice_attention_weights.unsqueeze(-1)  # (batch_size, num_instances, 1)\n\n        return x_weighted_features, slice_attention_weights\n\nclass CNN_ATT_GRU(nn.Module):\n    def __init__(self, num_layers=2, input_channels=1,\n                 cnn_feature_size=512, gru_hidden_size=256, num_classes=1, dropout_gru=0.3, dropout_fc=0.5, cnn='resnet'):\n        super(CNN_ATT_GRU, self).__init__()\n        self.cnn = torchvision.models.resnet18(torchvision.models.ResNet18_Weights.DEFAULT)\n        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.cnn.fc = nn.Identity()\n\n        self.attention = AttentionLayer(input_dim=cnn_feature_size, hidden_dim=cnn_feature_size)\n        self.bigru = BiGRU(input_size=cnn_feature_size,\n                           hidden_size=gru_hidden_size,\n                           num_layers=num_layers,\n                           dropout=dropout_gru)\n        self.fc = nn.Sequential(\n            nn.Linear(2 * gru_hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_fc),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_fc),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout_fc),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        batch_size, num_instances, channels, height, width = x.size()\n        x = x.view(batch_size * num_instances, channels, height, width)\n        x = self.cnn(x)\n        features = x.view(batch_size, num_instances, -1)\n\n        gru_features = self.bigru(features)\n        attention_features, _ = self.attention(gru_features)\n\n        # Pass aggregated features through FC layers\n        output = self.fc(attention_features)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:12:33.463247Z","iopub.execute_input":"2025-01-22T19:12:33.463443Z","iopub.status.idle":"2025-01-22T19:12:33.481241Z","shell.execute_reply.started":"2025-01-22T19:12:33.463427Z","shell.execute_reply":"2025-01-22T19:12:33.480506Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Model & Optimizer Definition","metadata":{}},{"cell_type":"code","source":"model = ViT(\n    img_size=img_size,\n    in_channels=1,\n    patch_size=patch_size,\n    embedding_dim=embedding_dim,\n    num_heads=num_heads,\n    num_transformer_layers=num_layers,\n    num_classes=num_classes\n).to(device)\n\n# model = CNN_ATT_GRU(num_layers=2, input_channels=1,\n#                     cnn_feature_size=512, gru_hidden_size=256, num_classes=num_classes, dropout_gru=0.3, dropout_fc=0.25, cnn='resnet').to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.BCEWithLogitsLoss()\nlikelihood = PGLikelihood().to(device)\n\n# summary(model, input_size=(TRAIN_BATCH_SIZE * MAX_SLICES, 1, 224, 224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:13:17.020826Z","iopub.execute_input":"2025-01-22T19:13:17.021159Z","iopub.status.idle":"2025-01-22T19:13:17.083188Z","shell.execute_reply.started":"2025-01-22T19:13:17.021136Z","shell.execute_reply":"2025-01-22T19:13:17.082522Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Training Step","metadata":{}},{"cell_type":"code","source":"# Function to calculate metrics for multi-label classification\ndef calculate_metrics(preds, labels):\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average='samples')  # Use 'samples' for multi-label\n    recall = recall_score(labels, preds, average='samples')  # Use 'samples' for multi-label\n    f1 = f1_score(labels, preds, average='samples')  # Use 'samples' for multi-label\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\n# Function to print epoch statistics\ndef print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n    print(f'Epoch [{epoch+1}/{num_epochs}], {phase.capitalize()}')\n    print(f'Loss: {loss:.4f}, '\n          f'Accuracy: {metrics[\"accuracy\"]:.4f}, Precision: {metrics[\"precision\"]:.4f}, '\n          f'Recall: {metrics[\"recall\"]:.4f}, F1: {metrics[\"f1\"]:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:13:18.721659Z","iopub.execute_input":"2025-01-22T19:13:18.721970Z","iopub.status.idle":"2025-01-22T19:13:18.727003Z","shell.execute_reply.started":"2025-01-22T19:13:18.721948Z","shell.execute_reply":"2025-01-22T19:13:18.726151Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Ensure the device is set to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)  # Move the model to the appropriate device\n\n# Use BCEWithLogitsLoss for multi-label classification\ncriterion = nn.BCEWithLogitsLoss()\n\n# Initialize variables to track the best validation performance\nbest_val_acc = 0  # Use infinity as the initial best loss\nbest_model_state_dict = None  # To store the best model's state dict\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    # Training phase\n    for imgs, _, labels, multi_labels in train_loader:\n        optimizer.zero_grad()\n\n        imgs, multi_labels = imgs.to(device), multi_labels.to(device)  # Move data to GPU\n\n        outputs = model(imgs)\n        loss = criterion(outputs, multi_labels.float())  # Use multi_labels for loss calculation\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n\n        total_loss += loss.item()\n        preds = outputs.ge(0.5).float()  # Apply sigmoid and threshold\n        all_preds.append(preds.cpu())\n        all_labels.append(multi_labels.cpu())\n\n    avg_loss = total_loss / len(train_loader)\n\n    # Concatenate all predictions and labels for metric calculation\n    all_preds = torch.cat(all_preds)\n    all_labels = torch.cat(all_labels)\n\n    train_metrics = calculate_metrics(all_preds.numpy(), all_labels.numpy())\n\n    print_epoch_stats(epoch, num_epochs, \"train\", avg_loss, train_metrics)\n\n    # Validation phase\n    model.eval()\n    val_total_loss = 0\n    val_all_preds = []\n    val_all_labels = []\n\n    with torch.no_grad():  # Disable gradient calculation for validation\n        for imgs, _, labels, multi_labels in val_loader:\n            imgs, multi_labels = imgs.to(device), multi_labels.to(device)  # Move data to GPU\n            outputs = model(imgs)  # Forward pass\n            loss = criterion(outputs, multi_labels.float())  # Use multi_labels for loss calculation\n            val_total_loss += loss.item()\n            preds = outputs.ge(0.5).float()  # Apply sigmoid and threshold\n            val_all_preds.append(preds.cpu())\n            val_all_labels.append(multi_labels.cpu())\n\n    val_avg_loss = val_total_loss / len(val_loader)\n\n    # Concatenate validation predictions and labels for metric calculation\n    val_all_preds = torch.cat(val_all_preds)\n    val_all_labels = torch.cat(val_all_labels)\n\n    val_metrics = calculate_metrics(val_all_preds.numpy(), val_all_labels.numpy())\n\n    print_epoch_stats(epoch, num_epochs, \"validation\", val_avg_loss, val_metrics)\n\n    # Check if the current validation accuracy is the best so far\n    if val_metrics['accuracy'] > best_val_acc:\n        best_val_acc = val_metrics['accuracy']\n        best_model_state_dict = model.state_dict()\n\n# After training, load the best model for testing\nprint(f'Best validation accuracy: {best_val_acc:.4f}')\nmodel.load_state_dict(best_model_state_dict)\nmodel.to(device)","metadata":{"jupyter":{"is_executing":true},"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:13:20.269356Z","iopub.execute_input":"2025-01-22T19:13:20.269663Z","iopub.status.idle":"2025-01-22T19:14:22.984298Z","shell.execute_reply.started":"2025-01-22T19:13:20.269640Z","shell.execute_reply":"2025-01-22T19:14:22.983369Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/1], Train\nLoss: 0.5693, Accuracy: 0.4008, Precision: 0.0972, Recall: 0.0408, F1: 0.0561\nEpoch [1/1], Validation\nLoss: 0.5615, Accuracy: 0.5595, Precision: 0.0000, Recall: 0.0000, F1: 0.0000\nBest validation accuracy: 0.5595\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"ViT(\n  (embedding_dropout): Dropout(p=0.1, inplace=False)\n  (patch_embedding): PatchEmbedding(\n    (conv): Conv2d(1, 64, kernel_size=(14, 14), stride=(14, 14))\n  )\n  (transformer_encoder): Sequential(\n    (0): TransformerEncoderBlock(\n      (msa_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n      )\n      (mlp_block): MLPBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=64, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=64, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (1): TransformerEncoderBlock(\n      (msa_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n      )\n      (mlp_block): MLPBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=64, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=64, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (2): TransformerEncoderBlock(\n      (msa_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n      )\n      (mlp_block): MLPBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=64, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=64, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (3): TransformerEncoderBlock(\n      (msa_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n      )\n      (mlp_block): MLPBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=64, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=64, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (4): TransformerEncoderBlock(\n      (msa_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n      )\n      (mlp_block): MLPBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=64, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=64, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (5): TransformerEncoderBlock(\n      (msa_block): MultiheadSelfAttentionBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n      )\n      (mlp_block): MLPBlock(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (mlp): Sequential(\n          (0): Linear(in_features=64, out_features=3072, bias=True)\n          (1): GELU(approximate='none')\n          (2): Dropout(p=0.1, inplace=False)\n          (3): Linear(in_features=3072, out_features=64, bias=True)\n          (4): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (classifier): Sequential(\n    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n    (1): Linear(in_features=64, out_features=6, bias=True)\n  )\n  (attention_layer): AttentionLayer(\n    (attention): Sequential(\n      (0): Linear(in_features=16448, out_features=128, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=128, out_features=1, bias=True)\n    )\n  )\n  (cls_attention): Linear(in_features=16448, out_features=6, bias=True)\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## Attention Map Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_attention_map(attn_weights, img_shape):\n    # Assuming attn_weights is of shape (num_heads, seq_length, seq_length)\n    attn_map = attn_weights.mean(dim=0)  # Average over heads\n\n    plt.figure(figsize=(8, 8))\n    plt.imshow(attn_map.detach().cpu().numpy(), cmap='viridis')\n    plt.colorbar()\n    plt.title('Attention Map')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:14:22.985783Z","iopub.execute_input":"2025-01-22T19:14:22.986047Z","iopub.status.idle":"2025-01-22T19:14:22.990451Z","shell.execute_reply.started":"2025-01-22T19:14:22.986024Z","shell.execute_reply":"2025-01-22T19:14:22.989643Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Evaluation Step","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.inference_mode():\n    for imgs, _, _, multi_labels in test_loader:\n        imgs, multi_labels = imgs.to(device), multi_labels.to(device)  # Move data to GPU\n        outputs = model(imgs)  # Forward pass\n\n        # Apply sigmoid and threshold to get binary predictions\n        predicted = outputs.ge(0.5).float()\n\n        # Collect all predictions and labels for computing metrics\n        all_preds.append(predicted.cpu())\n        all_labels.append(multi_labels.cpu())\n\n# Concatenate all predictions and labels\nall_preds = torch.cat(all_preds, dim=0)\nall_labels = torch.cat(all_labels, dim=0)\n\n# Convert to numpy arrays for metric calculation\nall_preds = all_preds.numpy()\nall_labels = all_labels.numpy()\n\n# Compute subset accuracy (exact match)\nsubset_accuracy = accuracy_score(all_labels, all_preds)\n\n# Compute precision, recall, and F1 score (use 'samples' or 'macro' for multi-label)\nprecision = precision_score(all_labels, all_preds, average='samples')\nrecall = recall_score(all_labels, all_preds, average='samples')\nf1 = f1_score(all_labels, all_preds, average='samples')\n\n# Compute Hamming loss (lower is better)\nhamming_loss_value = hamming_loss(all_labels, all_preds)\n\nprint(f'Subset Accuracy (Exact Match): {subset_accuracy:.4f}')\nprint(f'Precision (Samples): {precision:.4f}')\nprint(f'Recall (Samples): {recall:.4f}')\nprint(f'F1 Score (Samples): {f1:.4f}')\nprint(f'Hamming Loss: {hamming_loss_value:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T19:14:22.991800Z","iopub.execute_input":"2025-01-22T19:14:22.991996Z","iopub.status.idle":"2025-01-22T19:14:34.646144Z","shell.execute_reply.started":"2025-01-22T19:14:22.991979Z","shell.execute_reply":"2025-01-22T19:14:34.645088Z"}},"outputs":[{"name":"stdout","text":"Subset Accuracy (Exact Match): 0.5333\nPrecision (Samples): 0.0000\nRecall (Samples): 0.0000\nF1 Score (Samples): 0.0000\nHamming Loss: 0.2167\n","output_type":"stream"}],"execution_count":24}]}