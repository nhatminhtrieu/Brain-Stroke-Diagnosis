{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:47.504705Z",
     "start_time": "2025-01-20T15:46:46.241048Z"
    }
   },
   "source": [
    "import os.path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "def seed_everything(seed=39):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 SUPER is available.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:47.787810Z",
     "start_time": "2025-01-20T15:46:47.509071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "KAGGLE = os.path.exists(('kaggle/input'))\n",
    "ROOT_DIR = None\n",
    "DATA_DIR = ROOT_DIR + 'rsna-mil-training/' if KAGGLE else '../rsna-ich-mil/'\n",
    "DICOM_DIR = DATA_DIR\n",
    "CSV_PATH = DICOM_DIR + 'training_1000_scan_subset.csv' if KAGGLE else './data_analyze/training_dataset_2_redundancy.csv'\n",
    "# patient_scan_labels = pd.read_csv(CSV_PATH, nrows=1150)\n",
    "patient_scan_labels = pd.read_csv(CSV_PATH)\n",
    "dicom_dir = DICOM_DIR if KAGGLE else DATA_DIR"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:47.820804Z",
     "start_time": "2025-01-20T15:46:47.811165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "if os.path.exists('/media02/tdhoang01/python-debugging/config.yaml'):\n",
    "    path = '/media02/tdhoang01/python-debugging/config.yaml'\n",
    "else:\n",
    "    path = '../config.yaml'\n",
    "with open(path) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Accessing constants from config\n",
    "HEIGHT = config['height']\n",
    "WIDTH = config['width']\n",
    "CHANNELS = config['channels']\n",
    "\n",
    "TRAIN_BATCH_SIZE = config['train_batch_size']\n",
    "VALID_BATCH_SIZE = config['valid_batch_size']\n",
    "TEST_BATCH_SIZE = config['test_batch_size']\n",
    "TEST_SIZE = config['test_size']\n",
    "VALID_SIZE = config['valid_size']\n",
    "\n",
    "TRAINING_TYPE = config['training_type']\n",
    "GP_MODEL = config['gp_model']\n",
    "GP_KERNEL = config['kernel_type']\n",
    "\n",
    "MAX_SLICES = config['max_slices']\n",
    "SHAPE = tuple(config['shape'])\n",
    "\n",
    "NUM_EPOCHS = config['num_epochs']\n",
    "LEARNING_RATE = config['learning_rate']\n",
    "INDUCING_POINTS = config['inducing_points']\n",
    "THRESHOLD = config['threshold']\n",
    "\n",
    "NUM_CLASSES = config['num_classes']\n",
    "\n",
    "TARGET_LABELS = config['target_labels']\n",
    "\n",
    "MODEL_PATH = config['model_path']\n",
    "DEVICE = config['device']\n",
    "\n",
    "PROJECTION_LOCATION = config['projection_location']\n",
    "PROJECTION_HIDDEN_DIM = config['projection_hidden_dim']\n",
    "PROJECTION_OUTPUT_DIM = config['projection_output_dim']\n",
    "\n",
    "ATTENTION_HIDDEN_DIM = config['attention_hidden_dim']"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters definition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:47.861638Z",
     "start_time": "2025-01-20T15:46:47.857741Z"
    }
   },
   "source": [
    "# # Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 10\n",
    "img_size = 224  # MNIST images are 28x28 pixels\n",
    "num_classes = 6  # Digits from 0 to 9\n",
    "patch_size = 14  # Size of each patch (7x7)\n",
    "embedding_dim = 64  # Dimensionality of the embeddings\n",
    "num_heads = 4  # Number of attention heads\n",
    "num_layers = 6  # Number of transformer layers\n",
    "dropout_rate = 0.5  # Dropout rate for regularization"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:47.928166Z",
     "start_time": "2025-01-20T15:46:47.923669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_dataset(patient_scan_labels, test_size=TEST_SIZE, val_size=VALID_SIZE, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels['patient_label']\n",
    "    if test_size > 0:\n",
    "        # First, split off the test set\n",
    "        train_val_labels, test_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=test_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        # Calculate the validation size relative to the train_val set\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "\n",
    "        # Split the train_val set into train and validation sets\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            train_val_labels,\n",
    "            test_size=val_size_adjusted,\n",
    "            stratify=train_val_labels['patient_label'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        train_labels, val_labels = train_test_split(\n",
    "            patient_scan_labels,\n",
    "            test_size=val_size,\n",
    "            stratify=labels,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels, val_labels, test_labels\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "def split_dataset_for_multilabel(patient_scan_labels, test_size=0.15, val_size=0.25, random_state=42):\n",
    "    # Extract the labels from the DataFrame\n",
    "    labels = patient_scan_labels[['patient_any', 'patient_epidural', 'patient_intraparenchymal',\n",
    "                                  'patient_intraventricular', 'patient_subarachnoid', 'patient_subdural']].values\n",
    "\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(msss.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels = patient_scan_labels.iloc[train_idx]\n",
    "        test_labels = patient_scan_labels.iloc[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(train_labels, labels[train_idx]))\n",
    "\n",
    "        train_labels_final = train_labels.iloc[train_idx]\n",
    "        val_labels = train_labels.iloc[val_idx]\n",
    "\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(patient_scan_labels, labels))\n",
    "\n",
    "        train_labels_final = patient_scan_labels.iloc[train_idx]\n",
    "        val_labels = patient_scan_labels.iloc[val_idx]\n",
    "        test_labels = None\n",
    "\n",
    "    return train_labels_final, val_labels, test_labels"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:48.065849Z",
     "start_time": "2025-01-20T15:46:47.969713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset_generators.RSNA_Dataset import MedicalScanDataset\n",
    "class TrainDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for training medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "class TestDatasetGenerator(MedicalScanDataset):\n",
    "    \"\"\"Dataset class for testing medical scan data.\"\"\"\n",
    "    def __init__(self, data_dir, patient_scan_labels, augmentor=None):\n",
    "        super().__init__(data_dir, patient_scan_labels, augmentor)\n",
    "\n",
    "def get_train_loader(dicom_dir, patient_scan_labels, batch_size=TRAIN_BATCH_SIZE):\n",
    "    # original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=augmentor)\n",
    "    original_dataset = TrainDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(original_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "def get_test_loader(dicom_dir, patient_scan_labels, batch_size=TEST_BATCH_SIZE):\n",
    "    test_dataset = TestDatasetGenerator(dicom_dir, patient_scan_labels, augmentor=None)\n",
    "    return DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:48.073743Z",
     "start_time": "2025-01-20T15:46:48.071702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate and return performance metrics.\"\"\"\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"precision\": precision_score(labels, predictions, average='macro'),\n",
    "        \"recall\": recall_score(labels, predictions, average='macro'),\n",
    "        \"f1\": f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    \"\"\"Print statistics for an epoch.\"\"\"\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - {phase.capitalize()}:\")\n",
    "    print(f\"Loss: {loss:.4f}, Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "          f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "          f\"F1: {metrics['f1']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:50.697988Z",
     "start_time": "2025-01-20T15:46:48.120182Z"
    }
   },
   "source": [
    "weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "transform = weights.transforms()\n",
    "# print(transform)\n",
    "\n",
    "# train_labels, val_labels, test_labels = split_dataset(patient_scan_labels, test_size=TEST_SIZE)\n",
    "train_labels, val_labels, test_labels = split_dataset_for_multilabel(patient_scan_labels, test_size=TEST_SIZE)\n",
    "# test_labels = pd.read_csv('./data_analyze/testing_dataset_150.csv')\n",
    "train_loader = get_train_loader(dicom_dir, train_labels, batch_size=TRAIN_BATCH_SIZE)\n",
    "val_loader = get_train_loader(dicom_dir, val_labels, batch_size=VALID_BATCH_SIZE)\n",
    "test_loader = get_test_loader(dicom_dir, test_labels, batch_size=TEST_BATCH_SIZE)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['filename'] = patient_scan_labels['filename'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['labels'] = patient_scan_labels['labels'].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels[column] = patient_scan_labels[column].apply(\n",
      "/media/hskha23/Kha/Brain-Stroke-Diagnosis/dataset_generators/RSNA_Dataset.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_scan_labels['patient_label'] = patient_scan_labels['patient_label'].astype(bool)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Embedding Layer\n",
    "* A module that converts input images into patch embeddings suitable for a Vision Transformer.\n",
    "\n",
    "* This module takes an input image, divides it into patches, and then embeds each patch into a vector of \n",
    "specified dimensionality using a convolutional layer.\n",
    "\n",
    "* Attributes:\n",
    "    patch_size (int): The size of each square patch.\n",
    "    embed_dim (int): The dimensionality of the output embedding vector for each patch.\n",
    "    conv (nn.Conv2d): A convolutional layer that extracts patches from the input image.\n",
    "\n",
    "* Input Shape:\n",
    "    - Input tensor `x`: Shape (N, C, H, W)\n",
    "        - N: Batch size\n",
    "        - C: Number of channels (1 for grayscale images like MNIST)\n",
    "        - H: Height of the input image\n",
    "        - W: Width of the input image\n",
    "\n",
    "* Output Shape:\n",
    "    - Output tensor: Shape (N, num_patches, embed_dim)\n",
    "        - N: Batch size\n",
    "        - num_patches: The number of patches extracted from the image, calculated as:\n",
    "          $$ \\text{num\\_patches} = \\left(\\frac{H}{\\text{patch\\_size}}\\right) \\times \\left(\\frac{W}{\\text{patch\\_size}}\\right) $$\n",
    "        - embed_dim: The dimensionality of the embedding for each patch.\n",
    "\n",
    "* Methods:\n",
    "    forward(x): Forward pass to compute the patch embeddings from input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:50.714621Z",
     "start_time": "2025-01-20T15:46:50.712505Z"
    }
   },
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, in_channels, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to create patch embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (N, num_patches, embed_dim).\n",
    "        \"\"\"\n",
    "        x = self.conv(x)  # Apply convolution to create patches \n",
    "        # After convolution: shape (N, embed_dim, H/patch_size, W/patch_size)\n",
    "        \n",
    "        x = x.flatten(2)  # Flatten patches into a sequence \n",
    "        # After flattening: shape (N, embed_dim, num_patches)\n",
    "\n",
    "        return x.transpose(1, 2)  # Rearrange dimensions for transformer input \n",
    "        # Final output shape: (N, num_patches, embed_dim)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:50.761350Z",
     "start_time": "2025-01-20T15:46:50.756182Z"
    }
   },
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, attn_weights = self.multihead_attn(query=x, # query embeddings\n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=True) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Block Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:50.812851Z",
     "start_time": "2025-01-20T15:46:50.809560Z"
    }
   },
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "\n",
    "    # 5. Create a forward() method to pass the data through the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:50.863869Z",
     "start_time": "2025-01-20T15:46:50.858040Z"
    }
   },
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "\n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "\n",
    "    # 5. Create a forward() method\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        # x =  self.msa_block(x) + x\n",
    "        attn_output = self.msa_block(x)\n",
    "        \n",
    "        assert type(attn_output) == torch.Tensor, \"The MSA block output should be a PyTorch tensor.\"\n",
    "        x = attn_output + x\n",
    "\n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:50.936470Z",
     "start_time": "2025-01-20T15:46:50.931817Z"
    }
   },
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, feature_dim)\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)\n",
    "\n",
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "\n",
    "        # 3. Make the image size is divisible by the patch size\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "\n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "\n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "\n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(img_size=img_size,\n",
    "                                              in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embed_dim=embedding_dim)\n",
    "\n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "\n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention_layer = AttentionLayer(embedding_dim * (self.num_patches + 1),\n",
    "                                                hidden_dim=128)\n",
    "        self.cls_attention = nn.Linear(embedding_dim * (self.num_patches + 1), num_classes)\n",
    "\n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        # 12. Get batch size\n",
    "        batch_size, num_instances, channels, height, width = x.size()\n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size * num_instances, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = x.view(batch_size * num_instances, channels, height, width)\n",
    "        x = self.patch_embedding(x)\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        # 16. Add position embedding to patch embedding (equation 1)\n",
    "        x = self.position_embedding + x\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        \n",
    "        # attn_weights_list = [] # List to store the attention weights from each layer\n",
    "        # for layer in self.transformer_encoder:\n",
    "        #     x, attn_weights = layer(x)\n",
    "        #     attn_weights_list.append(attn_weights)\n",
    "        \n",
    "        # x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "        # return x, attn_weights_list\n",
    "        \n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x) # Shape: (batch_size * num_instances, num_patches + 1, hidden_size)\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        # x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "        # 19.1\n",
    "        # x = x[:, 0] # Get the class token\n",
    "\n",
    "        # Flatten the sequence of patches\n",
    "        x = x.view(batch_size, num_instances, -1)\n",
    "        # x, _ = self.attention_layer(x)\n",
    "        # Apply max pooling\n",
    "        x = x.max(dim=1).values\n",
    "        x = self.cls_attention(x)\n",
    "        return x.squeeze(-1)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:51.017136Z",
     "start_time": "2025-01-20T15:46:50.978115Z"
    }
   },
   "cell_type": "code",
   "source": "import layers.gaussian_process as GPModel",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:51.036899Z",
     "start_time": "2025-01-20T15:46:51.032304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional=True, dropout=0.6):\n",
    "        super(BiGRU, self).__init__()\n",
    "        self.bigru = nn.GRU(input_size, hidden_size, num_layers,\n",
    "                             batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, hidden_states=None):\n",
    "        if hidden_states is None:\n",
    "            num_directions = 2 if self.bigru.bidirectional else 1\n",
    "            hidden_states = torch.zeros(num_directions * self.bigru.num_layers,\n",
    "                                        x.size(0), self.bigru.hidden_size,\n",
    "                                        device=x.device)\n",
    "        gru_out, _ = self.bigru(x, hidden_states)\n",
    "        return gru_out\n",
    "\n",
    "class InstanceAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=1):\n",
    "        super(InstanceAttention, self).__init__()\n",
    "        # Feature-level attention\n",
    "        self.feature_attention = nn.Linear(input_dim, input_dim)\n",
    "        # Slice-level attention\n",
    "        self.slice_attention = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_instances, input_dim)\n",
    "\n",
    "        # Feature-level attention\n",
    "        feature_scores = self.feature_attention(x)  # (batch_size, num_instances, input_dim)\n",
    "        feature_attention_weights = torch.nn.functional.softmax(feature_scores, dim=2)\n",
    "        # Apply feature-level attention\n",
    "        x_weighted_features = x * feature_attention_weights  # (batch_size, num_instances, input_dim)\n",
    "\n",
    "        # Slice-level attention\n",
    "        slice_scores = self.slice_attention(x_weighted_features).squeeze(-1)  # (batch_size, num_instances)\n",
    "        slice_attention_weights = torch.nn.functional.softmax(slice_scores, dim=1)\n",
    "        slice_attention_weights = slice_attention_weights.unsqueeze(-1)  # (batch_size, num_instances, 1)\n",
    "\n",
    "        return x_weighted_features, slice_attention_weights\n",
    "\n",
    "class CNN_ATT_GRU(nn.Module):\n",
    "    def __init__(self, num_layers=2, input_channels=1,\n",
    "                 cnn_feature_size=512, gru_hidden_size=256, num_classes=1, dropout_gru=0.3, dropout_fc=0.5, cnn='resnet'):\n",
    "        super(CNN_ATT_GRU, self).__init__()\n",
    "        self.cnn = torchvision.models.resnet18(torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "        self.cnn.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "\n",
    "        self.attention = AttentionLayer(input_dim=cnn_feature_size, hidden_dim=cnn_feature_size)\n",
    "        self.bigru = BiGRU(input_size=cnn_feature_size,\n",
    "                           hidden_size=gru_hidden_size,\n",
    "                           num_layers=num_layers,\n",
    "                           dropout=dropout_gru)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * gru_hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_fc),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_fc),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_fc),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_instances, channels, height, width = x.size()\n",
    "        x = x.view(batch_size * num_instances, channels, height, width)\n",
    "        x = self.cnn(x)\n",
    "        features = x.view(batch_size, num_instances, -1)\n",
    "\n",
    "        gru_features = self.bigru(features)\n",
    "        attention_features, _ = self.attention(gru_features)\n",
    "\n",
    "        # Pass aggregated features through FC layers\n",
    "        output = self.fc(attention_features)\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Optimizer Definition"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:51.228204Z",
     "start_time": "2025-01-20T15:46:51.078924Z"
    }
   },
   "source": [
    "# model = ViT(\n",
    "#     img_size=img_size,\n",
    "#     in_channels=1,\n",
    "#     patch_size=patch_size,\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     num_heads=num_heads,\n",
    "#     num_transformer_layers=num_layers,\n",
    "#     num_classes=num_classes\n",
    "# )\n",
    "import gpytorch\n",
    "\n",
    "model = CNN_ATT_GRU(num_layers=2, input_channels=1,\n",
    "                    cnn_feature_size=512, gru_hidden_size=256, num_classes=num_classes, dropout_gru=0.3, dropout_fc=0.25, cnn='resnet')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "likelihood = GPModel.PGLikelihood().to(device)\n",
    "# mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(train_loader.dataset))\n",
    "# variational_ngd_optimizer = gpytorch.optim.NGD(model.gp_layer.variational_parameters(), num_data=len(train_loader.dataset), lr=0.01)\n",
    "\n",
    "# Print model summary\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(4 * 28, 1, 224, 224))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─ResNet: 1-1                            --\n",
      "|    └─Conv2d: 2-1                       3,136\n",
      "|    └─BatchNorm2d: 2-2                  128\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─MaxPool2d: 2-4                    --\n",
      "|    └─Sequential: 2-5                   --\n",
      "|    |    └─BasicBlock: 3-1              73,984\n",
      "|    |    └─BasicBlock: 3-2              73,984\n",
      "|    └─Sequential: 2-6                   --\n",
      "|    |    └─BasicBlock: 3-3              230,144\n",
      "|    |    └─BasicBlock: 3-4              295,424\n",
      "|    └─Sequential: 2-7                   --\n",
      "|    |    └─BasicBlock: 3-5              919,040\n",
      "|    |    └─BasicBlock: 3-6              1,180,672\n",
      "|    └─Sequential: 2-8                   --\n",
      "|    |    └─BasicBlock: 3-7              3,673,088\n",
      "|    |    └─BasicBlock: 3-8              4,720,640\n",
      "|    └─AdaptiveAvgPool2d: 2-9            --\n",
      "|    └─Identity: 2-10                    --\n",
      "├─AttentionLayer: 1-2                    --\n",
      "|    └─Sequential: 2-11                  --\n",
      "|    |    └─Linear: 3-9                  262,656\n",
      "|    |    └─ReLU: 3-10                   --\n",
      "|    |    └─Linear: 3-11                 513\n",
      "├─BiGRU: 1-3                             --\n",
      "|    └─GRU: 2-12                         2,365,440\n",
      "├─Sequential: 1-4                        --\n",
      "|    └─Linear: 2-13                      262,656\n",
      "|    └─ReLU: 2-14                        --\n",
      "|    └─Dropout: 2-15                     --\n",
      "|    └─Linear: 2-16                      262,656\n",
      "|    └─ReLU: 2-17                        --\n",
      "|    └─Dropout: 2-18                     --\n",
      "|    └─Linear: 2-19                      131,328\n",
      "|    └─ReLU: 2-20                        --\n",
      "|    └─Dropout: 2-21                     --\n",
      "|    └─Linear: 2-22                      1,542\n",
      "=================================================================\n",
      "Total params: 14,457,031\n",
      "Trainable params: 14,457,031\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─ResNet: 1-1                            --\n",
       "|    └─Conv2d: 2-1                       3,136\n",
       "|    └─BatchNorm2d: 2-2                  128\n",
       "|    └─ReLU: 2-3                         --\n",
       "|    └─MaxPool2d: 2-4                    --\n",
       "|    └─Sequential: 2-5                   --\n",
       "|    |    └─BasicBlock: 3-1              73,984\n",
       "|    |    └─BasicBlock: 3-2              73,984\n",
       "|    └─Sequential: 2-6                   --\n",
       "|    |    └─BasicBlock: 3-3              230,144\n",
       "|    |    └─BasicBlock: 3-4              295,424\n",
       "|    └─Sequential: 2-7                   --\n",
       "|    |    └─BasicBlock: 3-5              919,040\n",
       "|    |    └─BasicBlock: 3-6              1,180,672\n",
       "|    └─Sequential: 2-8                   --\n",
       "|    |    └─BasicBlock: 3-7              3,673,088\n",
       "|    |    └─BasicBlock: 3-8              4,720,640\n",
       "|    └─AdaptiveAvgPool2d: 2-9            --\n",
       "|    └─Identity: 2-10                    --\n",
       "├─AttentionLayer: 1-2                    --\n",
       "|    └─Sequential: 2-11                  --\n",
       "|    |    └─Linear: 3-9                  262,656\n",
       "|    |    └─ReLU: 3-10                   --\n",
       "|    |    └─Linear: 3-11                 513\n",
       "├─BiGRU: 1-3                             --\n",
       "|    └─GRU: 2-12                         2,365,440\n",
       "├─Sequential: 1-4                        --\n",
       "|    └─Linear: 2-13                      262,656\n",
       "|    └─ReLU: 2-14                        --\n",
       "|    └─Dropout: 2-15                     --\n",
       "|    └─Linear: 2-16                      262,656\n",
       "|    └─ReLU: 2-17                        --\n",
       "|    └─Dropout: 2-18                     --\n",
       "|    └─Linear: 2-19                      131,328\n",
       "|    └─ReLU: 2-20                        --\n",
       "|    └─Dropout: 2-21                     --\n",
       "|    └─Linear: 2-22                      1,542\n",
       "=================================================================\n",
       "Total params: 14,457,031\n",
       "Trainable params: 14,457,031\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:51.245147Z",
     "start_time": "2025-01-20T15:46:51.242954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to calculate metrics for multi-label classification\n",
    "def calculate_metrics(preds, labels):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='samples')  # Use 'samples' for multi-label\n",
    "    recall = recall_score(labels, preds, average='samples')  # Use 'samples' for multi-label\n",
    "    f1 = f1_score(labels, preds, average='samples')  # Use 'samples' for multi-label\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Function to print epoch statistics\n",
    "def print_epoch_stats(epoch, num_epochs, phase, loss, metrics):\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], {phase.capitalize()}')\n",
    "    print(f'Loss: {loss:.4f}, '\n",
    "          f'Accuracy: {metrics[\"accuracy\"]:.4f}, Precision: {metrics[\"precision\"]:.4f}, '\n",
    "          f'Recall: {metrics[\"recall\"]:.4f}, F1: {metrics[\"f1\"]:.4f}')"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-20T15:46:51.287105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Ensure the device is set to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Use BCEWithLogitsLoss for multi-label classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Initialize variables to track the best validation performance\n",
    "best_val_acc = 0  # Use infinity as the initial best loss\n",
    "best_model_state_dict = None  # To store the best model's state dict\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Training phase\n",
    "    for imgs, _, labels, multi_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        imgs, multi_labels = imgs.to(device), multi_labels.to(device)  # Move data to GPU\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, multi_labels.float())  # Use multi_labels for loss calculation\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.ge(0.5).float()  # Apply sigmoid and threshold\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(multi_labels.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Concatenate all predictions and labels for metric calculation\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    train_metrics = calculate_metrics(all_preds.numpy(), all_labels.numpy())\n",
    "\n",
    "    print_epoch_stats(epoch, num_epochs, \"train\", avg_loss, train_metrics)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    val_all_preds = []\n",
    "    val_all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for imgs, _, labels, multi_labels in val_loader:\n",
    "            imgs, multi_labels = imgs.to(device), multi_labels.to(device)  # Move data to GPU\n",
    "            outputs = model(imgs)  # Forward pass\n",
    "            loss = criterion(outputs, multi_labels.float())  # Use multi_labels for loss calculation\n",
    "            val_total_loss += loss.item()\n",
    "            preds = outputs.ge(0.5).float()  # Apply sigmoid and threshold\n",
    "            val_all_preds.append(preds.cpu())\n",
    "            val_all_labels.append(multi_labels.cpu())\n",
    "\n",
    "    val_avg_loss = val_total_loss / len(val_loader)\n",
    "\n",
    "    # Concatenate validation predictions and labels for metric calculation\n",
    "    val_all_preds = torch.cat(val_all_preds)\n",
    "    val_all_labels = torch.cat(val_all_labels)\n",
    "\n",
    "    val_metrics = calculate_metrics(val_all_preds.numpy(), val_all_labels.numpy())\n",
    "\n",
    "    print_epoch_stats(epoch, num_epochs, \"validation\", val_avg_loss, val_metrics)\n",
    "\n",
    "    # Check if the current validation accuracy is the best so far\n",
    "    if val_metrics['accuracy'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['accuracy']\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "# After training, load the best model for testing\n",
    "print(f'Best validation accuracy: {best_val_acc:.4f}')\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:44.054277889Z",
     "start_time": "2025-01-20T15:30:28.773056Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention_map(attn_weights, img_shape):\n",
    "    # Assuming attn_weights is of shape (num_heads, seq_length, seq_length)\n",
    "    attn_map = attn_weights.mean(dim=0)  # Average over heads\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(attn_map.detach().cpu().numpy(), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.title('Attention Map')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Step"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T15:46:44.054907856Z",
     "start_time": "2025-01-20T15:31:55.001482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for imgs, _, _, multi_labels in test_loader:\n",
    "        imgs, multi_labels = imgs.to(device), multi_labels.to(device)  # Move data to GPU\n",
    "        outputs = model(imgs)  # Forward pass\n",
    "\n",
    "        # Apply sigmoid and threshold to get binary predictions\n",
    "        predicted = outputs.ge(0.5).float()\n",
    "\n",
    "        # Collect all predictions and labels for computing metrics\n",
    "        all_preds.append(predicted.cpu())\n",
    "        all_labels.append(multi_labels.cpu())\n",
    "\n",
    "# Concatenate all predictions and labels\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Convert to numpy arrays for metric calculation\n",
    "all_preds = all_preds.numpy()\n",
    "all_labels = all_labels.numpy()\n",
    "\n",
    "# Compute subset accuracy (exact match)\n",
    "subset_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# Compute precision, recall, and F1 score (use 'samples' or 'macro' for multi-label)\n",
    "precision = precision_score(all_labels, all_preds, average='samples')\n",
    "recall = recall_score(all_labels, all_preds, average='samples')\n",
    "f1 = f1_score(all_labels, all_preds, average='samples')\n",
    "\n",
    "# Compute Hamming loss (lower is better)\n",
    "hamming_loss_value = hamming_loss(all_labels, all_preds)\n",
    "\n",
    "print(f'Subset Accuracy (Exact Match): {subset_accuracy:.4f}')\n",
    "print(f'Precision (Samples): {precision:.4f}')\n",
    "print(f'Recall (Samples): {recall:.4f}')\n",
    "print(f'F1 Score (Samples): {f1:.4f}')\n",
    "print(f'Hamming Loss: {hamming_loss_value:.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7500\n",
      "Precision: 0.7488\n",
      "Recall: 0.7242\n",
      "F1 Score: 0.7298\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
