{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9651167,"sourceType":"datasetVersion","datasetId":5854070},{"sourceId":9652074,"sourceType":"datasetVersion","datasetId":5705276}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":31820.964733,"end_time":"2024-11-01T17:11:16.418887","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-01T08:20:55.454154","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gpytorch","metadata":{"papermill":{"duration":13.647607,"end_time":"2024-11-01T08:21:11.940842","exception":false,"start_time":"2024-11-01T08:20:58.293235","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:15:59.773906Z","iopub.execute_input":"2024-11-15T10:15:59.774538Z","iopub.status.idle":"2024-11-15T10:16:17.668291Z","shell.execute_reply.started":"2024-11-15T10:15:59.774479Z","shell.execute_reply":"2024-11-15T10:16:17.666874Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gpytorch\n  Downloading gpytorch-1.13-py3-none-any.whl.metadata (8.0 kB)\nCollecting jaxtyping==0.2.19 (from gpytorch)\n  Downloading jaxtyping-0.2.19-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /opt/conda/lib/python3.10/site-packages (from gpytorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from gpytorch) (1.2.2)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from gpytorch) (1.14.1)\nCollecting linear-operator>=0.5.3 (from gpytorch)\n  Downloading linear_operator-0.5.3-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\nRequirement already satisfied: typeguard>=2.13.3 in /opt/conda/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.1 in /opt/conda/lib/python3.10/site-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from linear-operator>=0.5.3->gpytorch) (2.4.0+cpu)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->gpytorch) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->gpytorch) (3.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.5)\nDownloading gpytorch-1.13-py3-none-any.whl (277 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.8/277.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jaxtyping-0.2.19-py3-none-any.whl (24 kB)\nDownloading linear_operator-0.5.3-py3-none-any.whl (176 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jaxtyping, linear-operator, gpytorch\nSuccessfully installed gpytorch-1.13 jaxtyping-0.2.19 linear-operator-0.5.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport random\nimport warnings\nimport copy\nfrom collections import Counter\n\n# Third-party imports\nimport cv2\nimport gpytorch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pydicom\n\n# Machine learning imports\nfrom sklearn.metrics import (\n    auc,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n    roc_curve,\n    f1_score,\n    precision_score,\n    recall_score\n)\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n    Subset,\n)\n\n# Torchvision imports\nfrom torchvision import transforms, models\nfrom torchvision.models import ResNet18_Weights","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:46.081006Z","start_time":"2024-10-15T15:07:41.005714Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":8.567463,"end_time":"2024-11-01T08:21:20.524262","exception":false,"start_time":"2024-11-01T08:21:11.956799","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:17.675224Z","iopub.execute_input":"2024-11-15T10:16:17.675721Z","iopub.status.idle":"2024-11-15T10:16:27.528584Z","shell.execute_reply.started":"2024-11-15T10:16:17.675667Z","shell.execute_reply":"2024-11-15T10:16:27.527424Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(f\"GPUs Available: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"- {torch.cuda.get_device_name(i)}\")\nelse:\n    print(\"No GPUs available.\")","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:46.497429Z","start_time":"2024-10-15T15:07:46.096325Z"},"papermill":{"duration":0.103488,"end_time":"2024-11-01T08:21:20.642258","exception":false,"start_time":"2024-11-01T08:21:20.538770","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.530136Z","iopub.execute_input":"2024-11-15T10:16:27.530865Z","iopub.status.idle":"2024-11-15T10:16:27.538699Z","shell.execute_reply.started":"2024-11-15T10:16:27.530786Z","shell.execute_reply":"2024-11-15T10:16:27.536980Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"No GPUs available.\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nIgnore this warning\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n\"\"\"\nwarnings.filterwarnings('ignore', category=RuntimeWarning, message='os.fork()')","metadata":{"papermill":{"duration":0.021588,"end_time":"2024-11-01T08:21:20.678313","exception":false,"start_time":"2024-11-01T08:21:20.656725","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.542729Z","iopub.execute_input":"2024-11-15T10:16:27.543285Z","iopub.status.idle":"2024-11-15T10:16:27.554181Z","shell.execute_reply.started":"2024-11-15T10:16:27.543229Z","shell.execute_reply":"2024-11-15T10:16:27.552734Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Local","metadata":{"papermill":{"duration":0.013959,"end_time":"2024-11-01T08:21:20.706595","exception":false,"start_time":"2024-11-01T08:21:20.692636","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ## Dataset and Directory Configuration\n# DATASET_NAME = 'rsna-mil-training'\n# JOB_NAME = 'RSNA-ResNet18'\n# HPC_DIR = '/media02/tdhoang01/21127112-21127734/data'\n# OUTPUT_DIR = '/media02/tdhoang01/python-debugging/rsna/results'\n\n# ZIP_FILE_PATH = os.path.join(HPC_DIR, DATASET_NAME + '.zip')\n# CHECKPOINTS_DIR = os.path.join(OUTPUT_DIR, JOB_NAME, 'checkpoints')\n# FIGURES_DIR = os.path.join(OUTPUT_DIR, JOB_NAME, 'figures')\n# DICOM_DIR = f'{DATASET_NAME}/'\n\n# ## File Paths\n# CSV_PATH = f'{DATASET_NAME}/training_1000_scan_subset.csv'\n\n# ## Image Processing Parameters\n# MAX_SLICES = 60\n# HEIGHT = 224\n# WIDTH = 224\n\n# ## Training Hyperparameters\n# BATCH_PATIENTS = 4\n# NUM_EPOCHS = 2\n# LEARNING_RATE = 1e-4\n\n# ## Dataset Split Ratios\n# VAL_SIZE = 0.15\n# TEST_SIZE = 0.15\n\n# ## Create Necessary Directories\n# os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n# os.makedirs(FIGURES_DIR, exist_ok=True)\n\n# ## Load CSVs from zip\n# with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n#     medical_scan_data = pd.read_csv(zip_ref.open(CSV_PATH))","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:47.108742Z","start_time":"2024-10-15T15:07:46.826869Z"},"papermill":{"duration":0.021694,"end_time":"2024-11-01T08:21:20.742479","exception":false,"start_time":"2024-11-01T08:21:20.720785","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.556320Z","iopub.execute_input":"2024-11-15T10:16:27.557298Z","iopub.status.idle":"2024-11-15T10:16:27.568634Z","shell.execute_reply.started":"2024-11-15T10:16:27.557243Z","shell.execute_reply":"2024-11-15T10:16:27.567128Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle","metadata":{"papermill":{"duration":0.014058,"end_time":"2024-11-01T08:21:20.770751","exception":false,"start_time":"2024-11-01T08:21:20.756693","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Configuration\nDATASET_NAME = 'rsna-mil-training'\nJOB_NAME = 'RSNA-ResNet18'\nINPUT_DIR = '/kaggle/input'\nOUTPUT_DIR = '/kaggle/working'\n\n# Directories\nCHECKPOINTS_DIR = os.path.join(OUTPUT_DIR, JOB_NAME, 'checkpoints')\nFIGURES_DIR = os.path.join(OUTPUT_DIR, JOB_NAME, 'figures')\nDICOM_DIR = os.path.join(INPUT_DIR, DATASET_NAME, DATASET_NAME)\n\n# File Paths\nCSV_PATH = f'{DICOM_DIR}/training_1000_scan_subset.csv'\n\n# Image Processing Parameters\nMAX_SLICES = 60\nHEIGHT = 224\nWIDTH = 224\nCHANNELS = 1\n\n# Training Hyperparameters\nBATCH_PATIENTS = 8\nNUM_EPOCHS = 1\nLEARNING_RATE = 1e-4\nTHRESHOLD = 0.52\n\n# Early stopping\nPATIENCE = 10\nMIN_DELTA = 2e-4\n\n# Dataset Split Ratios\nVAL_SIZE = 0.2\nTEST_SIZE = 0.2\n\n# Create Necessary Directories\nos.makedirs(CHECKPOINTS_DIR, exist_ok=True)\nos.makedirs(FIGURES_DIR, exist_ok=True)\n\n# Load CSV from Kaggle environment (no need to unzip)\nmedical_scan_data = pd.read_csv(CSV_PATH)\n\n# Random seed\nSEED = 42\n\n# Augmentation levels\nN_LEVELS = 1","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:47.136336Z","start_time":"2024-10-15T15:07:47.132955Z"},"papermill":{"duration":0.052941,"end_time":"2024-11-01T08:21:20.837915","exception":false,"start_time":"2024-11-01T08:21:20.784974","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.570255Z","iopub.execute_input":"2024-11-15T10:16:27.570783Z","iopub.status.idle":"2024-11-15T10:16:27.631610Z","shell.execute_reply.started":"2024-11-15T10:16:27.570710Z","shell.execute_reply":"2024-11-15T10:16:27.630281Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"papermill":{"duration":0.026311,"end_time":"2024-11-01T08:21:20.878547","exception":false,"start_time":"2024-11-01T08:21:20.852236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.633454Z","iopub.execute_input":"2024-11-15T10:16:27.633995Z","iopub.status.idle":"2024-11-15T10:16:27.652862Z","shell.execute_reply.started":"2024-11-15T10:16:27.633936Z","shell.execute_reply":"2024-11-15T10:16:27.651246Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"medical_scan_data.head(1)","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:47.212661Z","start_time":"2024-10-15T15:07:47.197850Z"},"papermill":{"duration":0.032628,"end_time":"2024-11-01T08:21:20.925327","exception":false,"start_time":"2024-11-01T08:21:20.892699","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.654680Z","iopub.execute_input":"2024-11-15T10:16:27.655210Z","iopub.status.idle":"2024-11-15T10:16:27.684457Z","shell.execute_reply.started":"2024-11-15T10:16:27.655153Z","shell.execute_reply":"2024-11-15T10:16:27.683322Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"    patient_id study_instance_uid  \\\n0  ID_2e010e33      ID_bda0f47e84   \n\n                                              images  \\\n0  ['ID_b9035cb1e.dcm', 'ID_0713bed86.dcm', 'ID_5...   \n\n                                              labels  patient_label  length  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...              0      48  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>study_instance_uid</th>\n      <th>images</th>\n      <th>labels</th>\n      <th>patient_label</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_2e010e33</td>\n      <td>ID_bda0f47e84</td>\n      <td>['ID_b9035cb1e.dcm', 'ID_0713bed86.dcm', 'ID_5...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>0</td>\n      <td>48</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"medical_scan_data.columns","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:47.321026Z","start_time":"2024-10-15T15:07:47.316594Z"},"papermill":{"duration":0.022897,"end_time":"2024-11-01T08:21:20.962618","exception":false,"start_time":"2024-11-01T08:21:20.939721","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.686025Z","iopub.execute_input":"2024-11-15T10:16:27.686629Z","iopub.status.idle":"2024-11-15T10:16:27.695603Z","shell.execute_reply.started":"2024-11-15T10:16:27.686571Z","shell.execute_reply":"2024-11-15T10:16:27.694324Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Index(['patient_id', 'study_instance_uid', 'images', 'labels', 'patient_label',\n       'length'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Split sets in dataframe","metadata":{}},{"cell_type":"code","source":"def split_dataset(medical_scan_data, val_size, test_size, seed):\n    # Calculate total number of samples\n    total_samples = len(medical_scan_data)\n\n    # Calculate the number of samples for validation and testing\n    num_val_samples = int(val_size * total_samples)\n    num_test_samples = int(test_size * total_samples)\n\n    # Split the dataset into training and temp (validation + test)\n    train_data, temp_data = train_test_split(\n        medical_scan_data,\n        test_size=(num_val_samples + num_test_samples) / total_samples,\n        stratify=medical_scan_data['patient_label'],\n        random_state=seed\n    )\n\n    # Now split temp into validation and test sets\n    validate_data, test_data = train_test_split(\n        temp_data,\n        test_size=num_test_samples / (num_val_samples + num_test_samples),\n        stratify=temp_data['patient_label'],\n        random_state=seed\n    )\n\n    return train_data.reset_index(drop=True), validate_data.reset_index(drop=True), test_data.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T10:16:27.697553Z","iopub.execute_input":"2024-11-15T10:16:27.698235Z","iopub.status.idle":"2024-11-15T10:16:27.708394Z","shell.execute_reply.started":"2024-11-15T10:16:27.698178Z","shell.execute_reply":"2024-11-15T10:16:27.707130Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Data augmentation","metadata":{"papermill":{"duration":0.01429,"end_time":"2024-11-01T08:21:20.991283","exception":false,"start_time":"2024-11-01T08:21:20.976993","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DatasetAugmentor:\n    def __init__(self, height, width, n_levels=3, channels=3, seed=None):\n        self.height = height\n        self.width = width\n        self.n_levels = n_levels\n        self.channels = channels\n        self.seed = seed\n        \n        # Create parameters for levels from 0 to n_levels - 1\n        self.params = [self._create_transform(level) for level in range(n_levels)]\n        \n        self.print_params()\n\n    def _sample_value(self, value_range):\n        if isinstance(value_range, tuple):\n            random.seed(self.seed)\n            return random.uniform(value_range[0], value_range[1])\n        return value_range\n\n    def _create_transform(self, level):\n        base_degrees = 15\n        base_translate = (0.2, 0.2)\n        base_scale = (0.9, 1.2)\n        base_brightness = 0.2\n        base_contrast = 0.2\n        base_blur_sigma = (0.1, 0.6)\n\n        scale_factor = (level + 1) / self.n_levels\n\n        sampled_degrees = abs(self._sample_value((-base_degrees * scale_factor, base_degrees * scale_factor)))\n        sampled_translate_x = abs(self._sample_value((base_translate[0] * scale_factor)))\n        sampled_translate_y = abs(self._sample_value((base_translate[1] * scale_factor)))\n        sampled_scale = self._sample_value((\n            base_scale[0] + (base_scale[1] - base_scale[0]) * scale_factor,\n            base_scale[0] + (base_scale[1] - base_scale[0]) * scale_factor\n        ))\n        sampled_brightness = self._sample_value((0, base_brightness * scale_factor))\n        sampled_contrast = self._sample_value((0, base_contrast * scale_factor))        \n        sampled_blur_sigma = self._sample_value((\n            base_blur_sigma[0] + (base_blur_sigma[1] - base_blur_sigma[0]) * scale_factor,\n            base_blur_sigma[0] + (base_blur_sigma[1] - base_blur_sigma[0]) * scale_factor\n        ))\n        \n        return {\n            \"degrees\": sampled_degrees,\n            \"translate\": (sampled_translate_x, sampled_translate_y),\n            \"scale\": sampled_scale,\n            \"brightness\": sampled_brightness,\n            \"contrast\": sampled_contrast,\n            \"blur_sigma\": sampled_blur_sigma,\n            \"apply_elastic\": level > 0\n        }\n\n    def apply_transform(self, image, level):        \n        if self.seed is not None:\n            torch.manual_seed(self.seed)\n            random.seed(self.seed)\n\n        # Check if level is within the valid range of 0 to n_levels - 1\n        if level < 0 or level >= self.n_levels:\n            raise ValueError(f\"Level must be between 0 and {self.n_levels - 1}\")\n\n        params = self.params[level]\n        transform = self._get_transform(params)\n\n        transformed_image = transform(image)        \n        return transformed_image\n    \n    def _get_transform(self, params):\n        normalization_mean_std = {\n            1: ([0.15947446], [0.30001538]), # (brain, sub, soft) = (0.6, 0.2, 0.2)\n            # 3: ([0.16774411, 0.1360026, 0.19076315], [0.3101935, 0.27605791, 0.30469988])\n            3: ([0.16091957, 0.13210903, 0.18250477], [0.31097404, 0.27839983, 0.30436856])\n        }\n\n        mean, std = normalization_mean_std.get(self.channels, ([0], [1]))\n\n        # Create a list of transformations\n        transform_list = [\n            transforms.ToPILImage(),\n#             transforms.RandomHorizontalFlip(p=0.5),\n#             transforms.RandomAffine(degrees=params[\"degrees\"], translate=params[\"translate\"], scale=(params[\"scale\"], params[\"scale\"])),\n#             transforms.ColorJitter(brightness=params[\"brightness\"], contrast=params[\"contrast\"]),\n#             transforms.GaussianBlur(kernel_size=(3, 3), sigma=params[\"blur_sigma\"]),\n#             transforms.RandomApply([transforms.ElasticTransform()] if params[\"apply_elastic\"] else [], p=0.3),\n#             transforms.Resize(256),\n#             transforms.CenterCrop(self.height),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ]\n\n        if self.channels > 1:\n            transform_list.append(transforms.RandomApply([self._channel_shuffle], p=0.3))\n\n        return transforms.Compose(transform_list)\n\n    def _channel_shuffle(self, tensor):\n        torch.manual_seed(self.seed)\n        channels = tensor.shape[0]\n        indices = torch.randperm(channels)\n        return tensor[indices]\n\n    def print_params(self):\n        for i, param in enumerate(self.params):\n            print(f\"Level {i}:\")\n            print(f\"  Degrees: {param['degrees']}\")\n            print(f\"  Translation: (X: {param['translate'][0]}, Y: {param['translate'][1]})\")\n            print(f\"  Scale: {param['scale']}\")\n            print(f\"  Brightness Adjustment: {param['brightness']}\")\n            print(f\"  Contrast Adjustment: {param['contrast']}\")\n            print(f\"  Blur Sigma: {param['blur_sigma']}\")\n            print(f\"  Apply Elastic Transform: {'Yes' if param['apply_elastic'] else 'No'}\")\n            print(\"-\" * 10)","metadata":{"papermill":{"duration":0.038164,"end_time":"2024-11-01T08:21:21.043917","exception":false,"start_time":"2024-11-01T08:21:21.005753","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.710451Z","iopub.execute_input":"2024-11-15T10:16:27.711127Z","iopub.status.idle":"2024-11-15T10:16:27.740718Z","shell.execute_reply.started":"2024-11-15T10:16:27.711085Z","shell.execute_reply":"2024-11-15T10:16:27.739356Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"augmentor = DatasetAugmentor(HEIGHT, WIDTH, seed=SEED, n_levels=N_LEVELS, channels=CHANNELS)","metadata":{"papermill":{"duration":0.022071,"end_time":"2024-11-01T08:21:21.080407","exception":false,"start_time":"2024-11-01T08:21:21.058336","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.742303Z","iopub.execute_input":"2024-11-15T10:16:27.742802Z","iopub.status.idle":"2024-11-15T10:16:27.761553Z","shell.execute_reply.started":"2024-11-15T10:16:27.742758Z","shell.execute_reply":"2024-11-15T10:16:27.760280Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Level 0:\n  Degrees: 4.182803953736514\n  Translation: (X: 0.2, Y: 0.2)\n  Scale: 1.2\n  Brightness Adjustment: 0.12788535969157674\n  Contrast Adjustment: 0.12788535969157674\n  Blur Sigma: 0.6\n  Apply Elastic Transform: No\n----------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Local dataset gen.","metadata":{"papermill":{"duration":0.014955,"end_time":"2024-11-01T08:21:21.109937","exception":false,"start_time":"2024-11-01T08:21:21.094982","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Kaggle dataset gen.","metadata":{"papermill":{"duration":0.014422,"end_time":"2024-11-01T08:21:21.138885","exception":false,"start_time":"2024-11-01T08:21:21.124463","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class MedicalScanDataset(Dataset):\n    def __init__(self, dicom_dir, medical_scan_data, max_slices, height, width, augmentor=None, n_levels=N_LEVELS):\n        self.dicom_dir = dicom_dir\n        self.medical_scan_data = self._parse_medical_scan_data(medical_scan_data)\n        self.max_slices = max_slices\n        self.height = height\n        self.width = width\n        self.augmentor = augmentor\n        self.n_levels = n_levels\n        \n    def __len__(self):\n        return len(self.medical_scan_data) * (self.n_levels if self.augmentor else 1)\n\n    def __getitem__(self, idx):\n        if self.augmentor:\n            base_idx = idx // self.n_levels\n            augment_level = idx % self.n_levels\n        else:\n            base_idx = idx\n            augment_level = None\n\n        row = self.medical_scan_data.iloc[base_idx]\n        dicom_files = self._get_dicom_files(row)\n\n        labels = torch.tensor(row['labels'], dtype=torch.float32)\n        patient_label = torch.tensor(row['patient_label'], dtype=torch.float32)\n\n        images = [self._process_dicom_file(dicom_file) for dicom_file in dicom_files]\n\n        while len(images) < self.max_slices:\n            images.append(torch.zeros(CHANNELS, self.height, self.width))\n        \n        image_tensor = torch.stack(images[:self.max_slices])\n\n        if len(labels) < self.max_slices:\n            label_padding = torch.zeros(self.max_slices - len(labels), dtype=torch.float32)\n            labels = torch.cat((labels, label_padding))\n\n        if augment_level is not None:\n            image_tensor = torch.stack([self.augmentor.apply_transform(image, augment_level) for image in image_tensor])\n\n        return image_tensor, labels, patient_label\n\n    def _parse_medical_scan_data(self, medical_scan_data):\n        medical_scan_data['images'] = medical_scan_data['images'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n        medical_scan_data['labels'] = medical_scan_data['labels'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n        medical_scan_data['patient_label'] = medical_scan_data['patient_label'].astype(bool)\n        return medical_scan_data\n\n    def _get_dicom_files(self, row):\n        \"\"\"Retrieve and sort DICOM files by z-coordinate.\"\"\"\n        patient_id = row['patient_id'].replace(\"ID_\", \"\")\n        study_instance_uid = row['study_instance_uid'].replace(\"ID_\", \"\")\n        dicom_dir_path = os.path.join(self.dicom_dir, f\"{patient_id}_{study_instance_uid}\")\n\n        dicom_files = []\n        if os.path.exists(dicom_dir_path):\n            slices = []\n            for image in row['images']:\n                dicom_file_path = os.path.join(dicom_dir_path, image)\n                if os.path.exists(dicom_file_path):\n                    slices.append(dicom_file_path)\n\n            dicom_files = sorted(slices, key=lambda x: float(pydicom.dcmread(x).ImagePositionPatient[2]) if hasattr(pydicom.dcmread(x), 'ImagePositionPatient') else 0)\n\n        return dicom_files\n\n    def _process_dicom_file(self, dicom_file):\n        \"\"\"Load and preprocess a DICOM file.\"\"\"\n        try:\n            dicom = pydicom.dcmread(dicom_file)\n            img = self._preprocess_slice(dicom)\n            return torch.from_numpy(img).float()\n        except Exception as e:\n            print(f\"Error processing {dicom_file}: {e}\")\n            return torch.zeros(CHANNELS, self.height, self.width)\n\n#     def _preprocess_slice(self, dicom):\n#         \"\"\"Apply windowing to extract relevant image data.\"\"\"\n#         brain_img = self._window_image(dicom, 40, 80)\n#         subdural_img = self._window_image(dicom, 80, 200)\n#         soft_img = self._window_image(dicom, 40, 380)\n\n#         if CHANNELS == 3:\n#             normalized_images = np.array([\n#                 (brain_img - 0) / 80,\n#                 (subdural_img - (-20)) / 200,\n#                 (soft_img - (-150)) / 380\n#             ])\n            \n#         elif CHANNELS == 1:\n#             normalized_images = np.array([\n#                 (brain_img - 0) / 80,\n#             ])\n        \n#         return normalized_images.astype(np.float16)\n\n    def _preprocess_slice(self, dicom):\n        \"\"\"Apply windowing to extract relevant image data.\"\"\"\n        brain_img = self._window_image(dicom, 40, 80)\n        subdural_img = self._window_image(dicom, 80, 200)\n        soft_img = self._window_image(dicom, 40, 380)\n\n        if CHANNELS == 3:\n            normalized_images = np.array([\n                (brain_img - 0) / 80,\n                (subdural_img - (-20)) / 200,\n                (soft_img - (-150)) / 380\n            ])\n\n        elif CHANNELS == 1:\n            weights = np.array([0.6, 0.3, 0.1])\n\n            normalized_images = np.array([\n                brain_img / 80,\n                (subdural_img + 20) / 200,\n                (soft_img + 150) / 380\n            ])\n\n            combined_image = np.sum(normalized_images * weights[:, np.newaxis, np.newaxis], axis=0)\n            return combined_image.astype(np.float16)[np.newaxis, ...]\n        return normalized_images.astype(np.float16)\n\n    def _window_image(self, dcm, window_center, window_width):\n        \"\"\"Apply windowing and resize the DICOM image.\"\"\"\n        img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n        img_min = window_center - window_width // 2\n        img_max = window_center + window_width // 2\n        img = np.clip(img, img_min, img_max)\n        return cv2.resize(img, (self.height, self.width), interpolation=cv2.INTER_LINEAR)\n\n    def calculate_mean_std(self):\n        \"\"\"Calculate mean and standard deviation across dataset.\"\"\"\n        total_sum = np.zeros(CHANNELS)\n        total_squared_sum = np.zeros(CHANNELS)\n        total_count = 0\n\n        for idx in range(len(self.medical_scan_data)):\n            row = self.medical_scan_data.iloc[idx]\n            dicom_files = self._get_dicom_files(row)\n\n            for dicom_file in dicom_files:\n                img = self._process_dicom_file(dicom_file)\n                img_np = img.numpy()\n            \n                if np.all(img_np == 0):\n                    continue\n\n                # Update total count\n                total_count += img_np.shape[1] * img_np.shape[2]\n\n                # Calculate mean and variance for each channel\n                if CHANNELS == 3:\n                    mean = np.mean(img_np, axis=(1, 2))\n                    var = np.var(img_np, axis=(1, 2))\n                    total_sum += mean * (img_np.shape[1] * img_np.shape[2])\n                    total_squared_sum += var * (img_np.shape[1] * img_np.shape[2]) + (mean ** 2) * (img_np.shape[1] * img_np.shape[2])\n                elif CHANNELS == 1:\n                    mean = np.mean(img_np)\n                    var = np.var(img_np)\n                    total_sum += mean * (img_np.shape[1] * img_np.shape[2])\n                    total_squared_sum += var * (img_np.shape[1] * img_np.shape[2]) + (mean ** 2) * (img_np.shape[1] * img_np.shape[2])\n\n        final_mean = total_sum / total_count\n\n        # Calculate variance and standard deviation\n        if CHANNELS == 3:\n            final_var = total_squared_sum / total_count - (final_mean ** 2)\n            final_std = np.sqrt(final_var)\n        else:  # CHANNELS == 1\n            final_var = (total_squared_sum / total_count) - (final_mean ** 2)\n            final_std = np.sqrt(final_var)\n\n        return final_mean, final_std","metadata":{"papermill":{"duration":0.043216,"end_time":"2024-11-01T08:21:21.196789","exception":false,"start_time":"2024-11-01T08:21:21.153573","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-11-15T10:16:27.766656Z","iopub.execute_input":"2024-11-15T10:16:27.767151Z","iopub.status.idle":"2024-11-15T10:16:27.810120Z","shell.execute_reply.started":"2024-11-15T10:16:27.767105Z","shell.execute_reply":"2024-11-15T10:16:27.808938Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class TrainDatasetGenerator(MedicalScanDataset):\n    def __init__(self, dicom_dir, medical_scan_data, max_slices, height, width, augmentor=None, n_levels=N_LEVELS):\n        super().__init__(dicom_dir, medical_scan_data, max_slices, height, width, augmentor=augmentor, n_levels=n_levels)\n\nclass ValidateDatasetGenerator(MedicalScanDataset):\n    def __init__(self, dicom_dir, medical_scan_data, max_slices, height, width, augmentor=None, n_levels=N_LEVELS):\n        super().__init__(dicom_dir, medical_scan_data, max_slices, height, width, augmentor=augmentor, n_levels=n_levels)\n\nclass TestDatasetGenerator(MedicalScanDataset):\n    def __init__(self, dicom_dir, medical_scan_data, max_slices, height, width, augmentor=None, n_levels=N_LEVELS):\n        super().__init__(dicom_dir, medical_scan_data, max_slices, height, width, augmentor=augmentor, n_levels=n_levels)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T10:16:27.811840Z","iopub.execute_input":"2024-11-15T10:16:27.812218Z","iopub.status.idle":"2024-11-15T10:16:27.830230Z","shell.execute_reply.started":"2024-11-15T10:16:27.812179Z","shell.execute_reply":"2024-11-15T10:16:27.828581Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df, validate_df, test_df = split_dataset(medical_scan_data, VAL_SIZE, TEST_SIZE, seed=SEED)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T10:16:27.832793Z","iopub.execute_input":"2024-11-15T10:16:27.833896Z","iopub.status.idle":"2024-11-15T10:16:27.865649Z","shell.execute_reply.started":"2024-11-15T10:16:27.833808Z","shell.execute_reply":"2024-11-15T10:16:27.864069Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Train, Valid, Test generator\n\naugmentor = None for no augmentation","metadata":{}},{"cell_type":"code","source":"# Initialize dataset generators with the respective DataFrames\ntrain_dataset_generator = TrainDatasetGenerator(\n    dicom_dir=DICOM_DIR,\n    medical_scan_data=train_df,\n    max_slices=MAX_SLICES,\n    height=HEIGHT,\n    width=WIDTH,\n    augmentor=None,\n)\n\nvalidate_dataset_generator = ValidateDatasetGenerator(\n    dicom_dir=DICOM_DIR,\n    medical_scan_data=validate_df,\n    max_slices=MAX_SLICES,\n    height=HEIGHT,\n    width=WIDTH,\n    augmentor=None,\n)\n\ntest_dataset_generator = TestDatasetGenerator(\n    dicom_dir=DICOM_DIR,\n    medical_scan_data=test_df,\n    max_slices=MAX_SLICES,\n    height=HEIGHT,\n    width=WIDTH,\n    augmentor=None,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T10:16:27.867196Z","iopub.execute_input":"2024-11-15T10:16:27.867617Z","iopub.status.idle":"2024-11-15T10:16:28.015891Z","shell.execute_reply.started":"2024-11-15T10:16:27.867576Z","shell.execute_reply":"2024-11-15T10:16:28.014488Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Calculate Mean, std","metadata":{}},{"cell_type":"code","source":"medical_scan_data = MedicalScanDataset(\n    dicom_dir=DICOM_DIR,\n    medical_scan_data=medical_scan_data,\n    max_slices=MAX_SLICES,\n    height=HEIGHT,\n    width=WIDTH,\n    augmentor=augmentor,\n)\n\nmean, std = medical_scan_data.calculate_mean_std()\n\nprint(f'{mean}, {std}')","metadata":{"execution":{"iopub.status.busy":"2024-11-15T10:16:28.017626Z","iopub.execute_input":"2024-11-15T10:16:28.018154Z","iopub.status.idle":"2024-11-15T10:31:10.896152Z","shell.execute_reply.started":"2024-11-15T10:16:28.018098Z","shell.execute_reply":"2024-11-15T10:31:10.894248Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[0.15443499], [0.29786261]\n","output_type":"stream"}]},{"cell_type":"code","source":"images, labels, patient_label = train_dataset_generator[0]\nprint(images.shape, labels.shape, patient_label.shape)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T10:31:11.322717Z","iopub.execute_input":"2024-11-15T10:31:11.323491Z","iopub.status.idle":"2024-11-15T10:31:12.085957Z","shell.execute_reply.started":"2024-11-15T10:31:11.323427Z","shell.execute_reply":"2024-11-15T10:31:12.084369Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"torch.Size([60, 1, 224, 224]) torch.Size([60]) torch.Size([])\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(train_dataset_generator), len(validate_dataset_generator), len(test_dataset_generator))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader","metadata":{"papermill":{"duration":0.029626,"end_time":"2024-11-01T08:21:30.518137","exception":false,"start_time":"2024-11-01T08:21:30.488511","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DataloaderManager:\n    def __init__(self,\n                 train_dataset_generator: TrainDatasetGenerator,\n                 validate_dataset_generator: ValidateDatasetGenerator,\n                 test_dataset_generator: TestDatasetGenerator,\n                 batch_size=32):\n\n        self.train_loader = DataLoader(train_dataset_generator, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n        self.validate_loader = DataLoader(validate_dataset_generator, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n        self.test_loader = DataLoader(test_dataset_generator, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:51.439856Z","start_time":"2024-10-15T15:07:51.432637Z"},"papermill":{"duration":0.049976,"end_time":"2024-11-01T08:21:30.597010","exception":false,"start_time":"2024-11-01T08:21:30.547034","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataloader_manager = DataloaderManager(\n   train_dataset_generator=train_dataset_generator,\n   validate_dataset_generator=validate_dataset_generator,\n   test_dataset_generator=test_dataset_generator,\n   batch_size=BATCH_PATIENTS\n)","metadata":{"papermill":{"duration":0.046638,"end_time":"2024-11-01T08:21:30.674967","exception":false,"start_time":"2024-11-01T08:21:30.628329","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison original & augmented","metadata":{}},{"cell_type":"code","source":"def show_original_and_augmented_images(train_df, dicom_dir, max_slices, height, width, augmentor=None, n_levels=N_LEVELS):\n    \"\"\"\n    Display original and augmented images for the first patient in the train DataFrame.\n    \"\"\"\n    first_patient_row = train_df.iloc[0]\n    \n    dataset_no_augmentor = MedicalScanDataset(dicom_dir, train_df, max_slices, height, width, augmentor=None)\n    dataset_with_augmentor = MedicalScanDataset(dicom_dir, train_df, max_slices, height, width, augmentor)\n    \n    original_images, _, _ = dataset_no_augmentor[0]\n    \n    num_rows = n_levels + 1 if augmentor else 2\n    fig, axes = plt.subplots(num_rows, max_slices, figsize=(3 * max_slices, 3 * num_rows))\n    \n    for i in range(max_slices):\n        if i < len(original_images):\n            axes[0, i].imshow(original_images[i].permute(1, 2, 0).numpy())\n            axes[0, i].axis('off')\n            axes[0, i].set_title(f'Original Slice {i+1}')\n    \n    if augmentor:\n        for level in range(n_levels):\n            augmented_images, _, _ = dataset_with_augmentor[level]\n            for i in range(max_slices):\n                if i < len(augmented_images):\n                    axes[level + 1, i].imshow(augmented_images[i].permute(1, 2, 0).numpy())\n                    axes[level + 1, i].axis('off')\n                    axes[level + 1, i].set_title(f'Level {level + 1} Slice {i+1}')\n    \n    plt.suptitle(f'Patient {first_patient_row[\"patient_id\"]}: Original and Augmented Images with {n_levels} Levels')\n    plt.tight_layout()\n    plt.savefig('test.png')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_original_and_augmented_images(train_df=train_df, dicom_dir=DICOM_DIR, max_slices=10, height=HEIGHT, width=WIDTH, augmentor=augmentor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model configurations","metadata":{"papermill":{"duration":0.028425,"end_time":"2024-11-01T08:21:30.810014","exception":false,"start_time":"2024-11-01T08:21:30.781589","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## GP","metadata":{}},{"cell_type":"code","source":"class GPModel(gpytorch.models.ApproximateGP):\n    def __init__(self, inducing_points):\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(GPModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        assert x.dim() == 2 and x.size(1) == 512, f\"Expected input shape [*, 512] but got {x.shape}\"\n        \n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)","metadata":{"papermill":{"duration":0.040094,"end_time":"2024-11-01T08:21:30.878920","exception":false,"start_time":"2024-11-01T08:21:30.838826","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Attention","metadata":{}},{"cell_type":"code","source":"class AttentionLayer(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.PReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, x):\n        assert x.dim() == 3 and x.size(2) == 512, f\"Expected input shape [batch_size, num_instances, 512] but got {x.shape}\"\n        \n        attention_weights = self.attention(x)\n        weights = nn.functional.softmax(attention_weights, dim=1)\n\n        return (x * weights).sum(dim=1), weights.squeeze(-1)","metadata":{"papermill":{"duration":0.038353,"end_time":"2024-11-01T08:21:30.946230","exception":false,"start_time":"2024-11-01T08:21:30.907877","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ResNet","metadata":{}},{"cell_type":"code","source":"# 2. Model Definition\nclass ResNet18(nn.Module):\n    def __init__(self, num_classes=1):\n        super(ResNet18, self).__init__()\n        self.model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n        self.model.conv1 = nn.Conv2d(CHANNELS, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n        self.model.fc = nn.Identity()\n        self.dropout = nn.Dropout(p=0.1)\n        self.attention = AttentionLayer(input_dim=512, hidden_dim=512)\n        \n        self.inducing_points = torch.randn(64, 512)\n        self.gp_layer = GPModel(self.inducing_points)\n        \n        self.fc = nn.Linear(512 + 1, 1)\n        \n    def forward(self, x):\n        assert x.dim() == 5 and x.size(1) == MAX_SLICES and x.size(2) == CHANNELS and x.size(3) == HEIGHT and x.size(4) == WIDTH, \\\n            f\"Expected input shape [BATCH_PATIENTS={BATCH_PATIENTS}, MAX_SLICES={MAX_SLICES}, channels={CHANNELS}, height={HEIGHT}, width={WIDTH}] but got {x.shape}\"\n        \n        batch_size, bag_size = x.size(0), x.size(1)\n        \n        # Reshape to (batch_size * bag_size, channels, height, width)\n        x = x.view(-1, *x.shape[2:])\n        \n        features = self.model(x)\n\n        assert features.shape[1] == 512, f\"Expected features to have shape [*, 512] but got {features.shape}\"\n        \n        # Reshape back to (batch_size, bag_size, feature_dim)\n        features = features.view(batch_size, bag_size, -1)\n        \n        attended_features, attended_weights = self.attention(features)\n\n        assert attended_features.shape[1] == 512 and attended_weights.shape[0] == batch_size and attended_weights.shape[1] == bag_size,\\\n            f\"Expected attended_features shape [batch_size={batch_size}, feature_dim=512] but got {attended_features.shape} \" \\\n            f\"and attended_weights shape [batch_size={batch_size}, bag_size={bag_size}] but got {attended_weights.shape}\"\n\n        attended_features_reshaped = attended_features.view(-1, 512)\n\n        gp_output = self.gp_layer(attended_features_reshaped)\n        \n        gp_mean = gp_output.mean.view(batch_size, -1)\n\n        combine_features = torch.cat((attended_features, gp_mean), dim=1)\n        \n        return self.dropout(self.fc(combine_features)), attended_weights","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:52.586835Z","start_time":"2024-10-15T15:07:51.642585Z"},"papermill":{"duration":0.042983,"end_time":"2024-11-01T08:21:31.017694","exception":false,"start_time":"2024-11-01T08:21:30.974711","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\nmodel = ResNet18().to(device)","metadata":{"papermill":{"duration":0.79266,"end_time":"2024-11-01T08:21:31.839235","exception":false,"start_time":"2024-11-01T08:21:31.046575","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss and optimizer","metadata":{}},{"cell_type":"code","source":"# 3. Loss and Optimizer\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2)\n\n# mode: Monitors 'min' or 'max' changes in metrics.\n# factor: The multiplicative factor for reducing the learning rate. \n##  If the current learning rate is 0.01 and factor=0.5, the new learning rate will be 0.01 * 0.5 = 0.005\n# patience: Number of epochs to wait for improvement before reducing the learning rate.","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:52.616483Z","start_time":"2024-10-15T15:07:52.612038Z"},"papermill":{"duration":0.039461,"end_time":"2024-11-01T08:21:31.910163","exception":false,"start_time":"2024-11-01T08:21:31.870702","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combined_loss(output, gp_distribution, label, alpha=0.5):\n    bce_loss = nn.BCEWithLogitsLoss()(output, label.float())\n    kl_divergence = gp_distribution.variational_strategy.kl_divergence()\n    total_loss = (1 - alpha) * bce_loss + alpha * kl_divergence\n    \n    return total_loss","metadata":{"papermill":{"duration":0.036715,"end_time":"2024-11-01T08:21:31.975926","exception":false,"start_time":"2024-11-01T08:21:31.939211","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train, valid, test functions","metadata":{}},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer, scheduler, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    all_labels = []\n    all_predictions = []\n\n    for batch_idx, (image, _, label) in enumerate(train_loader):\n        image, label = image.to(device), label.to(device)\n\n        # Forward pass\n        optimizer.zero_grad()\n        \n        outputs, attention_weights = model(image)\n        output = outputs.squeeze()\n        label = label.reshape(-1)\n        \n        loss = combined_loss(output, model.gp_layer, label)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Track loss\n        running_loss += loss.item()\n\n        # Calculate accuracy\n        predicted_label = (output > THRESHOLD).float()\n        total += label.size(0)\n        correct += (predicted_label == label).sum().item()\n\n        # Collect predictions and labels for metrics\n        all_labels.extend(label.cpu().numpy())\n        all_predictions.extend(predicted_label.cpu().numpy())\n\n    # Calculate metrics\n    epoch_loss = running_loss / len(train_loader)\n    accuracy = 100 * correct / total\n    f1 = f1_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions)\n    recall = recall_score(all_labels, all_predictions)\n\n    print(f'Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}%, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n\n    scheduler.step(epoch_loss)\n    return epoch_loss","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:52.680223Z","start_time":"2024-10-15T15:07:52.670633Z"},"papermill":{"duration":0.040535,"end_time":"2024-11-01T08:21:32.045439","exception":false,"start_time":"2024-11-01T08:21:32.004904","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, validate_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    all_labels = []\n    all_predictions = []\n\n    with torch.inference_mode():\n        for batch_idx, (image, _, label) in enumerate(validate_loader):\n            image, label = image.to(device), label.to(device)\n            \n            # Forward pass\n            outputs, attention_weights = model(image)\n            output = outputs.squeeze()\n            label = label.reshape(-1)\n            \n            loss = combined_loss(output, model.gp_layer, label)\n            running_loss += loss.item()\n            \n            # Calculate accuracy\n            predicted_label = (output > THRESHOLD).float()\n            total += label.size(0)\n            correct += (predicted_label == label).sum().item()\n\n            # Collect predictions and labels for metrics\n            all_labels.extend(label.cpu().numpy())\n            all_predictions.extend(predicted_label.cpu().numpy())\n    \n    # Calculate metrics\n    val_loss = running_loss / len(validate_loader)\n    accuracy = 100 * correct / total\n    f1 = f1_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions)\n    recall = recall_score(all_labels, all_predictions)\n    \n    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.4f}%, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n    \n    return val_loss","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:52.742433Z","start_time":"2024-10-15T15:07:52.734550Z"},"papermill":{"duration":0.040294,"end_time":"2024-11-01T08:21:32.114909","exception":false,"start_time":"2024-11-01T08:21:32.074615","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, scheduler, epoch, loss):\n    checkpoint_path = os.path.join(CHECKPOINTS_DIR, f'checkpoint_epoch_{epoch+1}.pth')    \n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'loss': loss\n    }, checkpoint_path)\n    print(f\"Checkpoint saved: {checkpoint_path}\")","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:52.801793Z","start_time":"2024-10-15T15:07:52.796322Z"},"papermill":{"duration":0.038384,"end_time":"2024-11-01T08:21:32.182337","exception":false,"start_time":"2024-11-01T08:21:32.143953","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_best_checkpoint(model, optimizer, scheduler, checkpoint_dir):\n    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_') and f.endswith('.pth')]\n    if not checkpoint_files:\n        print(\"No checkpoints found.\")\n        return None, None\n\n    best_checkpoint = min(checkpoint_files, key=lambda x: float(x.split('_')[-1].split('.')[0]))\n    checkpoint_path = os.path.join(checkpoint_dir, best_checkpoint)\n    \n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=torch.device(device), weights_only=True)\n    except RuntimeError as e:\n        print(f\"Error loading checkpoint: {e}\")\n        print(f\"Checkpoint file: {checkpoint_path}\")\n        print(f\"File size: {os.path.getsize(checkpoint_path)} bytes\")\n        return None, None\n\n    try:\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        epoch = checkpoint['epoch']\n        best_val_loss = checkpoint['loss']\n        \n        print(f\"Loaded checkpoint from epoch {epoch+1} with validation loss {best_val_loss}\")\n        \n        return epoch, best_val_loss\n    \n    except KeyError as e:\n        print(f\"Error: Checkpoint file is missing expected data: {e}\")\n        print(f\"Available keys in checkpoint: {checkpoint.keys()}\")\n        return None, None","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:53.165742Z","start_time":"2024-10-15T15:07:53.159773Z"},"papermill":{"duration":0.042261,"end_time":"2024-11-01T08:21:32.253740","exception":false,"start_time":"2024-11-01T08:21:32.211479","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(y_true, y_scores):\n    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n    roc_auc = auc(fpr, tpr)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc='lower right')\n    plt.savefig(os.path.join(FIGURES_DIR, 'roc_curve.png'))\n    plt.show()\n    plt.close()","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:53.235969Z","start_time":"2024-10-15T15:07:53.227459Z"},"papermill":{"duration":0.039161,"end_time":"2024-11-01T08:21:32.321983","exception":false,"start_time":"2024-11-01T08:21:32.282822","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    disp.plot(cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.savefig(os.path.join(FIGURES_DIR, 'confusion_matrix.png'))\n    plt.show()\n    plt.close()","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:53.598335Z","start_time":"2024-10-15T15:07:53.594214Z"},"papermill":{"duration":0.037225,"end_time":"2024-11-01T08:21:32.388582","exception":false,"start_time":"2024-11-01T08:21:32.351357","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_final_model(model, optimizer, scheduler, final_model_path):\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n    }, final_model_path)\n    print(f\"Final model saved to {final_model_path}\")","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:53.657149Z","start_time":"2024-10-15T15:07:53.651664Z"},"papermill":{"duration":0.037325,"end_time":"2024-11-01T08:21:32.455452","exception":false,"start_time":"2024-11-01T08:21:32.418127","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_loader, device, criterion):\n    model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    all_labels = []\n    all_predictions = []\n    all_scores = []\n    test_loss = 0\n\n    with torch.inference_mode():\n        for image, _, label in test_loader:\n            image, label = image.to(device), label.to(device)\n            \n            outputs, attention_weights = model(image)\n            output = outputs.squeeze()\n            label = label.reshape(-1)\n            \n            loss = combined_loss(output, model.gp_layer, label)\n            test_loss += loss.item()\n            \n            predicted_label = (output > THRESHOLD).float()\n            total += label.size(0)\n            correct += (predicted_label == label).sum().item()\n            \n            # Collect predictions and labels for metrics\n            all_labels.extend(label.cpu().numpy())\n            all_predictions.extend(predicted_label.cpu().numpy())\n            all_scores.extend(output.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = 100 * correct / total\n    avg_loss = test_loss / len(test_loader)\n    f1 = f1_score(all_labels, all_predictions)\n    precision = precision_score(all_labels, all_predictions)\n    recall = recall_score(all_labels, all_predictions)\n    \n    print(f'Test Accuracy: {accuracy:.4f}%, Test Loss: {avg_loss:.4f}')\n    print(f'Test F1 Score: {f1:.4f}, Test Precision: {precision:.4f}, Test Recall: {recall:.4f}')\n    \n    # Plot metrics\n    plot_roc_curve(np.array(all_labels), np.array(all_scores))\n    plot_confusion_matrix(np.array(all_labels), np.array(all_predictions))\n\n    return accuracy, avg_loss","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:07:53.997995Z","start_time":"2024-10-15T15:07:53.983343Z"},"papermill":{"duration":0.041491,"end_time":"2024-11-01T08:21:32.525852","exception":false,"start_time":"2024-11-01T08:21:32.484361","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{"papermill":{"duration":0.029182,"end_time":"2024-11-01T08:21:32.583878","exception":false,"start_time":"2024-11-01T08:21:32.554696","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Early stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=PATIENCE, min_delta=MIN_DELTA):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float('inf')\n        self.early_stop = False\n\n    def __call__(self, val_loss):\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n\n        if self.counter >= self.patience:\n            self.early_stop = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train result","metadata":{"papermill":{"duration":0.029004,"end_time":"2024-11-01T08:21:32.642072","exception":false,"start_time":"2024-11-01T08:21:32.613068","status":"completed"},"tags":[]}},{"cell_type":"code","source":"loaded_epoch, loaded_best_val_loss = load_best_checkpoint(model, optimizer, scheduler, CHECKPOINTS_DIR)\n\nif loaded_epoch is not None:\n    start_epoch = loaded_epoch + 1\n    best_val_loss = loaded_best_val_loss\nelse:\n    start_epoch = 0\n    best_val_loss = float('inf')\n\ntrain_loader = dataloader_manager.train_loader\nvalidate_loader = dataloader_manager.validate_loader\ntest_loader = dataloader_manager.test_loader\n    \nearly_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)\n    \nfor epoch in range(start_epoch, NUM_EPOCHS):\n    train_loss = train(model, train_loader, criterion, optimizer, scheduler, device)\n    val_loss = validate(model, validate_loader, criterion, device)\n    \n    early_stopping(val_loss)\n    \n    if early_stopping.early_stop:\n        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n        break\n    \n    if val_loss < best_val_loss:\n        save_checkpoint(model, optimizer, scheduler, epoch, val_loss)\n        best_val_loss = val_loss","metadata":{"papermill":{"duration":31613.187832,"end_time":"2024-11-01T17:08:25.859197","exception":false,"start_time":"2024-11-01T08:21:32.671365","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate result","metadata":{"papermill":{"duration":0.034193,"end_time":"2024-11-01T17:08:25.927762","exception":false,"start_time":"2024-11-01T17:08:25.893569","status":"completed"},"tags":[]}},{"cell_type":"code","source":"accuracy, test_loss = evaluate(model, test_loader, device, criterion)\n\nfinal_model_path = os.path.join(CHECKPOINTS_DIR, 'final_model.pth')\nsave_final_model(model, optimizer, scheduler, final_model_path)","metadata":{"ExecuteTime":{"end_time":"2024-10-15T15:18:44.867379Z","start_time":"2024-10-15T15:17:49.586291Z"},"papermill":{"duration":135.877812,"end_time":"2024-11-01T17:10:41.839836","exception":false,"start_time":"2024-11-01T17:08:25.962024","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot with attention","metadata":{"papermill":{"duration":0.035659,"end_time":"2024-11-01T17:10:41.914793","exception":false,"start_time":"2024-11-01T17:10:41.879134","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def plot_images_with_attention(test_loader, model, device):\n    model.eval()\n    found_labels = {0: False, 1: False}\n    with torch.no_grad():\n        for image, _, label in test_loader:\n            image, label = image.to(device), label.to(device)\n            outputs, attention_weights = model(image)\n            attention_weights = attention_weights.view(image.size(0), -1)\n            for i in range(image.size(0)):\n                patient_label = label[i].item()\n                \n                if patient_label in [0, 1] and not found_labels[patient_label]:\n                    found_labels[patient_label] = True\n                    \n                    plt.figure(figsize=(16, 8))\n                    fig, axes = plt.subplots(10, 6, figsize=(16, 24))\n                    for j in range(MAX_SLICES):\n                        if j < image.size(1):\n                            ax = axes[j // 6, j % 6]\n                            ax.imshow(image[i, j].cpu().numpy().transpose(1, 2, 0))\n                            ax.axis('off')\n                            ax.set_title(f'Attention: {attention_weights[i][j].item():.4f}', fontsize=10)\n                    fig.suptitle(f'Patient Label: {patient_label}', fontsize=16)\n                    fig.subplots_adjust(top=0.9)\n                    plt.show()\n                if all(found_labels.values()):\n                    return","metadata":{"papermill":{"duration":0.049806,"end_time":"2024-11-01T17:10:41.999730","exception":false,"start_time":"2024-11-01T17:10:41.949924","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images_with_attention(test_loader, model, device)","metadata":{"papermill":{"duration":30.897167,"end_time":"2024-11-01T17:11:12.932564","exception":false,"start_time":"2024-11-01T17:10:42.035397","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}