{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 9596701,
     "sourceType": "datasetVersion",
     "datasetId": 5854070
    },
    {
     "sourceId": 9546002,
     "sourceType": "datasetVersion",
     "datasetId": 5705276
    }
   ],
   "dockerImageVersionId": 30787,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Third-party imports\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "# Torchvision imports\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet18_Weights"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:09.379210Z",
     "iopub.execute_input": "2024-10-11T18:09:09.379674Z",
     "iopub.status.idle": "2024-10-11T18:09:15.813031Z",
     "shell.execute_reply.started": "2024-10-11T18:09:09.379608Z",
     "shell.execute_reply": "2024-10-11T18:09:15.812227Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:06.813163Z",
     "start_time": "2024-10-13T17:23:02.866826Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:06.860451Z",
     "start_time": "2024-10-13T17:23:06.819165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"GPUs Available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"- {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: 1\n",
      "- NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "## Dataset and Directory Configuration\n",
    "DATASET_NAME = 'rsna-cnn-training'\n",
    "# HPC_DIR = '/media02/tdhoang01/21127112-21127734/data'\n",
    "# OUTPUT_DIR = '/media02/tdhoang01/python-debugging/rsna/results'\n",
    "\n",
    "HPC_DIR = 'D:/'\n",
    "OUTPUT_DIR = 'C:/Users/MINH/Downloads/Brain-Stroke-Diagnosis/rsna/results'\n",
    "\n",
    "ZIP_FILE_PATH = os.path.join(HPC_DIR, DATASET_NAME + '.zip')\n",
    "CHECKPOINTS_DIR = os.path.join(OUTPUT_DIR, 'checkpoints')\n",
    "FIGURES_DIR = os.path.join(OUTPUT_DIR, 'figures')\n",
    "DICOM_DIR = f'{DATASET_NAME}/'\n",
    "\n",
    "## File Paths\n",
    "CSV_PATH = f'{DATASET_NAME}/training_20_scan_subset.csv'\n",
    "SLICE_LABEL_PATH = 'sorted_training_dataset_with_labels.csv'\n",
    "\n",
    "## Image Processing Parameters\n",
    "MAX_SLICES = 60\n",
    "HEIGHT = 224\n",
    "WIDTH = 224\n",
    "\n",
    "## Training Hyperparameters\n",
    "BATCH_PATIENTS = 8\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "## Dataset Split Ratios\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15\n",
    "\n",
    "## Target Columns\n",
    "TARGET_COLUMNS = [\n",
    "    'any',\n",
    "    'epidural',\n",
    "    'intraparenchymal',\n",
    "    'intraventricular',\n",
    "    'subarachnoid',\n",
    "    'subdural'\n",
    "]\n",
    "\n",
    "## Create Necessary Directories\n",
    "os.makedirs(CHECKPOINTS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "## Load CSVs from zip\n",
    "with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
    "    patient_scan_labels = pd.read_csv(zip_ref.open(CSV_PATH))\n",
    "    patient_slice_labels = pd.read_csv(zip_ref.open(SLICE_LABEL_PATH))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:15.814689Z",
     "iopub.execute_input": "2024-10-11T18:09:15.815115Z",
     "iopub.status.idle": "2024-10-11T18:09:20.329455Z",
     "shell.execute_reply.started": "2024-10-11T18:09:15.815082Z",
     "shell.execute_reply": "2024-10-11T18:09:20.328536Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:08.929321Z",
     "start_time": "2024-10-13T17:23:07.209608Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "patient_scan_labels.head(1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:20.339498Z",
     "iopub.execute_input": "2024-10-11T18:09:20.339815Z",
     "iopub.status.idle": "2024-10-11T18:09:20.375537Z",
     "shell.execute_reply.started": "2024-10-11T18:09:20.339784Z",
     "shell.execute_reply": "2024-10-11T18:09:20.374624Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:08.960319Z",
     "start_time": "2024-10-13T17:23:08.945320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    patient_id study_instance_uid  any  epidural  intraparenchymal  \\\n",
       "0  ID_0002cd41      ID_66929e09d4    0         0                 0   \n",
       "\n",
       "   intraventricular  subarachnoid  subdural  \n",
       "0                 0             0         0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_0002cd41</td>\n",
       "      <td>ID_66929e09d4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "patient_slice_labels.head(1)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:20.377129Z",
     "iopub.execute_input": "2024-10-11T18:09:20.377456Z",
     "iopub.status.idle": "2024-10-11T18:09:20.396076Z",
     "shell.execute_reply.started": "2024-10-11T18:09:20.377421Z",
     "shell.execute_reply": "2024-10-11T18:09:20.394910Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:09.023334Z",
     "start_time": "2024-10-13T17:23:09.010331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           filename  any  epidural  intraparenchymal  intraventricular  \\\n",
       "0  ID_45785016b.dcm    0         0                 0                 0   \n",
       "\n",
       "   subarachnoid  subdural   patient_id study_instance_uid series_instance_uid  \\\n",
       "0             0         0  ID_0002cd41      ID_66929e09d4       ID_e22a5534e6   \n",
       "\n",
       "                 image_position  samples_per_pixel         pixel_spacing  \\\n",
       "0  [-125.000, -122.596, 35.968]                  1  [0.488281, 0.488281]   \n",
       "\n",
       "   pixel_representation window_center window_width  rescale_intercept  \\\n",
       "0                     1            30           80            -1024.0   \n",
       "\n",
       "   rescale_slope         ID  \n",
       "0            1.0  45785016b  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>study_instance_uid</th>\n",
       "      <th>series_instance_uid</th>\n",
       "      <th>image_position</th>\n",
       "      <th>samples_per_pixel</th>\n",
       "      <th>pixel_spacing</th>\n",
       "      <th>pixel_representation</th>\n",
       "      <th>window_center</th>\n",
       "      <th>window_width</th>\n",
       "      <th>rescale_intercept</th>\n",
       "      <th>rescale_slope</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_45785016b.dcm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_0002cd41</td>\n",
       "      <td>ID_66929e09d4</td>\n",
       "      <td>ID_e22a5534e6</td>\n",
       "      <td>[-125.000, -122.596, 35.968]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.488281, 0.488281]</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>-1024.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45785016b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:09.101369Z",
     "start_time": "2024-10-13T17:23:09.072356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DatasetGenerator(Dataset):\n",
    "    def __init__(self, zip_file_path, patient_scan_labels, patient_slice_labels, max_slices, height, width, target_columns):\n",
    "        self.zip_file_path = zip_file_path\n",
    "        self.patient_scan_labels = patient_scan_labels\n",
    "        self.patient_slice_labels = patient_slice_labels\n",
    "        self.max_slices = max_slices\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.target_columns = target_columns\n",
    "        self.channels = 3\n",
    "        self.dicom_paths = self._get_dicom_paths()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((self.height, self.width))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dicom_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dicom_files, _ = self.dicom_paths[idx]\n",
    "        images, labels = self._process_dicom_files(dicom_files)\n",
    "        return self._pad_data(images, labels)\n",
    "\n",
    "    def _get_dicom_paths(self):\n",
    "        dicom_paths = []\n",
    "        with zipfile.ZipFile(self.zip_file_path, 'r') as dicom_zip:\n",
    "            zip_file_list = dicom_zip.namelist()\n",
    "            top_level_folder = f\"{DATASET_NAME}/\" # 1000\n",
    "\n",
    "            for _, row in self.patient_scan_labels.iterrows():\n",
    "                patient_id = row['patient_id'].replace(\"ID_\", \"\")\n",
    "                study_instance_uid = row['study_instance_uid'].replace(\"ID_\", \"\")\n",
    "                dicom_dir_path = f\"{top_level_folder}{patient_id}_{study_instance_uid}/\"\n",
    "                dicom_files = [f for f in zip_file_list if f.startswith(dicom_dir_path) and f.endswith(\".dcm\")]\n",
    "                \n",
    "                if dicom_files:\n",
    "                    dicom_paths.append((dicom_files, row))\n",
    "                else:\n",
    "                    print(f\"No DICOM files found in {dicom_dir_path} within the zip file.\")\n",
    "        \n",
    "        return dicom_paths\n",
    "\n",
    "    def _process_dicom_files(self, dicom_files):\n",
    "        images = []\n",
    "        labels = []\n",
    "        with zipfile.ZipFile(self.zip_file_path, 'r') as dicom_zip:\n",
    "            for dicom_file in dicom_files:\n",
    "                with dicom_zip.open(dicom_file) as file:\n",
    "                    dicom = pydicom.dcmread(file)\n",
    "                    img = self._preprocess_slice(dicom)\n",
    "                    images.append(torch.from_numpy(img).float())\n",
    "                    labels.append(self._get_label(dicom_file))\n",
    "        \n",
    "        return torch.stack(images), torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def _preprocess_slice(self, dicom):\n",
    "        bsb_img = self._bsb_window(dicom)\n",
    "        return bsb_img.astype(np.float16)\n",
    "\n",
    "    def _get_label(self, dicom_file):\n",
    "        file_key = os.path.basename(dicom_file)\n",
    "        label_row = self.patient_slice_labels[self.patient_slice_labels['filename'] == file_key]\n",
    "        return 1.0 if not label_row.empty and np.any(label_row[self.target_columns].values == 1) else 0.0\n",
    "\n",
    "    def _pad_data(self, images, labels):\n",
    "        if images.shape[0] < self.max_slices:\n",
    "            padding = torch.zeros((self.max_slices - images.shape[0], self.channels, self.height, self.width))\n",
    "            images = torch.cat((images, padding), dim=0)\n",
    "            label_padding = torch.zeros(self.max_slices - labels.shape[0])\n",
    "            labels = torch.cat((labels, label_padding))\n",
    "        return images, labels\n",
    "\n",
    "    def _correct_dcm(self, dcm):\n",
    "        x = dcm.pixel_array + 1000\n",
    "        px_mode = 4096\n",
    "        x[x >= px_mode] -= px_mode\n",
    "        dcm.PixelData = x.tobytes()\n",
    "        dcm.RescaleIntercept = -1000\n",
    "\n",
    "    def _window_image(self, dcm, window_center, window_width):\n",
    "        if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "            self._correct_dcm(dcm)\n",
    "        \n",
    "        img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "        img = cv2.resize(img, (self.height, self.width), interpolation=cv2.INTER_LINEAR)\n",
    "       \n",
    "        img_min = window_center - window_width // 2\n",
    "        img_max = window_center + window_width // 2\n",
    "        img = np.clip(img, img_min, img_max)\n",
    "        \n",
    "        return img\n",
    "\n",
    "    def _bsb_window(self, dcm):\n",
    "        brain_img = self._window_image(dcm, 40, 80)\n",
    "        subdural_img = self._window_image(dcm, 80, 200)\n",
    "        soft_img = self._window_image(dcm, 40, 380)\n",
    "        \n",
    "        brain_img = (brain_img - 0) / 80\n",
    "        subdural_img = (subdural_img - (-20)) / 200\n",
    "        soft_img = (soft_img - (-150)) / 380\n",
    "        \n",
    "        bsb_img = np.stack([brain_img, subdural_img, soft_img], axis=0)\n",
    "        return bsb_img"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "class DataloaderManager:\n",
    "    def __init__(self, dataset, batch_size, val_size, test_size, num_workers=4):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_loader = None\n",
    "        self.validate_loader = None\n",
    "        self.test_loader = None\n",
    "        self._create_loaders()\n",
    "\n",
    "    def print_dataset_length(self):\n",
    "        print(f\"Length of dataset: {len(self.dataset)}\")\n",
    "\n",
    "    def print_lengths(self):\n",
    "        print(f\"Length of training dataset: {len(self.train_loader.dataset)}\")\n",
    "        print(f\"Length of validation dataset: {len(self.validate_loader.dataset)}\")\n",
    "        print(f\"Length of testing dataset: {len(self.test_loader.dataset)}\")\n",
    "    \n",
    "    # def _create_loaders(self):\n",
    "    #     dataset_length = len(self.dataset)\n",
    "    #     print(f\"Dataset length: {dataset_length}\")\n",
    "    # \n",
    "    #     # Create masks for indices with and without label 1\n",
    "    #     label_mask = np.array([1 in sample[1].tolist() for sample in self.dataset])\n",
    "    #     indices_with_one = np.where(label_mask)[0]\n",
    "    #     indices_without_one = np.where(~label_mask)[0]\n",
    "    # \n",
    "    #     print(f\"Total indices with one: {len(indices_with_one)}\")\n",
    "    # \n",
    "    #     # Calculate split sizes\n",
    "    #     val_size = int(dataset_length * self.val_size)\n",
    "    #     test_size = int(dataset_length * self.test_size)\n",
    "    # \n",
    "    #     # Shuffle and split indices\n",
    "    #     np.random.shuffle(indices_with_one)\n",
    "    #     np.random.shuffle(indices_without_one)\n",
    "    # \n",
    "    #     val_one_count = int(len(indices_with_one) * self.val_size)\n",
    "    #     test_one_count = int(len(indices_with_one) * self.test_size)\n",
    "    # \n",
    "    #     val_indices = np.concatenate((\n",
    "    #         indices_with_one[:val_one_count],\n",
    "    #         indices_without_one[:val_size - val_one_count]\n",
    "    #     ))\n",
    "    # \n",
    "    #     test_indices = np.concatenate((\n",
    "    #         indices_with_one[val_one_count:val_one_count + test_one_count],\n",
    "    #         indices_without_one[val_size - val_one_count:val_size - val_one_count + test_size - test_one_count]\n",
    "    #     ))\n",
    "    # \n",
    "    #     train_indices = np.concatenate((\n",
    "    #         indices_with_one[val_one_count + test_one_count:],\n",
    "    #         indices_without_one[val_size - val_one_count + test_size - test_one_count:]\n",
    "    #     ))\n",
    "    # \n",
    "    #     # Create datasets from selected indices using Subset\n",
    "    #     train_dataset = Subset(self.dataset, train_indices)\n",
    "    #     val_dataset = Subset(self.dataset, val_indices)\n",
    "    #     test_dataset = Subset(self.dataset, test_indices)\n",
    "    # \n",
    "    #     # Create DataLoaders for each set\n",
    "    #     self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    #     self.validate_loader = DataLoader(val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    #     self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    \n",
    "    def _create_loaders(self):\n",
    "        dataset_length = len(self.dataset)\n",
    "\n",
    "        # Create masks for indices with and without label 1\n",
    "        label_mask = np.array([1 in self.dataset.patient_scan_labels.iloc[i][self.dataset.target_columns].values.tolist() for i in range(dataset_length)])\n",
    "        indices_with_one = np.where(label_mask)[0]\n",
    "        indices_without_one = np.where(~label_mask)[0]\n",
    "\n",
    "        # Calculate split sizes\n",
    "        val_size = int(dataset_length * self.val_size)\n",
    "        test_size = int(dataset_length * self.test_size)\n",
    "\n",
    "        # Shuffle and split indices\n",
    "        np.random.shuffle(indices_with_one)\n",
    "        np.random.shuffle(indices_without_one)\n",
    "\n",
    "        val_one_count = int(len(indices_with_one) * self.val_size)\n",
    "        test_one_count = int(len(indices_with_one) * self.test_size)\n",
    "\n",
    "        val_indices = np.concatenate((\n",
    "            indices_with_one[:val_one_count],\n",
    "            indices_without_one[:val_size - val_one_count]\n",
    "        ))\n",
    "\n",
    "        test_indices = np.concatenate((\n",
    "            indices_with_one[val_one_count:val_one_count + test_one_count],\n",
    "            indices_without_one[val_size - val_one_count:val_size - val_one_count + test_size - test_one_count]\n",
    "        ))\n",
    "\n",
    "        train_indices = np.concatenate((\n",
    "            indices_with_one[val_one_count + test_one_count:],\n",
    "            indices_without_one[val_size - val_one_count + test_size - test_one_count:]\n",
    "        ))\n",
    "\n",
    "        # Create datasets from selected indices using Subset\n",
    "        train_dataset = Subset(self.dataset, train_indices)\n",
    "        val_dataset = Subset(self.dataset, val_indices)\n",
    "        test_dataset = Subset(self.dataset, test_indices)\n",
    "\n",
    "        # Create DataLoaders for each set\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "        self.validate_loader = DataLoader(val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:20.428840Z",
     "iopub.execute_input": "2024-10-11T18:09:20.429490Z",
     "iopub.status.idle": "2024-10-11T18:09:20.450111Z",
     "shell.execute_reply.started": "2024-10-11T18:09:20.429443Z",
     "shell.execute_reply": "2024-10-11T18:09:20.449233Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:09.212794Z",
     "start_time": "2024-10-13T17:23:09.195787Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = DatasetGenerator(\n",
    "    zip_file_path=ZIP_FILE_PATH,\n",
    "    patient_scan_labels=patient_scan_labels,\n",
    "    patient_slice_labels=patient_slice_labels,\n",
    "    max_slices=MAX_SLICES,\n",
    "    height=HEIGHT,\n",
    "    width=WIDTH,\n",
    "    target_columns=TARGET_COLUMNS\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:20.451320Z",
     "iopub.execute_input": "2024-10-11T18:09:20.451674Z",
     "iopub.status.idle": "2024-10-11T18:09:37.258880Z",
     "shell.execute_reply.started": "2024-10-11T18:09:20.451640Z",
     "shell.execute_reply": "2024-10-11T18:09:37.257901Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:09.274804Z",
     "start_time": "2024-10-13T17:23:09.261802Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "images, labels = dataset[0]\n\nprint(images.shape, labels.shape)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:37.260451Z",
     "iopub.execute_input": "2024-10-11T18:09:37.260893Z",
     "iopub.status.idle": "2024-10-11T18:09:43.744706Z",
     "shell.execute_reply.started": "2024-10-11T18:09:37.260849Z",
     "shell.execute_reply": "2024-10-11T18:09:43.743680Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.205564Z",
     "start_time": "2024-10-13T17:23:09.338818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 3, 224, 224]) torch.Size([60])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "dataloader_manager = DataloaderManager(dataset, batch_size=BATCH_PATIENTS, val_size=VAL_SIZE, test_size=TEST_SIZE)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-11T18:09:43.748011Z",
     "iopub.execute_input": "2024-10-11T18:09:43.748343Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.299138Z",
     "start_time": "2024-10-13T17:23:11.280688Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_image_grid(dataloader, grid_rows=10, max_slices=MAX_SLICES):\n",
    "    \"\"\"\n",
    "    Plot a grid of images from the given DataLoader.\n",
    "    \n",
    "    Args:\n",
    "    dataloader (DataLoader): DataLoader containing batches of images and labels.\n",
    "    grid_rows (int): Number of rows in the grid\n",
    "    max_slices (int): Maximum number of slices to plot\n",
    "    \"\"\"\n",
    "    # Get a first value in batch from the DataLoader\n",
    "    for batch_images, batch_labels in dataloader:\n",
    "        image_tensor = batch_images[0]\n",
    "        label_tensor = batch_labels[0]\n",
    "        break\n",
    "\n",
    "    num_slices = min(image_tensor.shape[0], max_slices)  # Ensure we don't exceed the number of slices\n",
    "    grid_cols = int(max_slices / grid_rows)  # Calculate number of columns\n",
    "    \n",
    "    # Calculate the figure size based on the image dimensions\n",
    "    img_size = HEIGHT # or WIDTH\n",
    "    dpi = plt.rcParams['figure.dpi']  # Get the default DPI\n",
    "    figsize = (grid_cols * img_size / dpi, grid_rows * img_size / dpi)\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=figsize)\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
    "    \n",
    "    # Loop through the number of slices and plot each image\n",
    "    for i in range(num_slices):\n",
    "        axes[i].imshow(image_tensor[i].permute(1, 2, 0).cpu().numpy())  # Convert to (height, width, channels) for plotting\n",
    "        axes[i].set_title(f\"Label: {label_tensor[i].item()}\")\n",
    "        axes[i].axis('off')  # Turn off axis for all subplots\n",
    "    \n",
    "    # Turn off any remaining empty subplots\n",
    "    for i in range(num_slices, grid_rows * grid_cols):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.345326Z",
     "start_time": "2024-10-13T17:23:11.331323Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "# plot_image_grid(test_loader)",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.393230Z",
     "start_time": "2024-10-13T17:23:11.378225Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "# 2. Model Definition\nclass ResNet18(nn.Module):\n    def __init__(self, num_classes=1):\n        super(ResNet18, self).__init__()\n        self.resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n        self.dropout = nn.Dropout(p=0.3)  # Add dropout layer\n\n    def forward(self, x):\n        batch_patients, num_slices, channels, height, width = x.size()\n        x = x.view(-1, channels, height, width)\n        x = self.resnet(x)\n        return self.dropout(x)  # Apply dropout before returning\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNet18().to(device)",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.679486Z",
     "start_time": "2024-10-13T17:23:11.427242Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Loss and Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# mode: Monitors 'min' or 'max' changes in metrics.\n",
    "# factor: The multiplicative factor for reducing the learning rate. \n",
    "##  If the current learning rate is 0.01 and factor=0.5, the new learning rate will be 0.01 * 0.5 = 0.005\n",
    "# patience: Number of epochs to wait for improvement before reducing the learning rate."
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.727497Z",
     "start_time": "2024-10-13T17:23:11.713494Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image).squeeze()\n",
    "        label = label.reshape(-1)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_label = (output > 0.5).float()\n",
    "        total += label.size(0)\n",
    "        correct += (predicted_label == label).sum().item()\n",
    "\n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}%')\n",
    "\n",
    "    scheduler.step(epoch_loss)\n",
    "    return epoch_loss"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.774927Z",
     "start_time": "2024-10-13T17:23:11.760505Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "def validate(model, validate_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(validate_loader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(image).squeeze()\n",
    "            label = label.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_label = (output > 0.5).float()\n",
    "            total += label.size(0)\n",
    "            correct += (predicted_label == label).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy for validation\n",
    "    val_loss = running_loss / len(validate_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.4f}%')\n",
    "    \n",
    "    return val_loss"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.822400Z",
     "start_time": "2024-10-13T17:23:11.808393Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.869793Z",
     "start_time": "2024-10-13T17:23:11.855791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, loss):\n",
    "    checkpoint_path = os.path.join(CHECKPOINTS_DIR, f'checkpoint_epoch_{epoch+1}.pth')    \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.917570Z",
     "start_time": "2024-10-13T17:23:11.903801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_best_checkpoint(model, optimizer, scheduler, checkpoint_dir):\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_epoch_') and f.endswith('.pth')]\n",
    "    if not checkpoint_files:\n",
    "        print(\"No checkpoints found.\")\n",
    "        return None, None  # Return (None, None) instead of None\n",
    "\n",
    "    best_checkpoint = min(checkpoint_files, key=lambda x: float(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, best_checkpoint)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    best_val_loss = checkpoint['val_loss']\n",
    "    \n",
    "    print(f\"Loaded checkpoint from epoch {epoch+1} with validation loss {best_val_loss}\")\n",
    "    \n",
    "    return epoch, best_val_loss"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_roc_curve(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(FIGURES_DIR, 'roc_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:11.963613Z",
     "start_time": "2024-10-13T17:23:11.949586Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(FIGURES_DIR, 'confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:12.009302Z",
     "start_time": "2024-10-13T17:23:11.995982Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:12.056123Z",
     "start_time": "2024-10-13T17:23:12.042360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_final_model(model, optimizer, scheduler, final_model_path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, test_loader, device, criterion):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_scores = []\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            \n",
    "            output = model(image).squeeze()\n",
    "            label = label.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            predicted_label = (output > 0.5).float()\n",
    "            total += label.size(0)\n",
    "            correct += (predicted_label == label).sum().item()\n",
    "            \n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_predictions.extend(predicted_label.cpu().numpy())\n",
    "            all_scores.extend(output.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}%')\n",
    "    print(f'Test Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    plot_roc_curve(np.array(all_labels), np.array(all_scores))\n",
    "    plot_confusion_matrix(np.array(all_labels), np.array(all_predictions))\n",
    "\n",
    "    return accuracy, avg_loss"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T17:23:12.101574Z",
     "start_time": "2024-10-13T17:23:12.088385Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "loaded_epoch, loaded_best_val_loss = load_best_checkpoint(model, optimizer, scheduler, CHECKPOINTS_DIR)\n",
    "\n",
    "if loaded_epoch is not None:\n",
    "    start_epoch = loaded_epoch + 1\n",
    "    best_val_loss = loaded_best_val_loss\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    train_loader = dataloader_manager.train_loader\n",
    "    validate_loader = dataloader_manager.validate_loader\n",
    "    \n",
    "    train_loss = train(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    val_loss = validate(model, validate_loader, criterion, device)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, val_loss)\n",
    "        best_val_loss = val_loss"
   ],
   "metadata": {
    "trusted": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-13T17:23:12.133584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "test_loader = dataloader_manager.test_loader\n",
    "accuracy, test_loss = evaluate(model, test_loader, device, criterion)\n",
    "\n",
    "final_model_path = os.path.join(CHECKPOINTS_DIR, 'final_model.pth')\n",
    "save_final_model(model, optimizer, scheduler, final_model_path)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
