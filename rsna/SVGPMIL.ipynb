{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import libraries",
   "id": "8d9995082118f899"
  },
  {
   "cell_type": "code",
   "id": "14d0fa2438ca9c8b",
   "metadata": {},
   "source": [
    "import gpytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "# warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "import random\n",
    "import os\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from layers.gaussian_process import PGLikelihood, SVGP_Model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Split dataset",
   "id": "bf0a5aa959a59c4"
  },
  {
   "cell_type": "code",
   "id": "63929fc8f64331c8",
   "metadata": {},
   "source": [
    "def split_dataset_for_multilabel(X, multi_y, test_size=0.15, val_size=0.25, random_state=42, num_classes=6, pos_label=0):\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(msss.split(X, multi_y))\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = multi_y[train_idx], multi_y[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(X_train, y_train))\n",
    "\n",
    "        X_train_final, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_final, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        msss_val = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(msss_val.split(X, multi_y))\n",
    "\n",
    "        X_train_final, X_val = X[train_idx], X[val_idx]\n",
    "        y_train_final, y_val = multi_y[train_idx], multi_y[val_idx]\n",
    "        X_test, y_test = None, None\n",
    "\n",
    "    return X_train_final, y_train_final, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.15, val_size=0.15, random_state=42, num_classes=1, pos_label=0):\n",
    "    if test_size > 0:\n",
    "        # First split: train + test\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        train_idx, test_idx = next(sss.split(X, y[:, pos_label]))\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Second split: train + validation\n",
    "        sss_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(sss_val.split(X_train, y_train[:, pos_label]))\n",
    "\n",
    "        X_train_final, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_final, y_val = y_train[train_idx], y_train[val_idx]\n",
    "    else:\n",
    "        # Only split into train and validation if test_size is 0\n",
    "        sss_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)\n",
    "        train_idx, val_idx = next(sss_val.split(X, y[:, pos_label]))\n",
    "\n",
    "        X_train_final, X_val = X[train_idx], X[val_idx]\n",
    "        y_train_final, y_val = y[train_idx], y[val_idx]\n",
    "        X_test, y_test = None, None\n",
    "\n",
    "    return X_train_final, y_train_final, X_val, y_val, X_test, y_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b5fea87f9e89555",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0182df4732b451a",
   "metadata": {},
   "source": "from layers.attention import MILAttentionLayer, GatedAttention",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35782ea183d1a9d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "4abe7b1efa6e8ea7",
   "metadata": {},
   "source": [
    "# Define the SVGP_MIL Model\n",
    "class SVGP_MIL(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, output_dim, Xtrain, BagLabels, num_ind=50, verbose=False):\n",
    "        super(SVGP_MIL, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.gp_layer = nn.ModuleList([SVGP_Model(torch.rand(num_ind, 1)) for _ in range(NUM_CLASSES)])\n",
    "\n",
    "        self.fc = nn.Linear(feature_dim + 1, 1)  # Fully connected layer\n",
    "        self.att_layer = nn.ModuleList([MILAttentionLayer(feature_dim, hidden_dim) for _ in range(NUM_CLASSES)])\n",
    "        self.fc_gp = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        gp_output = []\n",
    "        for i in range(NUM_CLASSES):\n",
    "            attended_features, _ = self.att_layer[i](x)\n",
    "            attended_features = self.dropout(attended_features)\n",
    "            gp = self.gp_layer[i](self.fc_gp(attended_features))\n",
    "            gp_output.append(gp)\n",
    "            combine_feature = torch.cat([attended_features, gp.mean.unsqueeze(-1)], dim=-1)\n",
    "            combine_feature = self.fc(combine_feature)\n",
    "            output.append(combine_feature)\n",
    "        output = torch.cat(output, dim=-1)\n",
    "\n",
    "        return output, gp_output, _"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89dada22ccbbbfbc",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "id": "4238045e823094e0",
   "metadata": {},
   "source": [
    "# Define a custom Dataset\n",
    "class MedicalDataset(Dataset):\n",
    "    def __init__(self, features, multi_labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.multi_labels = torch.tensor(multi_labels, dtype=torch.float32)\n",
    "        print(f'Shape of multi_labels: {self.multi_labels.shape}')\n",
    "        self.labels = self.multi_labels[:, 0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.multi_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx], self.multi_labels[idx]\n",
    "\n",
    "def load_data(csv_file, num_instances=57, test_size=0.2, random_state=42, dataset='rsna'):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Group features by bag_name\n",
    "    grouped = df.groupby('bag_name')\n",
    "\n",
    "    # Prepare feature lists and label lists\n",
    "    feature_list = []\n",
    "    bag_label_list = []\n",
    "    multi_label_list = []\n",
    "    if dataset == 'cq500':\n",
    "        multi_label_names = ['patient_ICH', 'patient_EDH', 'patient_IPH', 'patient_IVH', 'patient_SAH', 'patient_SDH']\n",
    "    else:\n",
    "        multi_label_names = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "\n",
    "    for bag_name, group in grouped:\n",
    "        # Extract features for the current bag\n",
    "        feature_cols = [f'feature_{i}' for i in range(8)]\n",
    "        features = group[feature_cols].values\n",
    "\n",
    "        # Pad with zeros if the number of instances is less than num_instances\n",
    "        if len(features) < num_instances:\n",
    "            padding_size = num_instances - len(features)\n",
    "            padding = np.zeros((padding_size, 8))\n",
    "            features = np.vstack((features, padding))  # Vertically stack features and padding\n",
    "\n",
    "        # Truncate if the number of instances is greater than num_instances\n",
    "        elif len(features) > num_instances:\n",
    "            features = features[:num_instances]\n",
    "\n",
    "        feature_list.append(features)\n",
    "\n",
    "        # Get the bag label for the current bag\n",
    "        bag_label = group['bag_label'].values[0]\n",
    "        bag_label_list.append(bag_label)\n",
    "\n",
    "        # Process multi-labels\n",
    "        multi_labels = []\n",
    "        for label in multi_label_names:\n",
    "            # If any instance in the bag has the label, set it to 1\n",
    "            label_value = 1 if (group[label] == 1).any() else 0\n",
    "            multi_labels.append(label_value)\n",
    "        multi_label_list.append(multi_labels)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(feature_list)\n",
    "    y = np.array(bag_label_list)\n",
    "    multi_y = np.array(multi_label_list)\n",
    "\n",
    "    return X, y, multi_y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Metrics",
   "id": "f6691d53ea8dafa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_metrics(y_true, y_pred, phase='val'):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    if NUM_CLASSES == 1:\n",
    "        # average = 'weighted' if phase == 'train' else 'binary'\n",
    "        # average = 'binary' if phase == 'test' else 'weighted'\n",
    "        average = 'weighted'\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "    else:\n",
    "        # average = None\n",
    "        average = 'weighted'\n",
    "        accuracy = np.mean((y_true == y_pred).mean(axis=0))\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average=average, zero_division=np.nan)\n",
    "    recall = recall_score(y_true, y_pred, average=average, zero_division=np.nan)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=np.nan)\n",
    "\n",
    "    if phase == 'test' and NUM_CLASSES != 1:\n",
    "        print(f'Accuracy: {(y_true == y_pred).mean(axis=0)}')\n",
    "        print(f'Precision: {precision} Recall: {recall} F1: {f1}')\n",
    "        return { 'partial_accuracy': np.average(accuracy), 'precision': np.average(precision), 'recall': np.average(recall), 'f1': np.average(f1) }\n",
    "\n",
    "    metrics = {\n",
    "        'partial_accuracy': accuracy,\n",
    "        'precision': np.average(precision),\n",
    "        'recall': np.average(recall),\n",
    "        'f1': np.average(f1)\n",
    "    }\n",
    "    return metrics"
   ],
   "id": "c0ac903eba3f29a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88cb8719111a98aa",
   "metadata": {},
   "source": "# Training and evaluation"
  },
  {
   "cell_type": "code",
   "id": "662009e9efe66391",
   "metadata": {},
   "source": [
    "def train(model, dataloader, optimizer, criterion, device, likelihood, var_optimizer, mlls, num_classes, threshold, use_likelihood=False):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    epoch_loss = 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with gpytorch.settings.num_likelihood_samples(100):\n",
    "        for features, labels, multi_labels in dataloader:\n",
    "            features, labels, multi_labels = features.to(device), labels.to(device), multi_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            for opt in var_optimizer:\n",
    "                opt.zero_grad()\n",
    "\n",
    "            predictions, gp_output, _ = model(features)\n",
    "            loss = calculate_loss(gp_output, multi_labels, predictions, criterion, mlls, num_classes)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            for opt in var_optimizer:\n",
    "                opt.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            probs = get_probabilities(predictions, gp_output, likelihood, num_classes, use_likelihood)\n",
    "            preds = (probs >= threshold).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(multi_labels.cpu().numpy() if num_classes > 1 else labels.cpu().numpy())\n",
    "\n",
    "    metrics = calculate_metrics(all_labels, all_preds)\n",
    "    return epoch_loss / len(dataloader), metrics['partial_accuracy'], metrics['precision'], metrics['recall'], metrics['f1']\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, likelihood, mlls, num_classes, threshold, use_likelihood=False, phase='val'):\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds, all_labels, all_scores = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels, multi_labels in dataloader:\n",
    "            features, labels, multi_labels = features.to(device), labels.to(device), multi_labels.to(device)\n",
    "\n",
    "            predictions, gp_output, _ = model(features)\n",
    "            loss = calculate_loss(gp_output, multi_labels, predictions, criterion, mlls, num_classes)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            probs = get_probabilities(predictions, gp_output, likelihood, num_classes, use_likelihood)\n",
    "            preds = (probs >= threshold).float()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(multi_labels.cpu().numpy() if num_classes > 1 else labels.cpu().numpy())\n",
    "            all_scores.extend(probs.cpu().numpy())\n",
    "\n",
    "    metrics = calculate_metrics(all_labels, all_preds, phase=phase)\n",
    "    cohen_kappa = cohen_kappa_score(all_labels, all_preds) if num_classes == 1 else 0\n",
    "\n",
    "    return (epoch_loss / len(dataloader), metrics['partial_accuracy'], metrics['precision'],\n",
    "            metrics['recall'], metrics['f1'], cohen_kappa, all_labels, all_scores)\n",
    "\n",
    "def get_probabilities(predictions, gp_output, likelihood, num_classes, use_likelihood):\n",
    "    if use_likelihood:\n",
    "        return torch.stack([likelihood[i](gp_output[i]).probs for i in range(num_classes)], dim=-1)\n",
    "    else:\n",
    "        return torch.sigmoid(predictions)\n",
    "\n",
    "def calculate_loss(gp_output, multi_labels, predictions, criterion, mlls, num_classes):\n",
    "    gp_loss = sum(-mll(gp, multi_labels[:, i]) for i, (mll, gp) in enumerate(zip(mlls, gp_output)))\n",
    "    gp_loss = gp_loss.mean() / num_classes\n",
    "\n",
    "    pred_loss = (criterion(predictions.squeeze(-1), multi_labels[:, 0]) if num_classes == 1\n",
    "                 else criterion(predictions, multi_labels))\n",
    "    total_loss = 0.5 * gp_loss + 0.5 * pred_loss\n",
    "    total_loss = total_loss.mean()\n",
    "    return total_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "178e0fcb2f5b717e",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "cbb4152404e810cb",
   "metadata": {},
   "source": [
    "def plot_roc_auc(y_true, y_scores, class_names=None):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if class_names is None:  # Binary classification\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC AUC = {roc_auc:.4f}')\n",
    "        roc_auc_values = [roc_auc]\n",
    "    else:  # Multi-label classification\n",
    "        roc_auc_values = []\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            fpr, tpr, _ = roc_curve(y_true[:, i], y_scores[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, lw=2, label=f'{class_name} (AUC = {roc_auc:.2f})')\n",
    "            roc_auc_values.append(roc_auc)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc_values\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, class_names=None):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    if class_names is None:  # Binary classification\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "        auc_pr = auc(recall, precision)\n",
    "        plt.plot(recall, precision, color='b', label=f'AUC-PR = {auc_pr:.4f}')\n",
    "        auc_pr_values = [auc_pr]\n",
    "    else:  # Multi-label classification\n",
    "        auc_pr_values = []\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            precision, recall, _ = precision_recall_curve(y_true[:, i], y_scores[:, i])\n",
    "            auc_pr = auc(recall, precision)\n",
    "            plt.plot(recall, precision, lw=2, label=f'{class_name} (AUC = {auc_pr:.2f})')\n",
    "            auc_pr_values.append(auc_pr)\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return auc_pr_values\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, threshold=0.5):\n",
    "    if class_names is None:  # Binary classification\n",
    "        y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred_binary)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "    else:  # Multi-label classification\n",
    "        n_classes = len(class_names)\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 15))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            cm = confusion_matrix(y_true[:, i], (y_pred[:, i] >= threshold).astype(int))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "            axes[i].set_xlabel('Predicted labels')\n",
    "            axes[i].set_ylabel('True labels')\n",
    "            axes[i].set_title(f'Confusion Matrix - {class_names[i]}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a30545c45d70d3",
   "metadata": {},
   "source": "# Set up"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def setup_data_loaders(X_train, y_train, X_val, y_val, X_test, y_multi_test, batch_size):\n",
    "    train_dataset = MedicalDataset(X_train, y_train)\n",
    "    val_dataset = MedicalDataset(X_val, y_val)\n",
    "    test_dataset = MedicalDataset(X_test, y_multi_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def setup_model_and_optimizer(feature_dim, hidden_dim, output_dim, X_train, y_train, device, learning_rate, num_classes):\n",
    "    model = SVGP_MIL(feature_dim, hidden_dim, output_dim, X_train, y_train).to(device)\n",
    "    likelihood = nn.ModuleList([PGLikelihood().to(device) for _ in range(num_classes)])\n",
    "    mlls = nn.ModuleList([gpytorch.mlls.VariationalELBO(likelihood[i], model.gp_layer[i], num_data=len(X_train)) for i in range(num_classes)])\n",
    "    mlls = [mll.to(device) for mll in mlls]\n",
    "    variational_ngd_optim = [gpytorch.optim.NGD(model.gp_layer[i].variational_parameters(), num_data=len(X_train), lr=0.01) for i in range(num_classes)]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(f'Len of X_train: {len(X_train)}')\n",
    "\n",
    "    return model, likelihood, mlls, variational_ngd_optim, optimizer"
   ],
   "id": "faaf3887ce1b0c00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main function",
   "id": "728aa9ae8130e94f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main(train_file, test_file, hidden_dim=64, learning_rate=0.00005, batch_size=16, num_epochs=100, num_instances=57, use_likelihood=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load and prepare data\n",
    "    X_train, y_train, y_multi_train = load_data(train_file, num_instances=num_instances)\n",
    "    num_instances_for_test = 32 if 'cq500' in test_file else num_instances\n",
    "    dataset = 'cq500' if 'cq500' in test_file else 'rsna'\n",
    "    X_test, y_test, y_multi_test = load_data(test_file, num_instances=num_instances_for_test, dataset=dataset)\n",
    "\n",
    "    split_func = split_dataset_for_multilabel if NUM_CLASSES > 1 else split_data\n",
    "    X_train, y_train, X_val, y_val, _, _ = split_func(X_train, y_multi_train, test_size=0.0, val_size=0.25, random_state=42, num_classes=NUM_CLASSES, pos_label=SINGLE_LABEL_POS)\n",
    "    # Setup data loaders\n",
    "    train_loader, val_loader, test_loader = setup_data_loaders(X_train, y_train, X_val, y_val, X_test, y_multi_test, batch_size)\n",
    "\n",
    "    # Setup model and optimizer\n",
    "    feature_dim = X_train.shape[2]\n",
    "    output_dim = 1\n",
    "    model, likelihood, mlls, variational_ngd_optim, optimizer = setup_model_and_optimizer(feature_dim, hidden_dim, output_dim, X_train, y_train, device, learning_rate, NUM_CLASSES)\n",
    "\n",
    "    # Setup loss function\n",
    "    pos_weights = torch.tensor([2.0, 25.0, 5.0, 5.0, 10.0, 10.0] if NUM_CLASSES > 1 else [2.0], dtype=torch.float32).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "    # Training loop\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    best_likelihood = None\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc, train_precision, train_recall, train_f1 = train(model, train_loader, optimizer, criterion, device, likelihood, variational_ngd_optim,mlls, NUM_CLASSES, threshold=0.5, use_likelihood=use_likelihood)\n",
    "        val_loss, val_acc, val_precision, val_recall, val_f1, val_co_kappa, val_labels, val_scores = evaluate(model, val_loader, criterion, device, likelihood, mlls, NUM_CLASSES, THRESHOLD, phase='val', use_likelihood=use_likelihood)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, Precision={train_precision:.4f}, Recall={train_recall:.4f}, F1={train_f1:.4f}')\n",
    "        print(f'Validation: Loss={val_loss:.4f}, Acc={val_acc:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}, F1={val_f1:.4f}\\n')\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = model.state_dict()\n",
    "            best_likelihood = likelihood.state_dict()\n",
    "\n",
    "    print(f'Best val accuracy: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model)\n",
    "    likelihood.load_state_dict(best_likelihood)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_precision, test_recall, test_f1, test_co_kappa, test_labels, test_scores = evaluate(model, test_loader, criterion, device, likelihood, mlls, NUM_CLASSES, THRESHOLD, phase='test', use_likelihood=use_likelihood)\n",
    "    print(f'Test: Loss={test_loss:.4f}, Acc={test_acc:.4f}, Precision={test_precision:.4f}, Recall={test_recall:.4f}, F1={test_f1:.4f}, Cohen Kappa={test_co_kappa:.4f}')\n",
    "\n",
    "    # Plot performance curves\n",
    "    if NUM_CLASSES == 1:\n",
    "        test_labels, test_scores = np.array(test_labels), np.array(test_scores)\n",
    "        test_auc = plot_roc_auc(test_labels, test_scores)\n",
    "        test_auc_pr = plot_pr_curve(test_labels, test_scores)\n",
    "        plot_confusion_matrix(test_labels, test_scores)\n",
    "    else:\n",
    "        class_names = ['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "        test_labels, test_scores = np.array(test_labels), np.array(test_scores)\n",
    "        test_auc = plot_roc_auc(test_labels, test_scores, class_names)\n",
    "        test_auc_pr = plot_pr_curve(test_labels, test_scores, class_names)\n",
    "        plot_confusion_matrix(test_labels, test_scores, class_names)\n",
    "\n",
    "    return test_acc, test_precision, test_recall, test_f1, test_co_kappa, test_auc, test_auc_pr"
   ],
   "id": "6bc881a6cf79550a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main",
   "id": "3c1900ef1e2599b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    NUM_CLASSES = 6\n",
    "    THRESHOLD = 0.5\n",
    "    SINGLE_LABEL_POS = 0\n",
    "    USE_LIKELIHOOD = False\n",
    "    seed_everything(seed=42)\n",
    "\n",
    "    # train_files = ['rsna_train_0_redundancy.csv', 'rsna_train_1_redundancy.csv', 'rsna_train_2_redundancy.csv', 'rsna_train_3_redundancy.csv', 'rsna_train_4_redundancy.csv']\n",
    "    # test_files = ['rsna_test_0_redundancy.csv', 'rsna_test_1_redundancy.csv', 'rsna_test_2_redundancy.csv', 'rsna_test_3_redundancy.csv', 'rsna_test_4_redundancy.csv']\n",
    "    # train_files = ['rsna_train_0_update.csv', 'rsna_train_1_update.csv', 'rsna_train_2_update.csv', 'rsna_train_3_update.csv', 'rsna_train_4_update.csv']\n",
    "    # test_files = ['rsna_test_0_update.csv', 'rsna_test_1_update.csv', 'rsna_test_2_update.csv', 'rsna_test_3_update.csv', 'rsna_test_4_update.csv']\n",
    "    train_files = ['data/cv_data/train_features.csv']\n",
    "    test_files = ['data/cv_data/test_features.csv']\n",
    "    # train_files = ['data/rsna/rsna_train_0_redundancy.csv']\n",
    "    # test_files = ['data/rsna/rsna_test_0_redundancy.csv']\n",
    "    all_results = []\n",
    "    all_auc_results = []\n",
    "    all_auc_pr_results = []\n",
    "    for train_file, test_file in zip(train_files, test_files):\n",
    "        # train_file = f'data/{train_file}'\n",
    "        # test_file = f'data/{test_file}'\n",
    "        results = main(train_file, test_file, num_instances=28, learning_rate=0.00005, batch_size=16, num_epochs=100, use_likelihood=USE_LIKELIHOOD)\n",
    "        all_results.append(results[:5])\n",
    "        all_auc_results.append(results[5])\n",
    "        all_auc_pr_results.append(results[6])\n",
    "\n",
    "    # Calculate and print mean and std for scalar metrics\n",
    "    all_results = np.array(all_results)\n",
    "    mean_results = np.mean(all_results, axis=0)\n",
    "    std_results = np.std(all_results, axis=0)\n",
    "\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'Cohen Kappa']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        print(f'{metric}: Mean = {mean_results[i]:.4f}, Std = {std_results[i]:.4f}')\n",
    "\n",
    "    # Handle AUC metrics separately\n",
    "    if NUM_CLASSES == 1:\n",
    "        print(f'ROC-AUC: Mean = {np.mean(all_auc_results):.4f}, Std = {np.std(all_auc_results):.4f}')\n",
    "        print(f'AUC-PR: Mean = {np.mean(all_auc_pr_results):.4f}, Std = {np.std(all_auc_pr_results):.4f}')\n",
    "    else:\n",
    "        # For multi-class, you might want to average across classes first\n",
    "        mean_auc = np.mean([np.mean(auc) for auc in all_auc_results])\n",
    "        std_auc = np.std([np.mean(auc) for auc in all_auc_results])\n",
    "        mean_auc_pr = np.mean([np.mean(auc_pr) for auc_pr in all_auc_pr_results])\n",
    "        std_auc_pr = np.std([np.mean(auc_pr) for auc_pr in all_auc_pr_results])\n",
    "        print(f'ROC-AUC: Mean = {mean_auc:.4f}, Std = {std_auc:.4f}')\n",
    "        print(f'AUC-PR: Mean = {mean_auc_pr:.4f}, Std = {std_auc_pr:.4f}')"
   ],
   "id": "80e64d1a42233829",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
