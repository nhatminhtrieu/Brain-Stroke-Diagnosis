{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/SimCLR.ipynb",
     "timestamp": 1732781375892
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:00:41.842829Z",
     "start_time": "2024-11-29T15:00:40.754370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCELoss\n",
    "# import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Define NTXentLoss (provided by you)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        similarity_matrix = torch.mm(z, z.T) / self.temperature\n",
    "        mask = torch.eye(2 * batch_size, device=z.device).bool()\n",
    "        similarity_matrix.masked_fill_(mask, -float('inf'))\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        denominator = exp_sim.sum(dim=1)\n",
    "        positive_samples = torch.cat(\n",
    "            [torch.arange(batch_size, 2 * batch_size), torch.arange(batch_size)], dim=0\n",
    "        ).to(z.device)\n",
    "        positives = similarity_matrix[torch.arange(2 * batch_size), positive_samples]\n",
    "        loss = -torch.log(torch.exp(positives) / denominator)\n",
    "        return loss.mean()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-29T10:08:54.329202Z",
     "iopub.execute_input": "2024-11-29T10:08:54.329822Z",
     "iopub.status.idle": "2024-11-29T10:09:15.549417Z",
     "shell.execute_reply.started": "2024-11-29T10:08:54.329785Z",
     "shell.execute_reply": "2024-11-29T10:09:15.548443Z"
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T15:00:41.848829Z",
     "start_time": "2024-11-29T15:00:41.845813Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:00:44.838904Z",
     "start_time": "2024-11-29T15:00:41.907716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_data, eval_data = train_test_split(mnist_data, train_size=0.1, random_state=42, stratify=mnist_data.targets)\n",
    "\n",
    "# Bag-level dataset\n",
    "class BagDataset(Dataset):\n",
    "    def __init__(self, data, bag_size=8):\n",
    "        self.data = data\n",
    "        self.bag_size = bag_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.bag_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, labels = [], []\n",
    "        for i in range(self.bag_size):\n",
    "            img, label = self.data[idx * self.bag_size + i]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        bag_label = 1 if 9 in labels else 0\n",
    "        return torch.stack(images), torch.tensor(bag_label)\n",
    "\n",
    "train_dataset = BagDataset(train_data)\n",
    "eval_dataset = BagDataset(eval_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True, num_workers=4, drop_last=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=128, shuffle=False, pin_memory=True, num_workers=4, drop_last=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:00:44.860229Z",
     "start_time": "2024-11-29T15:00:44.858331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:00:45.053599Z",
     "start_time": "2024-11-29T15:00:44.901686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, base_model, projection_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = base_model\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.attention = Attention(512, 1)\n",
    "        # self.fc = nn.Linear(512, 1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        batch_size, num_instances, channels, height, width = x.size()\n",
    "        x = x.view(-1, channels, height, width)  # Reshape to (batch_size * num_instances, channels, height, width)\n",
    "        features = self.encoder(x)\n",
    "        features = nn.Dropout(0.25)(features)\n",
    "        features = features.view(batch_size * num_instances, -1)  # Flatten to (batch_size * num_instances, feature_dim)\n",
    "        \n",
    "        projection_features = self.projection(features)\n",
    "        attention_features, _ = self.attention(features.view(batch_size, num_instances, -1))\n",
    "        output = self.fc(attention_features)\n",
    "        \n",
    "        return projection_features, output\n",
    "\n",
    "# Augmentation function\n",
    "def augment_batch(batch_images):\n",
    "    batch_size, num_instances, channels, height, width = batch_images.shape\n",
    "    aug_transform = transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(224, scale=(0.8, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3)], p=0.6),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Apply transformation to each image instance in the batch\n",
    "    augmented_batch = []\n",
    "    for i in range(batch_size):\n",
    "        augmented_instances = [aug_transform(transforms.ToPILImage()(img.cpu())) for img in batch_images[i]]\n",
    "        augmented_batch.append(torch.stack(augmented_instances))\n",
    "    \n",
    "    return torch.stack(augmented_batch).cuda()  # Move the augmented batch to GPU\n",
    "\n",
    "# Initialize models, loss function, and optimizer\n",
    "base_model = models.resnet18(weights=None)\n",
    "base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_model.fc = nn.Identity()\n",
    "encoder = Encoder(base_model).cuda()\n",
    "projection_dim = 256\n",
    "ntxent_loss = NTXentLoss().cuda()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# Training parameters\n",
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "bceLoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "# Training loop for contrastive learning\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda()  # Move the batch to GPU\n",
    "        labels = labels.cuda()\n",
    "        # aug1 = augment_batch(images).cuda()\n",
    "        aug1 = images \n",
    "        aug2 = augment_batch(images).cuda()\n",
    "        \n",
    "        z_i, outputs_1 = encoder(aug1)\n",
    "        z_j, outputs_2 = encoder(aug2)\n",
    "        \n",
    "        NTXLoss = ntxent_loss(z_i, z_j)\n",
    "        BCELoss_1 = bceLoss(outputs_1.squeeze(), labels.float())\n",
    "        BCELoss_2 = bceLoss(outputs_2.squeeze(), labels.float())\n",
    "        loss = 0.4 * NTXLoss + 0.3 * BCELoss_1 + 0.3 * BCELoss_2\n",
    "        loss = loss.mean()\n",
    "        # loss = ntxent_loss(z_i, z_j)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate predictions and update correct predictions count\n",
    "        predicted = (torch.sigmoid(outputs_1.squeeze()) > 0.5).float()  # Binary classification threshold\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = (correct_predictions / total_samples) * 100  # Convert to percentage\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.5f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in eval_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        z_i, output = encoder(images)\n",
    "        predicted = (torch.sigmoid(output.squeeze()) > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {correct/total}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-29T10:09:15.550943Z",
     "iopub.execute_input": "2024-11-29T10:09:15.551296Z",
     "iopub.status.idle": "2024-11-29T10:15:04.545496Z",
     "shell.execute_reply.started": "2024-11-29T10:09:15.551270Z",
     "shell.execute_reply": "2024-11-29T10:15:04.544444Z"
    },
    "ExecuteTime": {
     "end_time": "2024-11-29T15:02:24.875740Z",
     "start_time": "2024-11-29T15:00:45.071973Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 3.39113, Accuracy: 59.22%\n",
      "Epoch [2/100], Loss: 3.02312, Accuracy: 66.09%\n",
      "Epoch [3/100], Loss: 2.88068, Accuracy: 73.02%\n",
      "Epoch [4/100], Loss: 2.78280, Accuracy: 78.09%\n",
      "Epoch [5/100], Loss: 2.74355, Accuracy: 81.78%\n",
      "Epoch [6/100], Loss: 2.68787, Accuracy: 84.58%\n",
      "Epoch [7/100], Loss: 2.65900, Accuracy: 86.63%\n",
      "Epoch [8/100], Loss: 2.62379, Accuracy: 88.16%\n",
      "Epoch [9/100], Loss: 2.61141, Accuracy: 89.32%\n",
      "Epoch [10/100], Loss: 2.58888, Accuracy: 90.27%\n",
      "Epoch [11/100], Loss: 2.58131, Accuracy: 91.09%\n",
      "Epoch [12/100], Loss: 2.55641, Accuracy: 91.76%\n",
      "Epoch [13/100], Loss: 2.54758, Accuracy: 92.31%\n",
      "Epoch [14/100], Loss: 2.53070, Accuracy: 92.81%\n",
      "Epoch [15/100], Loss: 2.51705, Accuracy: 93.29%\n",
      "Epoch [16/100], Loss: 2.49763, Accuracy: 93.70%\n",
      "Epoch [17/100], Loss: 2.48363, Accuracy: 94.05%\n",
      "Epoch [18/100], Loss: 2.48901, Accuracy: 94.36%\n",
      "Epoch [19/100], Loss: 2.47312, Accuracy: 94.62%\n",
      "Epoch [20/100], Loss: 2.46989, Accuracy: 94.88%\n",
      "Epoch [21/100], Loss: 2.47246, Accuracy: 95.09%\n",
      "Epoch [22/100], Loss: 2.44678, Accuracy: 95.31%\n",
      "Epoch [23/100], Loss: 2.43844, Accuracy: 95.50%\n",
      "Epoch [24/100], Loss: 2.42932, Accuracy: 95.69%\n",
      "Epoch [25/100], Loss: 2.44177, Accuracy: 95.86%\n",
      "Epoch [26/100], Loss: 2.45765, Accuracy: 95.99%\n",
      "Epoch [27/100], Loss: 2.46628, Accuracy: 96.10%\n",
      "Epoch [28/100], Loss: 2.44195, Accuracy: 96.23%\n",
      "Epoch [29/100], Loss: 2.42851, Accuracy: 96.35%\n",
      "Epoch [30/100], Loss: 2.42648, Accuracy: 96.47%\n",
      "Epoch [31/100], Loss: 2.41776, Accuracy: 96.58%\n",
      "Epoch [32/100], Loss: 2.41909, Accuracy: 96.68%\n",
      "Epoch [33/100], Loss: 2.41418, Accuracy: 96.78%\n",
      "Epoch [34/100], Loss: 2.41332, Accuracy: 96.87%\n",
      "Epoch [35/100], Loss: 2.41244, Accuracy: 96.96%\n",
      "Epoch [36/100], Loss: 2.40368, Accuracy: 97.04%\n",
      "Epoch [37/100], Loss: 2.39567, Accuracy: 97.12%\n",
      "Epoch [38/100], Loss: 2.39720, Accuracy: 97.19%\n",
      "Epoch [39/100], Loss: 2.39186, Accuracy: 97.26%\n",
      "Epoch [40/100], Loss: 2.39583, Accuracy: 97.33%\n",
      "Epoch [41/100], Loss: 2.38832, Accuracy: 97.40%\n",
      "Epoch [42/100], Loss: 2.39184, Accuracy: 97.46%\n",
      "Epoch [43/100], Loss: 2.38375, Accuracy: 97.52%\n",
      "Epoch [44/100], Loss: 2.38784, Accuracy: 97.57%\n",
      "Epoch [45/100], Loss: 2.38157, Accuracy: 97.63%\n",
      "Epoch [46/100], Loss: 2.37805, Accuracy: 97.68%\n",
      "Epoch [47/100], Loss: 2.38384, Accuracy: 97.72%\n",
      "Epoch [48/100], Loss: 2.37534, Accuracy: 97.77%\n",
      "Epoch [49/100], Loss: 2.37844, Accuracy: 97.81%\n",
      "Epoch [50/100], Loss: 2.37320, Accuracy: 97.86%\n",
      "Epoch [51/100], Loss: 2.37262, Accuracy: 97.90%\n",
      "Epoch [52/100], Loss: 2.37544, Accuracy: 97.94%\n",
      "Epoch [53/100], Loss: 2.37426, Accuracy: 97.97%\n",
      "Epoch [54/100], Loss: 2.37314, Accuracy: 98.00%\n",
      "Epoch [55/100], Loss: 2.38155, Accuracy: 98.03%\n",
      "Epoch [56/100], Loss: 2.37913, Accuracy: 98.06%\n",
      "Epoch [57/100], Loss: 2.37350, Accuracy: 98.09%\n",
      "Epoch [58/100], Loss: 2.37217, Accuracy: 98.12%\n",
      "Epoch [59/100], Loss: 2.37141, Accuracy: 98.15%\n",
      "Epoch [60/100], Loss: 2.36527, Accuracy: 98.18%\n",
      "Epoch [61/100], Loss: 2.36578, Accuracy: 98.20%\n",
      "Epoch [62/100], Loss: 2.36024, Accuracy: 98.23%\n",
      "Epoch [63/100], Loss: 2.36142, Accuracy: 98.26%\n",
      "Epoch [64/100], Loss: 2.36294, Accuracy: 98.29%\n",
      "Epoch [65/100], Loss: 2.35602, Accuracy: 98.31%\n",
      "Epoch [66/100], Loss: 2.35830, Accuracy: 98.33%\n",
      "Epoch [67/100], Loss: 2.35739, Accuracy: 98.35%\n",
      "Epoch [68/100], Loss: 2.36547, Accuracy: 98.38%\n",
      "Epoch [69/100], Loss: 2.35563, Accuracy: 98.40%\n",
      "Epoch [70/100], Loss: 2.35308, Accuracy: 98.42%\n",
      "Epoch [71/100], Loss: 2.35251, Accuracy: 98.44%\n",
      "Epoch [72/100], Loss: 2.35062, Accuracy: 98.46%\n",
      "Epoch [73/100], Loss: 2.35387, Accuracy: 98.47%\n",
      "Epoch [74/100], Loss: 2.35667, Accuracy: 98.49%\n",
      "Epoch [75/100], Loss: 2.35907, Accuracy: 98.50%\n",
      "Epoch [76/100], Loss: 2.35105, Accuracy: 98.51%\n",
      "Epoch [77/100], Loss: 2.35017, Accuracy: 98.53%\n",
      "Epoch [78/100], Loss: 2.34359, Accuracy: 98.55%\n",
      "Epoch [79/100], Loss: 2.34549, Accuracy: 98.56%\n",
      "Epoch [80/100], Loss: 2.34293, Accuracy: 98.58%\n",
      "Epoch [81/100], Loss: 2.33761, Accuracy: 98.60%\n",
      "Epoch [82/100], Loss: 2.33513, Accuracy: 98.61%\n",
      "Epoch [83/100], Loss: 2.33439, Accuracy: 98.63%\n",
      "Epoch [84/100], Loss: 2.33284, Accuracy: 98.65%\n",
      "Epoch [85/100], Loss: 2.33062, Accuracy: 98.66%\n",
      "Epoch [86/100], Loss: 2.33446, Accuracy: 98.68%\n",
      "Epoch [87/100], Loss: 2.33112, Accuracy: 98.69%\n",
      "Epoch [88/100], Loss: 2.33682, Accuracy: 98.71%\n",
      "Epoch [89/100], Loss: 2.35373, Accuracy: 98.72%\n",
      "Epoch [90/100], Loss: 2.34781, Accuracy: 98.73%\n",
      "Epoch [91/100], Loss: 2.35049, Accuracy: 98.75%\n",
      "Epoch [92/100], Loss: 2.36198, Accuracy: 98.76%\n",
      "Epoch [93/100], Loss: 2.35245, Accuracy: 98.77%\n",
      "Epoch [94/100], Loss: 2.34707, Accuracy: 98.79%\n",
      "Epoch [95/100], Loss: 2.34365, Accuracy: 98.80%\n",
      "Epoch [96/100], Loss: 2.33781, Accuracy: 98.81%\n",
      "Epoch [97/100], Loss: 2.33433, Accuracy: 98.82%\n",
      "Epoch [98/100], Loss: 2.33079, Accuracy: 98.84%\n",
      "Epoch [99/100], Loss: 2.33079, Accuracy: 98.85%\n",
      "Epoch [100/100], Loss: 2.32870, Accuracy: 98.86%\n",
      "Accuracy: 0.9474158653846154\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Freeze the encoder and train a linear classifier"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:02:24.978694Z",
     "start_time": "2024-11-29T15:02:24.977395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Freeze the encoder\n",
    "# # Function to freeze all layers of a model\n",
    "# def freeze_model(model):\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "# \n",
    "# # Freeze the contrastive learning model\n",
    "# freeze_model(encoder)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the linear classifier"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:02:25.052415Z",
     "start_time": "2024-11-29T15:02:25.051063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class LinearClassifier(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim):\n",
    "#         super(LinearClassifier, self).__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(in_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, out_dim)\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(0.25)\n",
    "#         self.attention = Attention(in_dim, 1)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         x = x.view(32, 8, 128)\n",
    "#         # # Max pooling over the bag instances\n",
    "#         # x, _ = torch.max(x, dim=1)\n",
    "#         x = self.dropout(x)\n",
    "#         x, _ = self.attention(x)\n",
    "#         return self.fc(x)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:02:25.123932Z",
     "start_time": "2024-11-29T15:02:25.122377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Initialize the linear classifier\n",
    "# classifier = LinearClassifier(in_dim=projection_dim, out_dim=1).cuda()\n",
    "# \n",
    "# # Training parameters\n",
    "# epochs = 50\n",
    "# learning_rate = 5e-4\n",
    "# optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "# criterion = nn.BCEWithLogitsLoss()"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:02:25.199876Z",
     "start_time": "2024-11-29T15:02:25.198307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Training loop for linear classification with accuracy calculation\n",
    "# for epoch in range(epochs):\n",
    "#     classifier.train()\n",
    "#     total_loss = 0\n",
    "#     correct_predictions = 0\n",
    "#     total_samples = 0\n",
    "#     \n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.cuda(), labels.float().cuda()\n",
    "#         features = encoder(images)\n",
    "#         outputs = classifier(features).squeeze()\n",
    "#         \n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         \n",
    "#         # Backpropagation and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         total_loss += loss.item()\n",
    "# \n",
    "#         # Calculate predictions and update correct predictions count\n",
    "#         predicted = (torch.sigmoid(outputs) > 0.5).float()  # Binary classification threshold\n",
    "#         correct_predictions += (predicted == labels).sum().item()\n",
    "#         total_samples += labels.size(0)\n",
    "# \n",
    "#     # Calculate average loss and accuracy for this epoch\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     accuracy = (correct_predictions / total_samples) * 100  # Convert to percentage\n",
    "# \n",
    "#     print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T15:02:25.271144Z",
     "start_time": "2024-11-29T15:02:25.269699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Evaluation loop\n",
    "# classifier.eval()\n",
    "# encoder.eval()\n",
    "# correct, total = 0, 0\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in eval_loader:\n",
    "#         images, labels = images.cuda(), labels.float().cuda()\n",
    "#         # images = augment_batch(images)\n",
    "#         features = encoder(images)\n",
    "#         outputs = classifier(features).squeeze()\n",
    "#         predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# print(f\"Accuracy: {correct/total}\")\n",
    "# \n",
    "# # Save the model\n",
    "# torch.save(encoder.state_dict(), \"encoder.pth\")\n",
    "# torch.save(classifier.state_dict(), \"classifier.pth\")"
   ],
   "outputs": [],
   "execution_count": 11
  }
 ]
}
