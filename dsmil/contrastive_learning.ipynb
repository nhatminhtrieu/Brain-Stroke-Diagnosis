{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/SimCLR.ipynb",
     "timestamp": 1732781375892
    }
   ]
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:47:19.335137Z",
     "start_time": "2024-12-02T04:47:18.247891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCELoss\n",
    "# import torchvision.transforms as transforms\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Define NTXentLoss (provided by you)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        similarity_matrix = torch.mm(z, z.T) / self.temperature\n",
    "        mask = torch.eye(2 * batch_size, device=z.device).bool()\n",
    "        similarity_matrix.masked_fill_(mask, -float('inf'))\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        denominator = exp_sim.sum(dim=1)\n",
    "        positive_samples = torch.cat(\n",
    "            [torch.arange(batch_size, 2 * batch_size), torch.arange(batch_size)], dim=0\n",
    "        ).to(z.device)\n",
    "        positives = similarity_matrix[torch.arange(2 * batch_size), positive_samples]\n",
    "        loss = -torch.log(torch.exp(positives) / denominator)\n",
    "        return loss.mean()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-29T10:08:54.329202Z",
     "iopub.execute_input": "2024-11-29T10:08:54.329822Z",
     "iopub.status.idle": "2024-11-29T10:09:15.549417Z",
     "shell.execute_reply.started": "2024-11-29T10:08:54.329785Z",
     "shell.execute_reply": "2024-11-29T10:09:15.548443Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-02T04:47:19.340351Z",
     "start_time": "2024-12-02T04:47:19.337678Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:47:22.154914Z",
     "start_time": "2024-12-02T04:47:19.389471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_data, eval_data = train_test_split(mnist_data, train_size=0.3, random_state=42, stratify=mnist_data.targets)\n",
    "\n",
    "# Bag-level dataset\n",
    "class BagDataset(Dataset):\n",
    "    def __init__(self, data, bag_size=8):\n",
    "        self.data = data\n",
    "        self.bag_size = bag_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.bag_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, labels = [], []\n",
    "        for i in range(self.bag_size):\n",
    "            img, label = self.data[idx * self.bag_size + i]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        bag_label = 1 if 9 in labels else 0\n",
    "        return torch.stack(images), torch.tensor(bag_label)\n",
    "\n",
    "train_dataset = BagDataset(train_data)\n",
    "eval_dataset = BagDataset(eval_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4, drop_last=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=4, drop_last=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:47:22.176554Z",
     "start_time": "2024-12-02T04:47:22.174520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_weights = self.attention(x)\n",
    "        weights = F.softmax(attention_weights, dim=1)\n",
    "        return (x * weights).sum(dim=1), weights.squeeze(-1)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:47:22.345393Z",
     "start_time": "2024-12-02T04:47:22.201577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, base_model, projection_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = base_model\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.attention = Attention(512, 1)\n",
    "        # self.fc = nn.Linear(512, 1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        batch_size, num_instances, channels, height, width = x.size()\n",
    "        x = x.view(-1, channels, height, width)  # Reshape to (batch_size * num_instances, channels, height, width)\n",
    "        features = self.encoder(x)\n",
    "        features = nn.Dropout(0.25)(features)\n",
    "        features = features.view(batch_size * num_instances, -1)  # Flatten to (batch_size * num_instances, feature_dim)\n",
    "        \n",
    "        projection_features = self.projection(features)\n",
    "        attention_features, _ = self.attention(features.view(batch_size, num_instances, -1))\n",
    "        output = self.fc(attention_features)\n",
    "        \n",
    "        return projection_features, output\n",
    "\n",
    "# Augmentation function\n",
    "def augment_batch(batch_images):\n",
    "    batch_size, num_instances, channels, height, width = batch_images.shape\n",
    "    aug_transform = transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(224, scale=(0.8, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3)], p=0.6),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Apply transformation to each image instance in the batch\n",
    "    augmented_batch = []\n",
    "    for i in range(batch_size):\n",
    "        augmented_instances = [aug_transform(transforms.ToPILImage()(img.cpu())) for img in batch_images[i]]\n",
    "        augmented_batch.append(torch.stack(augmented_instances))\n",
    "    \n",
    "    return torch.stack(augmented_batch).cuda()  # Move the augmented batch to GPU\n",
    "\n",
    "# Initialize models, loss function, and optimizer\n",
    "base_model = models.resnet18(weights=None)\n",
    "base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_model.fc = nn.Identity()\n",
    "encoder = Encoder(base_model).cuda()\n",
    "projection_dim = 256\n",
    "ntxent_loss = NTXentLoss().cuda()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# Training parameters\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "bceLoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "# Training loop for contrastive learning\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda()  # Move the batch to GPU\n",
    "        labels = labels.cuda()\n",
    "        # aug1 = augment_batch(images).cuda()\n",
    "        aug1 = images \n",
    "        aug2 = augment_batch(images).cuda()\n",
    "        # aug2 = images\n",
    "        \n",
    "        z_i, outputs_1 = encoder(aug1)\n",
    "        z_j, outputs_2 = encoder(aug2)\n",
    "        \n",
    "        NTXLoss = ntxent_loss(z_i, z_j)\n",
    "        BCELoss_1 = bceLoss(outputs_1.squeeze(), labels.float())\n",
    "        BCELoss_2 = bceLoss(outputs_2.squeeze(), labels.float())\n",
    "        loss = 0.4 * NTXLoss + 0.3 * BCELoss_1 + 0.3 * BCELoss_2\n",
    "        loss = loss.mean()\n",
    "        # loss = ntxent_loss(z_i, z_j)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate predictions and update correct predictions count\n",
    "        predicted = (torch.sigmoid(outputs_1.squeeze()) > 0.5).float()  # Binary classification threshold\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = (correct_predictions / total_samples) * 100  # Convert to percentage\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.5f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in eval_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        z_i, output = encoder(images)\n",
    "        predicted = (torch.sigmoid(output.squeeze()) > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {correct/total}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2024-11-29T10:09:15.550943Z",
     "iopub.execute_input": "2024-11-29T10:09:15.551296Z",
     "iopub.status.idle": "2024-11-29T10:15:04.545496Z",
     "shell.execute_reply.started": "2024-11-29T10:09:15.551270Z",
     "shell.execute_reply": "2024-11-29T10:15:04.544444Z"
    },
    "ExecuteTime": {
     "end_time": "2024-12-02T04:49:51.378008Z",
     "start_time": "2024-12-02T04:47:22.364406Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 3.03937, Accuracy: 74.31%\n",
      "Epoch [2/50], Loss: 2.70825, Accuracy: 83.55%\n",
      "Epoch [3/50], Loss: 2.62387, Accuracy: 87.65%\n",
      "Epoch [4/50], Loss: 2.57440, Accuracy: 89.99%\n",
      "Epoch [5/50], Loss: 2.53745, Accuracy: 91.61%\n",
      "Epoch [6/50], Loss: 2.54068, Accuracy: 92.52%\n",
      "Epoch [7/50], Loss: 2.50965, Accuracy: 93.38%\n",
      "Epoch [8/50], Loss: 2.47856, Accuracy: 94.07%\n",
      "Epoch [9/50], Loss: 2.46844, Accuracy: 94.65%\n",
      "Epoch [10/50], Loss: 2.45623, Accuracy: 95.10%\n",
      "Epoch [11/50], Loss: 2.43674, Accuracy: 95.49%\n",
      "Epoch [12/50], Loss: 2.42786, Accuracy: 95.83%\n",
      "Epoch [13/50], Loss: 2.42450, Accuracy: 96.08%\n",
      "Epoch [14/50], Loss: 2.42110, Accuracy: 96.32%\n",
      "Epoch [15/50], Loss: 2.41485, Accuracy: 96.52%\n",
      "Epoch [16/50], Loss: 2.41130, Accuracy: 96.71%\n",
      "Epoch [17/50], Loss: 2.40327, Accuracy: 96.88%\n",
      "Epoch [18/50], Loss: 2.39383, Accuracy: 97.03%\n",
      "Epoch [19/50], Loss: 2.40393, Accuracy: 97.16%\n",
      "Epoch [20/50], Loss: 2.39562, Accuracy: 97.27%\n",
      "Epoch [21/50], Loss: 2.39074, Accuracy: 97.38%\n",
      "Epoch [22/50], Loss: 2.36733, Accuracy: 97.50%\n",
      "Epoch [23/50], Loss: 2.37285, Accuracy: 97.61%\n",
      "Epoch [24/50], Loss: 2.37627, Accuracy: 97.69%\n",
      "Epoch [25/50], Loss: 2.37905, Accuracy: 97.76%\n",
      "Epoch [26/50], Loss: 2.37180, Accuracy: 97.83%\n",
      "Epoch [27/50], Loss: 2.37109, Accuracy: 97.90%\n",
      "Epoch [28/50], Loss: 2.36680, Accuracy: 97.97%\n",
      "Epoch [29/50], Loss: 2.35825, Accuracy: 98.03%\n",
      "Epoch [30/50], Loss: 2.35878, Accuracy: 98.09%\n",
      "Epoch [31/50], Loss: 2.36280, Accuracy: 98.15%\n",
      "Epoch [32/50], Loss: 2.35434, Accuracy: 98.21%\n",
      "Epoch [33/50], Loss: 2.35497, Accuracy: 98.26%\n",
      "Epoch [34/50], Loss: 2.36186, Accuracy: 98.30%\n",
      "Epoch [35/50], Loss: 2.35496, Accuracy: 98.35%\n",
      "Epoch [36/50], Loss: 2.35166, Accuracy: 98.39%\n",
      "Epoch [37/50], Loss: 2.34411, Accuracy: 98.43%\n",
      "Epoch [38/50], Loss: 2.34233, Accuracy: 98.47%\n",
      "Epoch [39/50], Loss: 2.34200, Accuracy: 98.51%\n",
      "Epoch [40/50], Loss: 2.33769, Accuracy: 98.54%\n",
      "Epoch [41/50], Loss: 2.33785, Accuracy: 98.58%\n",
      "Epoch [42/50], Loss: 2.33599, Accuracy: 98.61%\n",
      "Epoch [43/50], Loss: 2.33626, Accuracy: 98.64%\n",
      "Epoch [44/50], Loss: 2.33508, Accuracy: 98.67%\n",
      "Epoch [45/50], Loss: 2.35073, Accuracy: 98.69%\n",
      "Epoch [46/50], Loss: 2.34171, Accuracy: 98.71%\n",
      "Epoch [47/50], Loss: 2.34080, Accuracy: 98.73%\n",
      "Epoch [48/50], Loss: 2.33908, Accuracy: 98.76%\n",
      "Epoch [49/50], Loss: 2.33264, Accuracy: 98.78%\n",
      "Epoch [50/50], Loss: 2.33724, Accuracy: 98.81%\n",
      "Test Accuracy: 0.9552210365853658\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Freeze the encoder and train a linear classifier"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:49:51.475966Z",
     "start_time": "2024-12-02T04:49:51.474600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Freeze the encoder\n",
    "# # Function to freeze all layers of a model\n",
    "# def freeze_model(model):\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "# \n",
    "# # Freeze the contrastive learning model\n",
    "# freeze_model(encoder)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define the linear classifier"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:49:51.545867Z",
     "start_time": "2024-12-02T04:49:51.544432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class LinearClassifier(nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim):\n",
    "#         super(LinearClassifier, self).__init__()\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(in_dim, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, out_dim)\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(0.25)\n",
    "#         self.attention = Attention(in_dim, 1)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         x = x.view(32, 8, 128)\n",
    "#         # # Max pooling over the bag instances\n",
    "#         # x, _ = torch.max(x, dim=1)\n",
    "#         x = self.dropout(x)\n",
    "#         x, _ = self.attention(x)\n",
    "#         return self.fc(x)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:49:51.614562Z",
     "start_time": "2024-12-02T04:49:51.613266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Initialize the linear classifier\n",
    "# classifier = LinearClassifier(in_dim=projection_dim, out_dim=1).cuda()\n",
    "# \n",
    "# # Training parameters\n",
    "# epochs = 50\n",
    "# learning_rate = 5e-4\n",
    "# optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "# criterion = nn.BCEWithLogitsLoss()"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:49:51.682801Z",
     "start_time": "2024-12-02T04:49:51.681118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Training loop for linear classification with accuracy calculation\n",
    "# for epoch in range(epochs):\n",
    "#     classifier.train()\n",
    "#     total_loss = 0\n",
    "#     correct_predictions = 0\n",
    "#     total_samples = 0\n",
    "#     \n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.cuda(), labels.float().cuda()\n",
    "#         features = encoder(images)\n",
    "#         outputs = classifier(features).squeeze()\n",
    "#         \n",
    "#         # Calculate the loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         \n",
    "#         # Backpropagation and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         total_loss += loss.item()\n",
    "# \n",
    "#         # Calculate predictions and update correct predictions count\n",
    "#         predicted = (torch.sigmoid(outputs) > 0.5).float()  # Binary classification threshold\n",
    "#         correct_predictions += (predicted == labels).sum().item()\n",
    "#         total_samples += labels.size(0)\n",
    "# \n",
    "#     # Calculate average loss and accuracy for this epoch\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     accuracy = (correct_predictions / total_samples) * 100  # Convert to percentage\n",
    "# \n",
    "#     print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T04:49:51.751977Z",
     "start_time": "2024-12-02T04:49:51.750313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Evaluation loop\n",
    "# classifier.eval()\n",
    "# encoder.eval()\n",
    "# correct, total = 0, 0\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in eval_loader:\n",
    "#         images, labels = images.cuda(), labels.float().cuda()\n",
    "#         # images = augment_batch(images)\n",
    "#         features = encoder(images)\n",
    "#         outputs = classifier(features).squeeze()\n",
    "#         predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# print(f\"Accuracy: {correct/total}\")\n",
    "# \n",
    "# # Save the model\n",
    "# torch.save(encoder.state_dict(), \"encoder.pth\")\n",
    "# torch.save(classifier.state_dict(), \"classifier.pth\")"
   ],
   "outputs": [],
   "execution_count": 11
  }
 ]
}
