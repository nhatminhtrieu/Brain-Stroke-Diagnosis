{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import Libraries",
   "id": "d675e2349b26c09f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-29T16:15:48.684250Z",
     "start_time": "2024-11-29T16:15:48.680819Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preparation",
   "id": "672bbbd8209f388f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:15:51.451126Z",
     "start_time": "2024-11-29T16:15:48.729865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_data, eval_data = train_test_split(mnist_data, train_size=0.1, random_state=42, stratify=mnist_data.targets)\n",
    "\n",
    "# Bag-level dataset\n",
    "class BagDataset(Dataset):\n",
    "    def __init__(self, data, bag_size=8):\n",
    "        self.data = data\n",
    "        self.bag_size = bag_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.bag_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, labels = [], []\n",
    "        for i in range(self.bag_size):\n",
    "            img, label = self.data[idx * self.bag_size + i]\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        bag_label = 1 if 9 in labels else 0\n",
    "        return torch.stack(images), torch.tensor(bag_label)\n",
    "\n",
    "train_dataset = BagDataset(train_data)\n",
    "eval_dataset = BagDataset(eval_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=4, drop_last=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32, shuffle=False, pin_memory=True, num_workers=4, drop_last=True)"
   ],
   "id": "bad48fa375531d1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Definition",
   "id": "4ad3fd07b10aa85b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NTXentLoss",
   "id": "20bbdf04d7172c16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:15:51.541246Z",
     "start_time": "2024-11-29T16:15:51.538586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define NTXentLoss (provided by you)\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        similarity_matrix = torch.mm(z, z.T) / self.temperature\n",
    "        mask = torch.eye(2 * batch_size, device=z.device).bool()\n",
    "        similarity_matrix.masked_fill_(mask, -float('inf'))\n",
    "        exp_sim = torch.exp(similarity_matrix)\n",
    "        denominator = exp_sim.sum(dim=1)\n",
    "        positive_samples = torch.cat(\n",
    "            [torch.arange(batch_size, 2 * batch_size), torch.arange(batch_size)], dim=0\n",
    "        ).to(z.device)\n",
    "        positives = similarity_matrix[torch.arange(2 * batch_size), positive_samples]\n",
    "        loss = -torch.log(torch.exp(positives) / denominator)\n",
    "        return loss.mean()"
   ],
   "id": "dbd03f603a2a8209",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dual-Stream MIL Model (DSMIL)",
   "id": "7bda39b2525a9499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:15:51.621268Z",
     "start_time": "2024-11-29T16:15:51.615884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(input_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class InstanceClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super(InstanceClassifier, self).__init__()\n",
    "        self.features_extractor = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.features_extractor.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.features_extractor.fc = nn.Identity()\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_instances, C, H, W = x.shape\n",
    "        x = x.view(batch_size * num_instances, C, H, W)\n",
    "        \n",
    "        instance_features = nn.Dropout(0.25)(self.features_extractor(x)).view(batch_size, num_instances, -1)\n",
    "        classes = self.fc(instance_features)\n",
    "        \n",
    "        return instance_features, classes\n",
    "    \n",
    "class BagClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1, hidden_dim=128, dropout_v=0.2, non_linear=True, passing_v=False):\n",
    "        super(BagClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if non_linear:\n",
    "            self.q = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            self.q = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        if passing_v:\n",
    "            self.v = nn.Sequential(\n",
    "                nn.Dropout(dropout_v),\n",
    "                nn.Linear(input_dim, input_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.v = nn.Identity()\n",
    "            \n",
    "        self.fc = FCLayer(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, features, classes):\n",
    "        batch_size = features.size(0)\n",
    "        num_instances = features.size(1)\n",
    "        features_dim = features.size(2)\n",
    "        \n",
    "        combine_features = features.view(features.shape[0] * features.shape[1], -1)\n",
    "        V = self.v(combine_features)\n",
    "        Q = self.q(combine_features)\n",
    "        assert V.shape[0] == Q.shape[0] == batch_size * num_instances, f'V: {V.shape}, Q: {Q.shape}'\n",
    "        assert V.shape[1] == features_dim, f'V: {V.shape} should be [{batch_size * num_instances}, {features_dim}]'\n",
    "        assert Q.shape[1] == self.hidden_dim, f'Q: {Q.shape} should be [{batch_size * num_instances}, {self.hidden_dim}]'\n",
    "        \n",
    "        # Get critical instance indices by squeezing classes\n",
    "        critical_indices = torch.squeeze(classes).argmax(dim=1)  # Shape [32]\n",
    "        assert critical_indices.shape[0] == batch_size, f'Critical indices: {critical_indices.shape}'\n",
    "\n",
    "        # Gather features for each batch using critical instance indices\n",
    "        m_features = features[torch.arange(batch_size).unsqueeze(1), critical_indices.unsqueeze(1)].squeeze()\n",
    "        assert m_features.shape[0] == batch_size, f'M features: {m_features.shape} should be [{batch_size}, {features_dim}]'\n",
    "        q_max = self.q(m_features)\n",
    "        assert q_max.shape[0] == batch_size and q_max.shape[1] == self.hidden_dim, f'Q max: {q_max.shape} should be [{batch_size}, {self.hidden_dim}]'\n",
    "        \n",
    "        A = torch.mm(Q, q_max.mT)\n",
    "        A = F.softmax(A / torch.sqrt(torch.tensor(Q.shape[-1]).float()), dim=0)\n",
    "        assert A.shape[0] == batch_size * num_instances and A.shape[1] == batch_size, f'A: {A.shape} should be [{batch_size * num_instances}, {batch_size}]'\n",
    "        \n",
    "        B = torch.mm(A.T, V)\n",
    "        assert B.shape[0] == batch_size and B.shape[1] == features_dim, f'B: {B.shape} should be [{batch_size}, {features_dim}]'\n",
    "        \n",
    "        B = B.view(1, B.shape[0], B.shape[1])\n",
    "        C = self.fc(B)\n",
    "        C = C.view(1, -1)\n",
    "        \n",
    "        return C, A, B"
   ],
   "id": "cd5780327a678c64",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoder Model",
   "id": "80b183f5701793a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:15:51.699120Z",
     "start_time": "2024-11-29T16:15:51.695886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, base_model, projection_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = base_model\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, projection_dim)\n",
    "        )\n",
    "        \n",
    "        self.instance_classifier = InstanceClassifier(512)\n",
    "        self.bag_classifier = BagClassifier(512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, num_instances, channels, height, width = x.size()\n",
    "        \n",
    "        instances_features, classes = self.instance_classifier(x)\n",
    "        \n",
    "        features = instances_features.view(batch_size * num_instances, -1)  # Flatten to (batch_size * num_instances, feature_dim)\n",
    "        \n",
    "        projection_features = self.projection(features)\n",
    "        \n",
    "        predicted_bags, A, B = self.bag_classifier(instances_features, classes)\n",
    "        \n",
    "        return projection_features, classes, predicted_bags, A, B\n",
    "\n",
    "# Augmentation function\n",
    "def augment_batch(batch_images):\n",
    "    batch_size, num_instances, channels, height, width = batch_images.shape\n",
    "    aug_transform = transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(36, scale=(0.8, 1)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3)], p=0.6),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Apply transformation to each image instance in the batch\n",
    "    augmented_batch = []\n",
    "    for i in range(batch_size):\n",
    "        augmented_instances = [aug_transform(transforms.ToPILImage()(img.cpu())) for img in batch_images[i]]\n",
    "        augmented_batch.append(torch.stack(augmented_instances))\n",
    "    \n",
    "    return torch.stack(augmented_batch).cuda()  # Move the augmented batch to GPU\n"
   ],
   "id": "70a7956091c803d9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Initialization",
   "id": "862eff286e786bcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:15:51.933042Z",
     "start_time": "2024-11-29T16:15:51.773548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize models, loss function, and optimizer\n",
    "base_model = models.resnet18(weights=None)\n",
    "base_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "base_model.fc = nn.Identity()\n",
    "encoder = Encoder(base_model).cuda()\n",
    "projection_dim = 256\n",
    "ntxent_loss = NTXentLoss().cuda()"
   ],
   "id": "aa34df87541f2986",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training & Evaluation",
   "id": "2fd8f0b3fc1bc9c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:59.003756Z",
     "start_time": "2024-11-29T16:15:52.008133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training parameters\n",
    "epochs = 150\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "bceLoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "# Training loop for contrastive learning\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.cuda()  # Move the batch to GPU\n",
    "        labels = labels.cuda()\n",
    "        # aug1 = augment_batch(images).cuda()\n",
    "        aug1 = images \n",
    "        aug2 = augment_batch(images).cuda()\n",
    "        \n",
    "        z_i, outputs_1, predicted_bags_1, _, _ = encoder(aug1)\n",
    "        z_j, outputs_2, predicted_bags_2, _, _ = encoder(aug2)\n",
    "        \n",
    "        NTXLoss = ntxent_loss(z_i, z_j)\n",
    "        max_agg_1 = torch.max(outputs_1, dim=1).values.squeeze()\n",
    "        max_agg_2 = torch.max(outputs_2, dim=1).values.squeeze()\n",
    "        \n",
    "        loss_max_1 = bceLoss(max_agg_1, labels.float())\n",
    "        loss_max_2 = bceLoss(max_agg_2, labels.float())\n",
    "        loss_bag_1 = bceLoss(predicted_bags_1.squeeze(), labels.float())\n",
    "        loss_bag_2 = bceLoss(predicted_bags_2.squeeze(), labels.float())\n",
    "        \n",
    "        loss = 0.2 * NTXLoss + 0.2 * loss_max_1 + 0.2 * loss_max_2 + 0.2 * loss_bag_1 + 0.2 * loss_bag_2\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate predictions and update correct predictions count\n",
    "        predicted = (torch.sigmoid(predicted_bags_1.squeeze()) > 0.5).float()  # Binary classification threshold\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = (correct_predictions / total_samples) * 100  # Convert to percentage\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.5f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    encoder.eval()\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in eval_loader:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        z_i, output, predicted_bags, _, _ = encoder(images)\n",
    "        predicted = (torch.sigmoid(predicted_bags.squeeze()) > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f\"Test Accuracy: {correct/total}\")"
   ],
   "id": "52689a59ab9a7caf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hskha23/miniconda3/envs/tf/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 1.78608, Accuracy: 55.71%\n",
      "Epoch [2/150], Loss: 1.64287, Accuracy: 60.39%\n",
      "Epoch [3/150], Loss: 1.42234, Accuracy: 69.20%\n",
      "Epoch [4/150], Loss: 1.30298, Accuracy: 75.17%\n",
      "Epoch [5/150], Loss: 1.26689, Accuracy: 78.97%\n",
      "Epoch [6/150], Loss: 1.25020, Accuracy: 81.25%\n",
      "Epoch [7/150], Loss: 1.19878, Accuracy: 83.39%\n",
      "Epoch [8/150], Loss: 1.13303, Accuracy: 85.26%\n",
      "Epoch [9/150], Loss: 1.11858, Accuracy: 86.68%\n",
      "Epoch [10/150], Loss: 1.11586, Accuracy: 87.84%\n",
      "Epoch [11/150], Loss: 1.09246, Accuracy: 88.76%\n",
      "Epoch [12/150], Loss: 1.09648, Accuracy: 89.47%\n",
      "Epoch [13/150], Loss: 1.09496, Accuracy: 90.09%\n",
      "Epoch [14/150], Loss: 1.07725, Accuracy: 90.69%\n",
      "Epoch [15/150], Loss: 1.06048, Accuracy: 91.23%\n",
      "Epoch [16/150], Loss: 1.07648, Accuracy: 91.63%\n",
      "Epoch [17/150], Loss: 1.05111, Accuracy: 92.06%\n",
      "Epoch [18/150], Loss: 1.03062, Accuracy: 92.47%\n",
      "Epoch [19/150], Loss: 0.99908, Accuracy: 92.84%\n",
      "Epoch [20/150], Loss: 1.01378, Accuracy: 93.18%\n",
      "Epoch [21/150], Loss: 0.99616, Accuracy: 93.48%\n",
      "Epoch [22/150], Loss: 0.99847, Accuracy: 93.73%\n",
      "Epoch [23/150], Loss: 0.99021, Accuracy: 93.96%\n",
      "Epoch [24/150], Loss: 0.98834, Accuracy: 94.19%\n",
      "Epoch [25/150], Loss: 0.97371, Accuracy: 94.39%\n",
      "Epoch [26/150], Loss: 0.98180, Accuracy: 94.57%\n",
      "Epoch [27/150], Loss: 0.96470, Accuracy: 94.77%\n",
      "Epoch [28/150], Loss: 0.96767, Accuracy: 94.94%\n",
      "Epoch [29/150], Loss: 0.99868, Accuracy: 95.08%\n",
      "Epoch [30/150], Loss: 0.96274, Accuracy: 95.24%\n",
      "Epoch [31/150], Loss: 0.96602, Accuracy: 95.37%\n",
      "Epoch [32/150], Loss: 0.96190, Accuracy: 95.47%\n",
      "Epoch [33/150], Loss: 0.94624, Accuracy: 95.60%\n",
      "Epoch [34/150], Loss: 0.97698, Accuracy: 95.72%\n",
      "Epoch [35/150], Loss: 0.96942, Accuracy: 95.83%\n",
      "Epoch [36/150], Loss: 0.94745, Accuracy: 95.93%\n",
      "Epoch [37/150], Loss: 0.95823, Accuracy: 96.03%\n",
      "Epoch [38/150], Loss: 0.94549, Accuracy: 96.12%\n",
      "Epoch [39/150], Loss: 0.93618, Accuracy: 96.22%\n",
      "Epoch [40/150], Loss: 0.92777, Accuracy: 96.32%\n",
      "Epoch [41/150], Loss: 0.93289, Accuracy: 96.41%\n",
      "Epoch [42/150], Loss: 0.95683, Accuracy: 96.48%\n",
      "Epoch [43/150], Loss: 0.95981, Accuracy: 96.56%\n",
      "Epoch [44/150], Loss: 0.93064, Accuracy: 96.63%\n",
      "Epoch [45/150], Loss: 0.94558, Accuracy: 96.70%\n",
      "Epoch [46/150], Loss: 0.92419, Accuracy: 96.77%\n",
      "Epoch [47/150], Loss: 0.96973, Accuracy: 96.81%\n",
      "Epoch [48/150], Loss: 0.94337, Accuracy: 96.86%\n",
      "Epoch [49/150], Loss: 0.92141, Accuracy: 96.93%\n",
      "Epoch [50/150], Loss: 0.94121, Accuracy: 96.98%\n",
      "Epoch [51/150], Loss: 0.93565, Accuracy: 97.02%\n",
      "Epoch [52/150], Loss: 0.94136, Accuracy: 97.08%\n",
      "Epoch [53/150], Loss: 0.92470, Accuracy: 97.13%\n",
      "Epoch [54/150], Loss: 0.92133, Accuracy: 97.19%\n",
      "Epoch [55/150], Loss: 0.91759, Accuracy: 97.24%\n",
      "Epoch [56/150], Loss: 0.90788, Accuracy: 97.29%\n",
      "Epoch [57/150], Loss: 0.91432, Accuracy: 97.34%\n",
      "Epoch [58/150], Loss: 0.91485, Accuracy: 97.38%\n",
      "Epoch [59/150], Loss: 0.91590, Accuracy: 97.43%\n",
      "Epoch [60/150], Loss: 0.91570, Accuracy: 97.47%\n",
      "Epoch [61/150], Loss: 0.91803, Accuracy: 97.50%\n",
      "Epoch [62/150], Loss: 0.91707, Accuracy: 97.53%\n",
      "Epoch [63/150], Loss: 0.92158, Accuracy: 97.56%\n",
      "Epoch [64/150], Loss: 0.92267, Accuracy: 97.59%\n",
      "Epoch [65/150], Loss: 0.92938, Accuracy: 97.63%\n",
      "Epoch [66/150], Loss: 0.92645, Accuracy: 97.66%\n",
      "Epoch [67/150], Loss: 0.91466, Accuracy: 97.69%\n",
      "Epoch [68/150], Loss: 0.91076, Accuracy: 97.73%\n",
      "Epoch [69/150], Loss: 0.90223, Accuracy: 97.76%\n",
      "Epoch [70/150], Loss: 0.92417, Accuracy: 97.79%\n",
      "Epoch [71/150], Loss: 0.91453, Accuracy: 97.82%\n",
      "Epoch [72/150], Loss: 0.90153, Accuracy: 97.85%\n",
      "Epoch [73/150], Loss: 0.90518, Accuracy: 97.88%\n",
      "Epoch [74/150], Loss: 0.91337, Accuracy: 97.91%\n",
      "Epoch [75/150], Loss: 0.94480, Accuracy: 97.92%\n",
      "Epoch [76/150], Loss: 0.92584, Accuracy: 97.95%\n",
      "Epoch [77/150], Loss: 0.92089, Accuracy: 97.97%\n",
      "Epoch [78/150], Loss: 0.90446, Accuracy: 98.00%\n",
      "Epoch [79/150], Loss: 0.91927, Accuracy: 98.02%\n",
      "Epoch [80/150], Loss: 0.90625, Accuracy: 98.04%\n",
      "Epoch [81/150], Loss: 0.91108, Accuracy: 98.06%\n",
      "Epoch [82/150], Loss: 0.90484, Accuracy: 98.08%\n",
      "Epoch [83/150], Loss: 0.89620, Accuracy: 98.11%\n",
      "Epoch [84/150], Loss: 0.90761, Accuracy: 98.13%\n",
      "Epoch [85/150], Loss: 0.90360, Accuracy: 98.15%\n",
      "Epoch [86/150], Loss: 0.89925, Accuracy: 98.17%\n",
      "Epoch [87/150], Loss: 0.91567, Accuracy: 98.19%\n",
      "Epoch [88/150], Loss: 0.90401, Accuracy: 98.21%\n",
      "Epoch [89/150], Loss: 0.89902, Accuracy: 98.23%\n",
      "Epoch [90/150], Loss: 0.90582, Accuracy: 98.25%\n",
      "Epoch [91/150], Loss: 0.89633, Accuracy: 98.27%\n",
      "Epoch [92/150], Loss: 0.90094, Accuracy: 98.29%\n",
      "Epoch [93/150], Loss: 0.89426, Accuracy: 98.31%\n",
      "Epoch [94/150], Loss: 0.89066, Accuracy: 98.33%\n",
      "Epoch [95/150], Loss: 0.89956, Accuracy: 98.34%\n",
      "Epoch [96/150], Loss: 0.94053, Accuracy: 98.35%\n",
      "Epoch [97/150], Loss: 0.91445, Accuracy: 98.37%\n",
      "Epoch [98/150], Loss: 0.90164, Accuracy: 98.38%\n",
      "Epoch [99/150], Loss: 0.89644, Accuracy: 98.40%\n",
      "Epoch [100/150], Loss: 0.90521, Accuracy: 98.41%\n",
      "Epoch [101/150], Loss: 0.90366, Accuracy: 98.43%\n",
      "Epoch [102/150], Loss: 0.89227, Accuracy: 98.44%\n",
      "Epoch [103/150], Loss: 0.88953, Accuracy: 98.46%\n",
      "Epoch [104/150], Loss: 0.89738, Accuracy: 98.47%\n",
      "Epoch [105/150], Loss: 0.90165, Accuracy: 98.48%\n",
      "Epoch [106/150], Loss: 0.90862, Accuracy: 98.50%\n",
      "Epoch [107/150], Loss: 0.90201, Accuracy: 98.51%\n",
      "Epoch [108/150], Loss: 0.90231, Accuracy: 98.52%\n",
      "Epoch [109/150], Loss: 0.89947, Accuracy: 98.53%\n",
      "Epoch [110/150], Loss: 0.89272, Accuracy: 98.54%\n",
      "Epoch [111/150], Loss: 0.88688, Accuracy: 98.55%\n",
      "Epoch [112/150], Loss: 0.89016, Accuracy: 98.57%\n",
      "Epoch [113/150], Loss: 0.89960, Accuracy: 98.58%\n",
      "Epoch [114/150], Loss: 0.89839, Accuracy: 98.59%\n",
      "Epoch [115/150], Loss: 0.89805, Accuracy: 98.60%\n",
      "Epoch [116/150], Loss: 0.89976, Accuracy: 98.61%\n",
      "Epoch [117/150], Loss: 0.89131, Accuracy: 98.62%\n",
      "Epoch [118/150], Loss: 0.90778, Accuracy: 98.63%\n",
      "Epoch [119/150], Loss: 0.89597, Accuracy: 98.64%\n",
      "Epoch [120/150], Loss: 0.88721, Accuracy: 98.65%\n",
      "Epoch [121/150], Loss: 0.88405, Accuracy: 98.66%\n",
      "Epoch [122/150], Loss: 0.88303, Accuracy: 98.68%\n",
      "Epoch [123/150], Loss: 0.88190, Accuracy: 98.69%\n",
      "Epoch [124/150], Loss: 0.88067, Accuracy: 98.70%\n",
      "Epoch [125/150], Loss: 0.88106, Accuracy: 98.71%\n",
      "Epoch [126/150], Loss: 0.88719, Accuracy: 98.72%\n",
      "Epoch [127/150], Loss: 0.90936, Accuracy: 98.73%\n",
      "Epoch [128/150], Loss: 0.92663, Accuracy: 98.74%\n",
      "Epoch [129/150], Loss: 0.92853, Accuracy: 98.74%\n",
      "Epoch [130/150], Loss: 0.90252, Accuracy: 98.75%\n",
      "Epoch [131/150], Loss: 0.89919, Accuracy: 98.76%\n",
      "Epoch [132/150], Loss: 0.89639, Accuracy: 98.77%\n",
      "Epoch [133/150], Loss: 0.88707, Accuracy: 98.78%\n",
      "Epoch [134/150], Loss: 0.88395, Accuracy: 98.79%\n",
      "Epoch [135/150], Loss: 0.88223, Accuracy: 98.80%\n",
      "Epoch [136/150], Loss: 0.88086, Accuracy: 98.81%\n",
      "Epoch [137/150], Loss: 0.87990, Accuracy: 98.81%\n",
      "Epoch [138/150], Loss: 0.87979, Accuracy: 98.82%\n",
      "Epoch [139/150], Loss: 0.87935, Accuracy: 98.83%\n",
      "Epoch [140/150], Loss: 0.87908, Accuracy: 98.84%\n",
      "Epoch [141/150], Loss: 0.87795, Accuracy: 98.85%\n",
      "Epoch [142/150], Loss: 0.87770, Accuracy: 98.86%\n",
      "Epoch [143/150], Loss: 0.87806, Accuracy: 98.86%\n",
      "Epoch [144/150], Loss: 0.87743, Accuracy: 98.87%\n",
      "Epoch [145/150], Loss: 0.87656, Accuracy: 98.88%\n",
      "Epoch [146/150], Loss: 0.87624, Accuracy: 98.89%\n",
      "Epoch [147/150], Loss: 0.87590, Accuracy: 98.90%\n",
      "Epoch [148/150], Loss: 0.87635, Accuracy: 98.90%\n",
      "Epoch [149/150], Loss: 0.87593, Accuracy: 98.91%\n",
      "Epoch [150/150], Loss: 0.87521, Accuracy: 98.92%\n",
      "Test Accuracy: 0.9523809523809523\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:59.084520Z",
     "start_time": "2024-11-29T16:18:59.083113Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2d6ef0f4cc987c4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T16:18:59.162109Z",
     "start_time": "2024-11-29T16:18:59.160974Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2eb660a0d59d60d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
